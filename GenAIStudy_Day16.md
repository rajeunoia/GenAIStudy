# Day 16 Study Time Gen AI

**Time Interval:** 00:00 - 99:00  
**Summary**  
- **üîç Overview**: The session continues from the previous discussion on **regularization**, specifically focusing on **L regularization** and **elastic net regularization**, addressing their necessity and the issues they aim to solve.
- **üìê Introduction to Cross Validation**: The instructor introduces **cross validation**, also known as **k-fold cross validation**, explaining its types and purpose in evaluating model performance by minimizing overfitting.
- **üõ† Hold-Out Cross Validation**: Hold-out cross-validation is described as the simplest form, where data is split into training and testing subsets, emphasizing the risks of bias in the testing data when only one test set is used.
- **üéØ K-Fold Cross Validation Details**: K-fold cross validation involves dividing the dataset into **K subsets**. The model is trained multiple times, each time using different subsets as the test data, effectively reducing bias in model evaluation.
- **üìä Stratified K-Fold Validation**: The session discusses stratified k-fold cross validation, which ensures that each fold maintains the same class distribution as the original dataset, crucial for datasets with imbalanced classes.
- **üöÄ Conclusion and Next Steps**: The instructor wraps up the session, noting the importance of understanding cross-validation methods. There are plans to explore more advanced validation techniques in future sessions while acknowledging the significance of alignment in validation processes for competitions and model submissions.

# Transcript 


0:4:9 -  hey hi everyone um so in continuation to uh yesterday what we have covered we have looked at regularization um and um we have looked at L regularization rization stuff you understood and we also looked at elastic net regation and how do we add it what is the need for it and what is the problem that it's trying to solve and stuff the let's get to um the next one that is there in line is cross validation um it's also called kold cross validation so let's look up on what if there are I don't think there are types of cross validation but let's ask chg and learn about it okay I'm sharing my screen has expired [Music] this is all about regularization right validation okay what is cross different G so process multiple for training and validation to [Music] oh five that is too many but okay hold out cross validation I never heard it frankly hold out cross validation is the simplest form of cross validation is randomly split into two subs training valid mod train on training set and on validation set that's like hold out cross validation is actually that the training test data split and and testing that's that's what usually we do so that's called hold out Closs validation or like typical Closs validation kind of stuff okay k foration k equal sized full Zone subsets model K time time using K minus one folds for training and the remaining fold for a so instead of um validating with one test set they divide the whole training data set into K test sets so that there is no um there is no bias towards not only training data but also on the test data because what happens we've always been training with the same training data set on the hold out cross validation if you see there is only two data sets one is training data set and another one is

0:6:41 -  test data set so you keep training with training data set you go and test with test data set and if the test results are not good you go back and tweak the training data set so uh indirectly if you see even though we don't use the test data set in the training process your objective is to train in a way that it adopts to the training data as well it performs well with test data so with one single test data set there is a chance that you are actually um indirectly your your model you're aligning with the test data set to produce better results on test data set that problem is there so to solve that problem they bought in kold Cross validation where instead of dealing with one single test data set they'll deal with K subsets of test data sets possible so what they do for each subset um of on the kold they consider the rest of the data set as uh as a training data set and this one has like out of the K pieces they'll start first one this is test reminding all is take K minus one this is data sets as the training they validate with the sub test subset and then they repeat the same thing with all subsets okay of them and for each of the K you dur testing process they validate to see how much what is the uh loss by cross and there are other metrics that FC today so they'll calculate those and they'll compare those metrics are they uh among them one thing they can do um during analysis second is they can also what he says is they'll actually average or this K iterations to obtain a final estimate which is very nice so that means um we are testing on the whole data set but when we are not testing on the whole data set in one go if you test it on the whole data set one in one go what will happen is like the in the hold out if you use the whole training data set it's is ideally not possible to use whole data set as train data set and test data set it will definitely over fit because you're trying to fit in the whole data you're trying to test on the whole data and it'll try to fit in for the whole seta you'll go get into our fitting so so but kfold cross validation uh is actually being tested with K subsets and uh because of the K subsets uh that we are using the uh the

0:9:14 -  validation is on uh all of them and we doing an average of it so it's a better validation than using one single sub as a training test it makes sense kold stratified kold cross Val validation set so these are all these are all papers I guess somebody would have published kold cross validation and then somebody would have actually optimized it as a stratified maybe stratified I don't know what satified word means here is um similar 2K fold cross validation but in ensures that each fold has same proportion of class labels as the original data set for classification does with imbalanced class distribution to ensure that each class is every okay so the um issue with the training and test data set if you if if you just like let's say you have um one to 100,000 100,000 records as your input data you said 80% is um test training data and 20% is test data um and you say let's say 1 to 80,000 I'll take it as training data set and from 81,000 to or 80,1 to 100,000 I'll take it as test data set then what would what happen is there is a chance that your test data is at 801 to 2000 for to the 100,000 80,1 to 100,000 dat set has more um positive cases uh than negative cases so what happens is your test data is biased towards the positive Cas your validations even though they are B of positive and negative scenarios it is still there is there always some kind of a uh bias towards your test validations it's not accurate so what they do and and what he says is it depends on um your data also there are certain data set that I have seen where you have the data itself has 80% positive results and 20% for example somebody having cancer right the number of it depends on on the test data like how many people you take like how many patients you take and what is the um testing and all that but let's say not cancer is a different story but let's say you go to diabetes right there a lot of people who get test for

0:12:33 -  diabetes but many of them might not have diabetes so uh considering that fact the majority of the cases will be negative there and very few cases will go get into positive so in that case what happens in your data set you have large number of like 80% are no no diabetes and 20% are diabetes so your your data set is actually influenced by no data no um diabetes than the the yes datab diabetes kind of stuff right so what happens is there's imbalance in the number of postive to number of negative so when you pick up the overall data set and if you split it especially and you at cases you could also be in a scenario where your test data train data has all positive cases around if you have only all positive cases what would happen so your algorithm doesn't need to recognize any pattern because pattern is all irrespective of the features it's always positive so then you write any equation will become correct equation so uh that's not the right thing to do so what they do is uh um they'll make sure the division is in a way that um like 80,000 and 20,000 if they are there um they'll have they'll make sure that in the 80,000 you have um you have 70 uh like you have 20,000 then you he he'll make sure that the percentages of distribution positive and negative are are same between both of them right so just just a second guys is there a way I [Music] okay okay s yeah bye sorry um I that's that's an important call so I need to take that call um I'm back what you're talking about is satified careall cross validation so what happens is now the percentages right like let's say out of this 80k uh 80% and 20% uh let's say the distribution in the training test training data also if you make sure that it's 80% 20% and then test data also will make sure it is 80% 20% that means out of 20,000 we'll make sure 16,000 are positive cases and 4,000 are negative cases similarly here in

0:16:32 -  80,000 64,000 are positive cases and 16,000 are negative cases so you have equal distribution in both training test and training data sets that's what he says in stratified K kfold cross validation so that the subset that you pick in the kfold cross validation will not have all positives all negatives can of stuff it will always have a distribution of positives and negatives so the scores are more meaning full and the scores when you do average especially the scores are on few are not in One Direction one second sorry all right okay okay okay okay by looks like uh we are not going to complete today okay let's do shop sharing this is the first time this is happening I can keep talking um but uh I need to respond on some important email [Music] um that's the whole thing is not coming in um I think that's bad today I think I need to drop off the session at least I should have completed cross validation so that we could continue with the remaining metrics tomorrow but um I think it's an aent thing that I need to attend to uh leave one out cross validation does Mak sense repeated careful cross validation I think those are all more multiple optimizations on top of kfold validations of this there are not different cross validations but there are different kinds of kfold cross validations itself but um okay but I think they are effective in some scenarios and we need to understand in with scenario which one needs to be used that will be um that will be more effective so but again if you go to kagle competitions and stuff you should have alignment that guy if he's doing on something we should also have you should also measure on the same thing if that guy is doing validations basing on certain process you should also align with it or else you will not be having similar scores when you're validating it locally to validate when you submit it to competition or other person who's validating same thing happens in servers also like you submit the model to a server and then if it's validating on the server the server validations and you should be on alignment otherwise it will not be on same page okay uh fine