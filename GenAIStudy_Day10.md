# Day 10 , Study Time Gen AI

**Time Interval:** 00:00 - 32:12  
**Summary**  
- **üîç Overview**: The session continues from the previous discussions, focusing on deeper insights into **decision trees**. The instructor introduces calculations related to **Information Gain** and its significance in the decision-making process.
- **üìä Information Gain Explanation**: The instructor clarifies that **Information Gain** indicates the reduction in uncertainty when a dataset is split based on an attribute. Emphasizing its role in selecting the most effective attribute for splitting, the instructor shares the formula used to calculate Information Gain.
- **üß† Understanding Entropy**: The concept of **entropy** is discussed in detail, highlighting its importance in measuring the purity of a dataset. The instructor explains how entropy is calculated for root and child nodes, stressing that lower entropy indicates higher purity.
- **üõ† Practical Application with Decision Trees**: The session outlines methods used to efficiently split datasets into manageable parts using linear regression principles and classification strategies. The idea of ‚Äúdivide and conquer‚Äù is reiterated as a key approach in this process.
- **üéØ Selecting Features**: The instructor notes that feature selection is crucial for optimal decision-making in trees, with categorical features generally providing clearer splits.
- **üåü Optimization Techniques**: Optimization strategies are discussed, including reducing features considered for calculations while maintaining accuracy and preventing overfitting.
- **üöÄ Next Steps**: The session concludes with a preview of future topics, such as **random forests**, and a commitment to provide further insights into decision-making processes and optimization methods in machine learning.

# Transcript 


0:5:25 -  hey hi everyone um good morning so today we will continue the dentry this is day 10 and uh we'll continue the day entry and then get more details on the to understand it better um so we'll refer to the uh we refer to the document that you're preparing okay I should share the screen sorry [Music] oh yeah I can here good let's get started let's look at the document okay I think we took a sample we tried to calculate information again um small correction on the Information Gain formula The Information Gain formula is not this it is missing the um one of the key thing is [Music] um information G formula goes like this entropy of um parent minus sum of weighted entropy of children [Music] f for 8 is under 38 okay so um there are a few things I found out actually I've been I've been been looking at things but I'm not um studied it but uh you think I looked at is um uh and on what we learned with respect to di trees are it's nothing what simpler that if the diagram that if you remember we are actually applying we dividing we are like going with the principle divide and conquer so what we are doing we are we are using linear regression um and we are dividing the uh overall data set into smaller pieces and then and and then conquering them separately um so what you do if if you remember the diagram that we have seen the other day you split the overall data set basing on one feature with certain value and um break it when becomes two parts these two parts we we try to um classify them separately so there are two smaller problems to solve and then you you it depends on whether you your destination

0:7:55 -  is to classify it into one particular class it could be um Apple orange something that so once once one of the child nodes uh reaches certain value we stop there saying okay we've reached it so we Traverse the other node till we uh reach a conclusion so in order to take a call on which root node to be selected which split is the right split to go with uh to do that we calculate um same in in linear regression for for uh classification we we sorry for logistic regression for classification we calculate error through uh entropy right same entropy is used here and um entropy of each the root node entropy of the child nodes and basing on these all three we we calculate um we calculate something called Information Gain which is nothing but how much which reflects um how much the entrop is being reduced because that's our objective right even when we do listic regression and then if you do gradient Des something like that R regression with uh cost function and stuff our objective is to reduce the um overall loss as much as possible so what we see the entropy if if we are at root node and that's our solution like one line is our solution then what would be the uh entropy right so and we try to reduce it so uh entropy of the parent node minus if because of splitting it into these two parts how much the entropy is reducing we'll we'll calculate it and uh we'll pick up the best plate um basing on which the entropy is uh reduced more right so if the uh Information Gain when I say they um are we we we also use the word impurity impurity is where uh for a selected list of values that are there the more they are mixed up so like the more they are having a combination of the both apples and oranges to something which has more

0:10:36 -  specific Apple sorry most oranges that is pure so we travel from impurity to Purity right so we travel from X loss to reducing the loss as much as possible kind of stuff so that's what Information Gain is so that's one way of calculating there is also another way of calculating uh the um so the other way of making a decision on uh how which split is the best split to go with and what is the feature and what is the uh value like so given overall data set we can say X1 is equal to 1 X1 is equal to0 or we can go with um X2 we can go with X3 there all combinations are possible right so actually I was spending more time on how do you pick up this feature how do you pick up this value itself that's that's the key thing right because um it's not easy you have so many features let's say you have thousand features and then there are they might have continuous values they might have categorical vales continuous values categorical values means where you can um classify them saying okay for example um somebody uh somebody's not sick we trying to predict something let's say and we we'll say whether did he have Caesars or not so that's it that's more like s or no right um or it could also be like um um not a good one but how many um how many war thingss did he have right so uh it let be relative like how many did he have in last 24 hours something like that then we'll have a number like it could be 1 2 three or something like that so the values features and values will be different so the categorical will be where you have uh um you have like whether he had favor or not 01 how many one things do you have that is a continuous values it could be two it could could be three it could be any number we don't know what what the number is so the number can be like regression Series right regression has continuous values uh whereas U uh classification problems are categorical values where can be indicated by CL example apple and orange is one class like class one class zero class one kind of stuff right

0:13:13 -  [Music] um so um yeah for for I I think if it's a categorical feature we are kind of good right so because what happens you you pick up either X is uh X1 is zero or X1 is one and it gives you a split so two values that you need to verify you may be calculating the um entropies you may calculate the information gain and stuff and you you you compare the Information Gain between X1 is equal to 1 X1 equal to Z and then you can make a call right uh but whereas continuous values how do you take a call that's a big question right so [Music] um there are so I think there are initially I think what they did is they blindly went picked up uh values and then they did for example X1 is continuous values like 4 7 24 2 something like that continuous values are there they picked up each value like X1 is equal to 4 let's say if I draw this this value and and they calculated it seems for all possible values that means if there are 1 million records They calculated for 1 million which is like big calculation right that's like time taking a lot for me for some reason that doesn't didn't look appropriate because there are so many values how do they so I felt they would have come up with some optimization with it [Music] um I find few optimization but they're not that great but the way they are I'll I'll tell you a few optimization that I've seen um so one optimization instead of picking up each value they take an mean of each values like there if there is 4 10 20 4 + 10 by 2 is 7 so they go with 7 10 + 20 by 2 is 15 so they'll go with 15 like that so instead of going with each value they're reducing almost like uh you can say almost like half right so matter of fact not even half it's maybe if the r Fields they may reduce it to n minus one that's all so uh not a great optimization right then another optimization is they said um if you have list of values and the corresponding classes you you sort the list of values and then you you pick up

0:15:46 -  values where there is a transition that is let's say the output values are classes one and zero and if you see list of ones and then become zero then that you take a split point so that our objective is to reduce the impurity so what happens if you take that line the the the top funds whether it could be one one or 5 on 6 ones or whatever assuming the output is one and zero you you take that as the left node and remaining as as the right node so but still if you take a 1 million records you don't know the first one can be one second one can be zero let's say assume that 1 0 1 0 1 0 is are there for some reason worst case most probably then you you get split for almost same as the previous and minus one spits kind of stuff right so that is those are the findings that I had I I I spent a lot of time on this to find more optimizations but U um I couldn't find um more um but I think the objective is uh the through which the people are thinking is categorical features are the best ones can we that we can use so how can we convert a continuous values to categorical values um maybe classify it into uh categories basing on the Range like um you take something where um the values are ranging from 0 to 10 maybe they will if they want to classify as two values basing on I I mean maybe they want to classify similar to the output and they they might classify them as 0 to um 3 3 to 4 to 6 and 7 to 10 um possible and the three classes are 0 to 5 5 to 10 right um I that's one op that could be an optimization technique but the accuracy of it or is it the right thing to do may not be because um maybe ones and zeros are majorly in 0 to five and you are labeling all of 0 to you're making it categorical and then you're putting 0 to five and it might not be an optimal split for you right so even if we do we need to repeat it

0:18:20 -  you can't just at uh root node level let's say you go with 0 to 5 and 5 to 10 um after splitting it into two values again you need to pick up all the values and you need to split it like 0 to 5 should be further split and then 5 to 10 should be forther split I me this is my thinking of what one optimization that could be done um to convert a continuous values to categorical values uh the mathematical calculation uh of this does it help uh and um and with respect to accuracy of the decision with respect to doing it each value based is there any difference I I feel there might be a little difference but I don't want to get into the those details it will it will take more time but I am sure there could be a lot of optimization that we can think of right another um way of calculating this the overall decision making of the split is on Information Gain there is an Al also uh uh another one they use called zi index uh zi index is 1 minus uh sum of uh probability at uh I Square so this is easy um for a given um node they they calculate probabilities of each uh value and um they Square it and then they add it when I say square of probability of each value I meant to say not um not each each value as in x1's value or X2 value or something that they calculate probability of each output the focus is always an output uh not influen much on the input kind because our objective is to find out the output right so what we do you g index um you calculate the Gen index of the root node and then you calculate I have an example that I asked CH GP I can show you guys which is nice actually it gives you a good understanding I just asked take two or three features and try to build a tree

0:21:20 -  using gen index kind of stuff so it has picked up features feature one um it has taken more categorical so the calculations are easy but for us also it is easy to understand let's assume the feature one is these values feature two is these values and Target is this values then um the approach is to to calculate the uh Gene index for root node root node has all the values as is it's not split and then when you split it basing on certain feature the possibility is feature 1 is equal to one feature one equal to0 feature 2 is equal to 1 feature 2 equal to Z there four four possible ways in which you can split it when you split it basing on each of them the target is split in different ways so you calculate the Gen index for each of four each four of them um and see how they impact and what's the right split to do so for root node the math is uh there are four ones out of eight so it's 1 - 4 by square + 4 by square so one probability of 1 probability of 0 whole squar right 1us probability of 0 whole Square probability of 1 whole Square equal 0.5 that is root node now to select a split on which split is good feature 1 equal to 1 that means here if you pick up feature 1 is equal to 1 it is here here 1 1 1 0 so 3 ones and zero on one side three zeros and one on the one side right um so feature 1 equal to 1 uh one you get class zero two times out of three um let's look at this so split on feature 1 equal to 1 let's look at that so if I say feature 1 equal to 1 feature 1 = to 1 then for me what happens is on on one on the one side you'll have uh 3 ones and one zero and the right side you'll have three zeros and one one correct the probability for uh both are actually same divisions the probability will be similar for both of them so this will become uh technically it should

0:25:20 -  [Music] become why did he split the whole 10 into three in feature is equal to 1al to 1 is again 1 2 3 4 something wrong with this calculation is what I'm thinking okay let's see so it will become 3x 4 and uh 1X 4 should be like this 3x 4 1X 4 for each node one and then for each not you should have supposed to have you should have supposed to have two nodes so 1 = to0 he says three and five split 4 okay feure is 2 and six split and here also if you see okay recalculate is supposed to be feature one and feature 2 if you look at it 1 0 1 1 0 that is 22 this is okay one and uh 2 means um H 22 only [Music] feature subset 1 is 111 subset 2 0 now this guy is doing definitely wrong feature two see the it's wrong because it's supposed to be four I mean sum of the subsets should be eight correct how is feature 2 equal to Z only three values on the left side and three values on the right side how is that possible and what happens to the remaining two values as if they're unknown so okay chat GPT is going wrong here but um but if we do on our own what happens if feature 1 equal to 1 if you take it then uh what would happen is it will it'll split it into uh 3 comma 1 sets where 3 1es and one0 3 zos and 1 one so it will be 3x 4 1X 4 so it's 1 - uh 3x 4 1X 4 whole Square so it is

0:27:55 -  um 1 minus um return by 16 5 by 8 6 480 uh 226 45 0.625 1 - 0.625 and then that will become 75 375 0.375 so here if if F1 is equal to 1 is one line 0.375 is the G index F1 is equal to 0 the split would be um1 is equal to 0 then the split would be 0 0 and uh 1 0 similarly for obviously if it's same thing I think it's going to be F1 is equal to 0 is also going to be 0.375 uh and F2 is = to 1 will be um I think 3 two zeros and two ones is what we have seen right 0 1 1 Z yeah so it's two two zeros and 2 1's two Zer 2 1 so here also it will be same but 2x 2 is 2x 4 S + 2x 4 S that is 1 by2 s 1X 4 + 1 by 4 is 1 by 2 1 - 2 is 0 point wherever there's equal Purity the it will become 0.5 as it similar to our root node and so comparing these two I think we have a the least gen index is achieved with either F1 equal to one or F1 equal to Z and you can pick any one of them and move forward that will be the scenario let's say we we pick up F1 equal to 1 as our root node then um uh you have two splits one is uh 3 ones and uh uh one Zer another one is three zeros and one one now again we can uh for each node we should separately take it and then think of uh uh splits uh we F1 is already classified so you we can't take F1 F1 will be singular so we should go with FS2 and uh so we should calculate for F2 is equal to 0 and F2 equal to 1 for each of them them

0:30:34 -  um one for each of them so let's look at um this is little tricky because um yeah but um we'll we'll take it feature one one on the left side feature one is one right that's already the root node so it like let's pick 1 one one one then feature two corresponding values needs to be picked up it's 0 one0 1 one um one Z again and one one and corresponding Target values are uh one one and uh one and zero right coming to the the other side of it is 0 0 0 0 feature one and corresponding values are feature 2 is one um zero I think all rest of them are zeros oh no one zero yeah it's it's equalent spit in feature two and the target uh values for this will be um 0 1 is 0 0 0 is 0 0 1 is 1 0 0 is 0 okay that's a good um now on each of them we should pick up feature 2 is equal to 0 and feature 2 is equal to 1 if feature 2 is equal to 0 we get uh feature 2 = 0 and feature 2 = to 1 one both should be calculated each is equal to 0 is a split you get 1A one on one side 1A 0 on side feature one feature 2 is equal to 1 if you pick it up you you get uh 1 0 on one side and one one on one side this is for the left node coming to the right node if you pick up feature 2 is equal to Z um Z you get 0 0 on one side 0 one on one side and the feature 2 = to 1 on the right node it'll uh become 01 on left node and 0 0 on the right so looking at

0:33:10 -  um this thing um you we are getting few Leaf nodes um once you get uh Leaf node what would uh happen is I think um 1 minus okay anyway let's let's just do it g Index right 1 minus uh 1 by 2 whole squ uh no no sorry uh + 0 by2 S 2x2 sare + 0 by2 sare if 1 for 1A 1 if we calculate it's 1 - 1 is this is zero so Z index is zero for uh the most uh uh it is it is it is 0.5 uh for um for the most impure and zero for the most pure it is 1 1 has Z 0 comma 1 will have uh have zero uh here it is 1 Comm 1 is also zero so few we can 1 comma 0 will be um 1 by 2 1 by two it's that will be 0.5 and uh we have 0a 1 that's also 0.5 0 0 0.5 so obviously uh we should both are of same so you again you can take any one of them here let's say you you pick up F2 = 1 because we picked up F1 = to 1 uh let's pick up F2 = 1 for both sides then uh it will it will PR it as uh 1A 0 and 1A 1 and here it is 0 comma 0 and 0a 1 same 0 comma 0 and 0a 1 Leaf notes need not be split because they're already on the Last Mile and then 1 comma 0 and stuff now uh oh interesting 1 comma 0 and 0a one uh that are there with us uh that is uh let me Mark them here um we we're picking feature 2 is equal to 1

0:36:18 -  right and uh these two and here it is uh feat to equal to my bad feature 2al to 1 is actually making a good split here on the left side it it makes uh 1A 1 on one side Fe 2 equal to 1 and the remaining three zeros on the other side side maybe I've written it wrong way doesn't matter so you you get further splits there and I think um 1 comma 0 once you have here um oh okay so not always basing on this is interesting right what happened so X1 is 1 X2 is 1 and and output is one X1 is one my um X2 is uh one output is z this two combination is there then you get 1 comma 0 on the one of the last node you already have exhausted x 1 = to 1 and X2 is equal to and if you go with X1 = 1 rx2 = to 1 both combination you see that uh the input values are 1 1 one 1 1 1 0 you can't split it further right because the only possibility is X1 = to 1 and X2 equal to 1 and both of them will give you same Leaf node it can't be split further so uh the final Leaf nodes can't always take full Purity that's a good learning um so you you might also end up in a set which can't be split further into two pieces oh interesting good good learning nice so that's how this example goes basing on gen index can this is there are two parts when to use which one uh if you have more categorical you go for Gen index if you have more continuous values you go for Information Gain um and um for the same example that we picked up if we go with information again what what happens is the Information Gain will give you um a solution which is more overfitting um than gen index um um I use the term overfitting that means um it it overfitting and underfitting is nothing but overfitting

0:39:5 -  is you assume that um the values that you see itself is a universal values and then you you um your algorithm is more biased towards those values so what happens when you get a new record and for some reason if the new record is outside the scope of the training date seta then um that will predict wrong because you are assuming that whatever you have is the universal set there's nothing more outside it so you you you overfit to that particular set that is given to you so overfitting will reduce your probability of predicting the unknowns properly underfitting is where you you don't you you even if you give a value that is there within the trained data set it will not predict properly so the unknown maybe it will predict properly but predict it may or may not but for even the training data if you repeat it also it not predict properly objective is to go in between where uh it it should not predict correct values for all the training data sets so that's a tricky piece part of it so that means tomorrow when you get a training data sets value uh it it it should not 100% predict a prop right value for all of the values that is how we can actually uh um that is how you can actually predict saying there is the unknowns right because unknowns are different values from this and and if this is over fitting to this then you you you don't predict properly on the unknown values kind of stuff so that's with the decision Tre I thought I'll cover a little bit on um random Forest uh we write code for one of them to so let me complete it like let me take five more minutes and quickly complete it random Forest is is nothing let us just ask uh what are random for us and uh why do we need it emble learning technique okay I I'll say what is Ensemble learning Tech Technic emble is nothing but um so the is more divid and Conquer is what our learning was right way where we divide the data sets by using a model at one level like you use one linear regression or logistic regression at one place using which you split it into two

0:41:42 -  parts again you use another model so if you look at it if you if if you say that this one algorithm and even if it's one line you call it one algorithm right so so you what you did one one line and and Childs and then you use another other lines one other algorithms and uh different ones and and then you you you keep splitting you use multiple so you use multiple models together and uh um together these models actually has given you a specific output so emble uh learning technique is where you you do the same thing by using different models and the models can all so here what we did we were using um logistic regession for for decry classify fa there is also Deary regressor also which helps us in predicting a value um where you you split it you you and and then yeah obviously you don't you can't predict because the output data set is regression is continuous value so you you can't predict an unknown value but what we what we do is the known values will be taken and and you um if you have split to certain level in desity regression we take an average of those Leaf nodes or something like that and um we move and that we'll as we'll put that as prediction so my take would be Deary for classification is looks much appropriate than regression but it is possible to have regression but coming back to random Forest you can actually pick up multiple models it could be from assembling techniques it could also be different models for example you take um but it should be similar you can't take one linear and one logistic doesn't make sense because your prediction your your model that you pick is trying to predict the same thing right like one can't be classification other so um it doesn't make sense but for example you can take logistic regression and decry POS ltic reg is one algorithm the is multiple listic regressions put together again similarly Deion Tre and itself is combination random Forest is again emble learning Tech use for

0:44:14 -  classification collection of instead of going with one single de Tre we can go with the collection of dish Andries and we calculate the output of U uh these dries and uh basing on the output of all the dries for example let's say if you take 10 dries and six are output of the dri six of the dries says for given unknown values it classifies them as apples four of them as oranges we take go with whatever is classified more six is higher than four so you go with um apples if if there is 5i you you pick anyone or um so you you pick up the majority kind out of this so it's as simple as that there is nothing very complicated about random forest with respect to same Deion trees but you use random dises but as I said the the complaint on De Tre for me is you um the math is pretty um hard to calculate right so where you have th000 features and then you have 1 million records so you need to repeat your calculation basing on the splits and stuff you need to do lot of 1 million uh 1 million output points on the root node you split them um again you need to go to thousand features all thousand features you need to calculate before taking a call what should be on the root node and again you need to repeat the same thing for the below so this is definitely a huge exercise considering that fact how do can we optimize it is one way especially if you take all thousand features and alls you might end up in overfitting the particular decry so uh one of the technique is to not go with all thousand features instead in when you go for random Forest you pick a subset of features and subset of values for example out of the Thousand features you can say I I pick up 50 fees and then build one tree and then I in of 1 million records I'll take a subset of [Music] um let's say thousand values right uh and then you you build on top of it and using those subsets so if you take um

0:46:42 -  100 features out of thousand you you you can split it into 10 10 set of features right so 10 set of features and you you correspondingly randomly sample records from the 1 million records and you train on top of it right the the lower the I mean the lower the sample set the less accurate the predictions will be so and the lower the features the less accurate the predictions would be but what we doing we you're using an assemble learning technique so um we are we are taking a subset of the features and subset of values and then we we are conquering them each of them separately and then we looking at the aggregated view of it so um this is giving as as it says why go for random higher accuracy robustness uh to or fitting feature importance we and we sample the features and then look at it um we are not splitting we are sampling the features so we might also have repetition of features few in the few of the trees which is also possible um handling of missing data and outliers it seem I don't know how it handles that robust to missing data and out layers they can handle data sets with missing Val without the need of imputation and out layers have less impact on the overall performance due to the emble nature of the model okay yeah parallel training which is which is good because you you even in De trees right you split once you split it this two becomes independent you can you can test both of them you can work on both of those trees separately then it becomes four uh like 1 2 2 2 4 and all let's say if it becomes they assuming you don't find a Le node then four can also be par executed so similar random Forest if there are 10 dries that you pick up as random Forest then 10 of them can be par um versatility random for can be applied for both classification making them versatile for range ofse learning problems okay or are powerful and flexible okay let's ask last question before we close it what are different uh variance of random

0:50:10 -  forest with different optimizations provid a table list with a brief on each of them Forest extra trees there a lot of Varian on random Forest because different people would have come and they would have [Music] um given different variations of it on how to pick up because once you say random first you're picking multiple de Tre so there are lot of questions right how do we pick up this how many multiple trees can we pick up what is the um um how how does the decision making overall happen right that's the uh key thing on um number of how do you split into number of Tes how do you make the choice of features how do you make the choice of number of values basing on uh on that and then optimizing it further um spits are chosen randomly without optimization extra trees totally random random with no optimization isolation Forest an anomal detection method based on random for isolates anomalies in the data okay rotation Forest random Forest where each St Tre is trained on a random sub Subspace of each other um feature space okay extremely random variant random streaming updating models incrementally yeah so xgb extreme radi boosting and R boosting machine are both uh are both anable learning techniques that be can of gradient boosting algorithms while they share they also have distin characteristics XG boost is optimization optim as for Speed and efficiency it implement parallelism tree pruning cash awareness whereas GBM is not an optimized as X used maybe slower especially dealing with XG boost includes regation techniques XG does not have bu regulation techniques [Music] use oh so I was actually thinking xgb is always extreme boost is only applied [Music]

0:53:11 -  forar paration so xgb and GB they applied only for Rand Forest are not specific to random for they both boosting to various types of models including de trees linear models typ de trees lar models other Bas various tasks machine learning T including classification regression ranking uhgb and gband and can be applied for anything I this logic I was thinking that xgb andg gbmr specific to random for good point and I applied them also without knowing this but this is this um extreme gradient uh boosting and gradient boosting all machines are are actually a widely used techniques um obviously actually the sounds much better than CBM so people don't use CBM they go for uh xgb XG boost and XG boost is widely used in problems solving in kaggle and stuff which um for me that was actually a winner for my kagle competition kind of stuff so it's good one but again we need not rely on as as we understood about uh emble learning techniques so um we need not rely on specific models also you can actually pick up multiple models and um you can apply these uh XC booster on top of it as well and the assemble model and that that's also um more accurate that also gives more accurate than following going with individual models kind of so these two are specific techniques so that's it for today I think we have we have crossed um time so nice we've completed Des trees random Forest uh we looked at what is different kind of random Forest that are possible and we also used what kind of optimizations or approaches that we can use with respect to X GBM GPM and stuff um that's it so we'll move on to the next topic tomorrow I'll decide on the next topic and as soon you guys know tomorrow when we when we get started okay thank