# Day 8 Study Time Gen AI

**Day 8: Study Session on Generative AI**

**Time Interval:** 00:00 - 38:00  
**Summary:**

- **üîç Overview**: The session begins with a recap of previous lessons, covering the **introduction to AI** and **machine learning**, as well as in-depth discussions on **linear regression** and **logistic regression**. The instructor indicates a transition to exploring additional algorithms relevant to generative AI.
  
- **üìä Algorithms and Generative AI**: The session emphasizes the importance of having a strong grasp of core concepts before moving into generative AI. The goal is to achieve proficiency in using generative AI for problem solving, requiring learners to understand various models and their relevance to specific applications.

- **üß† Core Understanding for Effective Implementation**: The instructor stresses that successful application of AI hinges on a solid understanding of core concepts. The better the foundational knowledge, the more efficient the implementation.

- **üîÑ Practical Applications and Model Utilization**: The discussion includes insights into how AI models can be implemented through APIs, focusing on understanding inputs and outputs. Questions of performance, such as synchronous versus asynchronous calls, are also addressed.

- **üå± Model Development and Customization**: The importance of understanding different AI frameworks is discussed, highlighting the need for customization based on specific use cases. The session notes that familiarization with available components allows for efficient model development.

- **‚öôÔ∏è Building Proprietary Models**: The instructor outlines the steps needed to create proprietary models, particularly those trained on extensive datasets. The emphasis here is on fine-tuning models for enhanced performance, considering both generic and domain-specific data.

- **üìà Performance Metrics and Optimization Techniques**: The conversation covers key metrics for model tuning, including accuracy and resource requirements, reinforcing the notion that models should evolve continuously based on incoming data.

- **üîÑ Keeping the Process Ongoing**: The quite personal analogy between model development and mentorship is made, reinforcing the idea that models learn and grow over time, necessitating a commitment to ongoing maintenance and adjustment.

- **üöÄ Next Steps**: Forward-looking, the conversation hints towards a focus on decision trees and random forests in future sessions, underscoring their relevance in both generative AI and traditional machine learning frameworks.

This session embodies the foundational principles that underlie effective application and development within the generative AI landscape, preparing learners for more advanced discussions in future classes.

# Transcript 


0:2:40 -  hi everyone good morning [Music] so this is uh day eight and um what we have covered is Introduction to artifici CH theaction to machine learning and then we have understood linear regression and then we studied logistic regression um we'll move on to the Third Kind of um algorithm um I would like to there are multiple algorithms before starting this video actually looking at what are all the the model because thinking was to see how many model should be learn before going into generative a because is our objective right like to gain enough expertise in gener way so that um we can um the direction is to uh for myself again the direction is to have a Genera a applicability so that um I use generative AI for problem solving um that that's the expectation by 50% and for which I believe to have efficient application of anything you should understand the Core Concepts in it the better you understand the more efficient way you can implement it it's not just implementation um implementation is see I think people needs to understand this that is implementation means um um it could be at different levels right so somebody would have built a model and they would have exposed an API so you can actually simply uh understand how to call that particular API what are the inputs that you need to give and you you run it so there's an AI model depends on the size of the model the time it takes and stuff it could be a synchronous call a synchronous call which is again that's a different ways of interaction interacting with that particular model but you call that model the model gives you an output so is that um is that generative AI yeah that's also generative AI you're solving problem using it right and um you you create a so that there's like a chat to create a

0:5:11 -  chat that's good enough you create a context um if you need to set some additional information on the context you can set it then you can create a sequence of um questions or Journey for each user different Journey parts for different user and then you can actually interact with the model to respond basing on the particular context particular questions that the user is asking and stuff we can actually set up um the overall path right is simple application of um AI but um um and you there is not I mean to the level of understanding that you need to have on generate way to implement something simple like this is um the is is bare minimal right but um um our application um might need um I mean this application that we discuss just now might not be sufficient uh for the real world applications and stuff so there may not I mean the assumption is the model is already fin tun you have a use case and it fits in perfectly and then you can use it and then um you can take it forward but things don't work like that right like your our use cases will be unique the models might not be a right fit so yeah of course we need to go understand what are the different models which would be a right fit for it um and what is the accuracy of each model um how much how many resources are required for the to execute that model um and uh um so uh how this model can be uh fine tuned as per the requirements that are provided and so to the to to fine tune a model you need a very deeper understanding of the specific model uh that is one thing second is um the different Frameworks uh we should understand those Frameworks again to utilize that Frameworks in more efficient way on r2c uh what are all the components that there um so you can within the Frameworks what they give is more like a plug and play where sorry so where you can um look at

0:7:46 -  different where you can look at different components that you stitch them together like to achieve some things right so you can put them together in a flow or something like that only if you can understand each component in detail on how to use it and um how it works and uh why would why people would use it so if you have answers clear answers the the the more deeper and clear answers that you have for these Concepts the better you can actually uh use them and uh take it forward so um that's 50% of application pie part of it that I've discussed right so another 50% is is actually maybe um we might need to uh build a new model new uh um model um which actually um it's more like a proprietary model people don't want to see if you especially if you come to gener models right the key thing is it to train it on some huge amount of data right like language models are nothing but um language model are trained on large data that's all so it depends on large data that is been that it has been trained on right let's say somebody trains on Kora somebody trains on stack Overflow somebody trains on GitHub repositories like that there could be a number of things on which we as be trained and then the answers that a model gives is always influenced by the the input um data with which it has been trained because that is always there in the memory of that particular model so it responds basing on the on the training data that is fed into it the the more the training data is of specific category the responses are fine tun for it so there could be a scenario where people it is it is more sensitive uh application where the accuracy need are high and you can't hallucinate and all that stuff right then you need to um build a model maybe from scratch with the relevant data that is very specific to this particular application um and maybe necessarily need not necessar be generically with this it could

0:10:21 -  be it could be um huge amount of generic data plus domain specific data so maybe domain LM needs to be uh uh created from the scratch they needs to be optimized um they they should be made more efficient because um people will come back on requirment saying this should have this much of response time should have this much accuracy those details will be on the table and we should achieve it uh unless we understand what are all the techniques that are there and um how do we pick up which techniqu make um what kind of optimization techniques you will use what kind of embeddings you'll use um and uh using all those and building these models um what what kind of limitations do you have with respect to retraining how do you see the model evolving it's more like for me the models are more like kids you you you um design and create it basing on your basic you created kid and then the kid learns and evolves and grows the more it is exposed more it experiences things and stuff it it more the knowledge it will learn and then it keeps evolving it its capabilities uh will keep growing and um so that's how we need to design it so to design with that kind of an extendability maintainability is is very critical because developing of a model happens through a certain period but maintaining it is a lifelong journey and if people are not able to maintain it people are not able to understand what is being done in it and um if it is not open let's say there is a future optimization that comes in which is which makes it much faster much better exposed to the current data and stuff we should uh um we should be able to plug it in seamlessly without uh rebuilding um everything from scatch or retraining for example if you have trained it on some hypothetically but let's say one pabit of data and then we we made some change and then now we we we can't sit back and say okay I have changed the whole concept like something like ambing or or some other step within the

0:12:55 -  generative AI model building if I change it we can simply say we need to read train it like but imagine um storing uh embedding and training on a 1 paby data uh multiple times would cost you a lot right at least at this point of time it cost you a lot I don't know eventually maybe the cost come down and one PAB is maybe it may be it might become nothing the compute power that is required to could be nothing I don't know but um um how it evolves but those are the things that we need to keep in mind to address Uh current um implementations and also we should keep in mind the future implementations right so that all being said so there are two sides that I spoke of one is application of generative Ai and one is getting into the core of generative AI where we try to build models and stuff to be efficient in both directions we need to have sufficient knowledge applications is one side of the game where um and um from the overall levels of understanding that you need to have on generative AI uh applications would need some level of understanding that is required um but whereas if you want if you get into the other 50% where you focus on building the models and fine-tuning them um building domain speciic models or something like that then you need much more deeper in the stack of if you if application is is on the top if you understand this and of course the foundation the three layers right the foundational layer is mandatory for everybody the the top layer and Foundation layer this these are understood the application of Z AI is good but whereas if you want to get into um uh other use cases like you want to build LGE Lang models and all that stuff then you need the the combination of the found and um you need the combination of the middle layer which which gets into the real uh details on how these models are built and stuff and obviously the third layer third layer of application where you have the application components and stuff those are also important for second layer so all three layers are important if you want to be on the on the 50% side where you building the models and stuff um

0:15:30 -  and one two three layers if you see one and three are together at least important to be on the application of a models seeing all that working backwards um the foundations are important but what needs to be considered as Foundation is the key question that I was actually thinking through today morning because there a lot of things right the Ai and machine learning that we that we look at um now has evolved over years many many years right so considering that um there are lot of Concepts there are a lot of papers submitted there a lot of models that are that have come out and um um I mean models wise if you look at it there will be like maybe millions of models right because everybody had different use case at that point point of time whatever model they have they would have taken it optimized it and then come out to another model um and and optimization also they would not have arrived directly with the best possible optimization like like I said they they would have CAU from loss function then they created cost function uh then they they got gradient descent like that there are different stages so um technically do you need to understand all those Concepts we don't need to but if you understand the journey if to understand different optimizations the destination that these people have reached they would have reached they would have started with simple simple linear regression they would have Ed in complex neural networks like RNN CNS or something like that to understand these better you should actually have a little bit of view on the journey in which it has evolved um so but how deep you need to go into it depends on time you have and um stuff so lot of people who recommended uh uh whom we I asked on how I how I can become how I can gain expertise in AI field uh machine learning uh deep learning and and this generative AI sections andu they said you need to spend a lot of time you should get your Basics right you you can't you can't just peak of something without understanding what it is how it is implemented what is the algorithm behind it and stuff if you

0:18:0 -  without understanding it you just there are a lot of people who speak things right they don't really understand what what is behind it um they also have a point saying I do not because I don't deal with things at that level that's that's nice there nothing wrong about it but um to speak is okay to implement is another thing but to optimize or to uh create an evolutionary journey to a model and stuff the those are the things if you get into they you need to get you need to have a deeper understanding of how things work otherwise you'll not be able to think through at that level so why I'm explaining today investing a lot of time and putting all this into picture is you people should have Clarity on their long-term goal on where they are how much St they have and stuff right so my journey goes in this direction I'm clear 50% on um having the core capabilities of model building and stuff 50% on application of it's also actually not a good strategy because overall what I'm saying is I want to learn everything so uh with the time that I have with the amount of learning that I can pick up and stuff it is good to pick up any one of them and and and go with it um but my target is to go for the super set which has three layers so that I cover both of them right uh but it takes a lot of time so do we have time is the question if you have time yes learning with three layers if that's your objectives you should learn all three layers and move forward with it um but if you you don't have enough time to spend on understanding all three layers then uh you you might not be able to complete it the challenge is see you should be practical right so if you don't have enough time and uh if you start off doing it what happens is you start off you you reach a particular point and um you you are you're forced to move out of it move out of The Learning Journey because uh you don't have literally don't have enough time or you can

0:20:29 -  actually tell yourself saying you don't have enough time but um my understanding is you keep going learn how much ever you can there's nothing wrong with it there is always something additional that you learn every time you start so um just don't give up you you study for you start off you study for 5 days very excited you learn things um and there's something else critical coming in some family uh emergency there some other interesting topic that has been pushed there is work that needs to be taken care of there are a lot of things right so it's okay it's okay to pause it uh because of other things that are going on in life right you may not be able to spend and uh this much time every day so but restart it again and uh trust me those five days that you spent in learning would have given you some exper and more knowledge exper and more insight so maybe you need to restart you need to start from the Core Concepts again and then you should start building but you'll definitely be faster than you're the first time that you you you were looking at it learning it and stuff right so you will evolve over the period of time but it's it's your persistence that you how strongly you want it to be there that you revisit don't give up on it and then um you we should also learn from our mistakes on why um uh we couldn't complete it last time and how do we um increase it but to see this is a long journey so we really don't know how far we can complete it but um we should uh learn from your previous mistakes on why we couldn't continue for more days and then add those inputs uh and then move forward so I I felt like today we need to discuss this that's because uh that is what my thinking went today morning saying okay we spending too much of time on this is this good is this the right thing to do and stuff so that's the reason I thought I'll I'll talk to you guys about this which is good now moving on uh next algorithm that is there in the list that we want to cover is uh decision trees and uh random Forest um I'll see uh how far we can go get deep

0:23:19 -  into it but I want this to be this is important uh Deion series and the random for um outside Genera AI scope observed um these are widely used the amount of adoptation that is there besides neural networks uh in different forms and and um are are the Genera models with respect to Transformers and coders decoders and all that stuff besides those there is a the middle era where where these dision trees random forest and Stu they're extensively used for many models and um they are very efficient um so we'll get to it and I I also feel they are they're not different as I said linear regression will be a basis for all these um dishan trees random forests and stuff but and and um let's learn it it's is actually more crude than what uh it sounds like saying okay dish and trees random forest and so sounds sounds like more okay more intelligent stuff and stuff but if you get to the detail on what it really does it is definitely the basic thing that it is okay so let's get started let's ask CH GPT on what add is how they are formed and all that stuff so that we understand much more better so today we we might go through the entry um through so we will go through dis trees and um random Forest today um I'll take this to a new page so that so um decision trees are very I like deis trees so much because deis trees also um are closer to human thinking right so I think Whoever has uh thought of dries uh they would have they would have thought through exactly like how would human deal with with the things right so um and I think it would have definitely uh come out with the way people were dealing with the data and stuff especially okay so what happens you have when you have data and the data would have some input features um and the input features are like

0:25:50 -  um let's say as you know X1 uh one X2 X3 X4 X5 X6 X7 then what they would do is okay what kind of data is there what is X1 what is X2 what is X3 what X4 X5 for example something like um a personal a person's history on what is his age what is his current bank balance what is his uh um credit history like like that what what is his marriage status what is his present overall uh Equity there could be a number of metrics that you need to you can pull out of individual person what is he spending uh what what kind of um spending pattern he has what is his uh monthly um income they're going to be a number of features that that come in right so what we will think of saying okay if I am uh somebody who's expecting to decide on whether I should give a loan or not then actually what I need to understand is um um okay I'm like a new traine and um when I come in um I I have all this data provided to me and then there is there is of the past customers who have been issued a loan or rejected a loan and then if I were here then I would actually understand okay this is all this data so how many people are been given loan how many people are not given loan um are we can actually also talk of how did we decide on how much loan amount that we want to give there are two problems one is a classification problem is a regression problem where recog problem is more to predict on what is the amount that needs to be uh amount is is is to be given to these people and stuff right there's kind of problem so how do I think um to make this decision whether the loan needs to be given or not given whether the um amount of money that needs to be given um for as part of loan so these all decisions um they taken already then I would understand what are the features that are more influenced influenced on how much um the

0:28:24 -  decision making decision making is um for example I would I would see the amount of loan given is very highly influenced by um the uh the overall earning that this guy has and his credit score maybe those features were majorly I could see a pattern right um and um and then I pick up something like okay let's look at his credit score and and the loan issu all kind of stuff so then I can segregate and okay if the credit score is above certain number I can actually see that um uh uh I can actually see that the loan is given the credit score is less than that the pro mostly it is being rejected uh similarly I can actually analyze and see that the monthly income is at this level the loan is given it's not otherwise so the looking at the past history of data it it tells me on what basis the decisions are made correct um so um my analysis goes from understanding what are the list of features and uh what is output basing onist of features what which features are the ones which are influencing the uh output largely and then I can look at in those features each feature how is it influencing the decision um is there some metric is there some range that can that can actually give me more information on how it could be for example um if you take like loan amounts kind of stuff uh if the uh basing on the income ranges if I look at it on what is the average income range for for the loan to be around 1 million like that I can I can I can do all these analysis and then I can classify right then what I will do basing on this analysis and understanding of individual uh features and their correlation with the output and stuff I'll I'll I'll actually keep that in mind and somebody comes in and then talks to me um I mean I can use a system and then put in all the details that he gives and stuff and then we can work on it for a few days um but before I sell something sell a loan to somebody or something like that my decision making on what kind of a loan

0:30:56 -  should I put on table because there are number of loans or different sizes and stuff so I would these will actually give me a pattern of thinking where to start off first as soon as I meet I know I know the right questions to ask because um those questions give me answers the answers are actually closer to my to the features that I understood so as as I receive more data from that guy I can answer more features that I have have the more features that I can um answer the more decisions that I can make okay this guy's credit score is like this this guy's annual income is like this this guy's current status is like this current depths are like this so all those information as as it feeds into me those are more my inputs and basing on that inputs I can actually go through that decisions take a decision on the final thing right so there's a chain of thinking saying this feature is this is like this that's exactly what decision three would also do uh the decision tree looks at all the features and um but humanly you can understand the feature name you can understand the you can understand um the uh General ways on how a feature and a um amount is um correlated based on experience and stuff but whereas coming to [Music] um um coming to a model or or machine Learning System it doesn't know which feature is um more uh important which feature has uh which feature makes more sense to um more sense to get towards a an effective output right so it doesn't know so it um in in decision three what they do is they'll um like like what I said they they'll go pick uh one feature out of all these seven and uh so on XM features they'll pick up one feature and and and see how the output

0:33:28 -  decision is being impacted okay so let's say they pick uh XI and um basing on XI um and again assuming we convert everything to numerical format and stuff for I'll say XI greater than XI is equal to certain value right like one when XI is equal to 1 how is output being uh how is output uh uh either output being impacted I can say XI is equal to one or XI equal to something now the expectation in decision trees is to have uh uh segregation of the output in an effective way for example if you if you go to classif classification problems and and let's say let's talk about binary classification whether loan needs to be issued are not issued something like that right then um it would actually see pick up a feature and it will pick up certain value as the features value which would effectively segregate the output into two parts where POS and negative Parts it's part of it and um preferbly in a way that um one of them has more positive or negatives so um a one which a a value of XI where you basing on which you can split obviously you can split right um it into two boundaries if value of XI is this then your um outputs are going will be classified basing on value of XI if it is um is equal to 20 um for example let's make example say as a salary of or annual income of a person and the annual income of the person is is um uh 120k then

0:35:59 -  um what is a what is the chance of loan being issued and what is the chance of loan not being issued so those are classified and those are made two different modes and how do you pick up something which is the right value for XI to start off as the first feature with certain value is uh you [Music] you you you can't actually assign um all possible real numbers right does doesn't make sense right so people would have started they're like okay let me try withus 100 to 100us 500 to 500 something that but we don't know what's what's xact what is it range what could be the possible values we don't know right so what it does is and our objective is to train on the existing training data so from the training data we can actually get a range right seeing what is the range of values are there in the particular column you get the range and those range of values will be your Target and um in within the range also we need not C to everything see um for example uh uh years of experience if it is there um like age or age if it is there then you would have age let's say from 20 to 60 and you need not not look at 21 22 23 like that everything because in 20 to 60 if you within the particular range you can actually pick up all the unique distinct values and in the distinct values you should cover all of them maybe 24 25 is not there in the data for some reason right so if data is not there you put those values there you you don't get anything like you say age is equal to 24 and then you don't see any boundaries there on how it uh works and stuff right so that will not give you any segregation so um I'll explain it much more in detail um most probably tomorrow and uh I I'll we'll walk through a um another steps but that's your first step towards Deion uh you pick up a feature and you use certain value to it and basing on those value you should you want to

0:38:34 -  decide um on on what are the possible Journey parts that you can go forward to so that is it's like a tree right so where you have a decision to make basing on the decision you go two directions but the two directions may not be your output right like loan being Sho loan being should it could also be depends on how many features that you want to conu right you you can consider all there are number of features and could be you want to say a number of features should be considered if you consider a number of features what happens you should consider all of them possibly uh effective like you you start with x i then you pick up another one you need to pick up all features so and then only you should finally take a call on what uh is the output R it depends on whether that particular uh output that you picked picked is already giving you a decision for example I just picked x i and XJ U basing on XI being less than certain a value and XJ is greater than b value if I if I take it in the training seator all XI less than a and XJ greater than b uh gives me a decision saying all of them are loan approved then I need not look at the remaining features these two itself will tell me saying if this is the combination that is that forget about reminding all features just give him loan right so that's the beauty of um the trees that is you do not Traverse all the features that are there you can pick up um you might end up with trees where the decisions are made basing on few features also so that's the uh way de work but we'll look at how Deion I said still Deion is also in a way they they look at linear they have underlying they have that linear aggression we of thinking so how does it uh work uh is something we'll get to the detail tomorrow um I think today majority of the time we have spent on um discussing on the future plan on how generative how we are Marching towards generative a and stuff uh that took a lot of time so we couldn't cover much of the entries but we'll cover more on desan trees and random Forest tomorrow um sorry if it's taken a lot of time but I would really [Music]