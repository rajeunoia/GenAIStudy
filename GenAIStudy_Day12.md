# Day 12 Study Time Gen AI

**Session Summary: Machine Learning Concepts**

**Time Interval:** 00:00 - 37:25

- **üîç Overview:** The session recaps the previous lessons on **Support Vector Machines (SVM)**, foundational models, and their relevance to machine learning. The focus is on understanding model optimization without delving too deeply into complex algorithms.
  
- **üìê SVM Principles:** The instructor discusses how SVMs work, explaining the role of hyperplanes in classifying data points while maximizing margins between different classes. The session illustrates how points can be transformed so they are linearly separable, using functions to shift data points.

- **üß† Understanding Margins:** A significant part of the discussion revolves around margins‚Äîdefining the distances between different class predictions and how SVM optimizes these to avoid overfitting or underfitting.

- **üõ† Classification vs. Regression:** The instructor contrasts SVM with linear and logistic regression, emphasizing their various applications and flaws, particularly in dealing with outliers and model robustness.

- **üîç Naive Bayes Classification:** The session then transitions to **Naive Bayes**, illustrating its probabilistic approach based on the independence of features and how it applies Bayes' theorem for classification tasks.

- **üìä Practical Application:** An example to clarify Naive Bayes concepts is provided, showcasing how age, income, and education level influence a person's purchase decision.

- **‚öôÔ∏è Model Evolution:** The instructor emphasizes understanding the evolution of models by recognizing the shortcomings of older methods, leading to the development of new algorithms. This historical perspective is critical for grasping current techniques in machine learning.

- **üöÄ Next Steps:** The session concludes with a note on moving towards clustering techniques in future lessons, urging participants to deepen their understanding of the discussed algorithms, especially SVM, and Naive Bayes to enhance their practical skills in application.

# Transcript 


0:2:35 -  hey I everyone so yesterday we have seen sfm a little bit on how it works and stuff um the objective itself is not to get to the detail get to the algorithms and stuff let me be frank inm um algorithms actually also look a little complex so um which makes it much more interesting to get into the details but um it will be too many to be squeezed in and um also at some point we should see look at the relevance with respect to with respect to ji and these are all foundational models which are which are good because they all come with some level of optimization so they said everything is underlying Bas is lar regation only here also I would actually say it is integration itself like w w x + P which is there same thing is used key thing is WX instead of using X they are using some kind of function like h of X or G of X kind of stuff which is nothing but this soal Kel function so that the points that are there are actually shifted in a way that they actually separable through this plane instead of having all the points here in single the single plane they they are applying some function and what they're doing function which also works along the labels okay basing on not only X and also Y and basing on the X and Y they're moving those points for example all the um all the ones which are labeled green uh will come up so they they're this function that is applied on them it basing on the value y value that is what it says green right so basing on the green value and stuff all of those points are actually moved up and something on the plane which says they are red points basing on the Y value the Y value will be different for red and green right so basing on the Y value they they're moved down and um not NE there could be more points and let's say there is also blue points instead of green red there is also Blue Points so the blue points can

0:5:22 -  be bought the plane to one side of it right like some side corner then what we could do is we can have multiple planes also to separate it not necessarily it needs to be always single plane then we can keep um then we we put points up below and then and we can have plane separating this and then we can also have and Conquer after these two are separated we can actually separate with other plan the remaining points as well so that is possible um okay yesterday when we were discussing we also thought of discussing about um margin which actually makes it much more efficient and from SN perspective I don't I I didn't go into the detail on how it is formed and stuff but when when you talk of separation of the points that are there above and below let's say there is some Gap in between right like this is the gap in between these these two points right your plane that needs to have classification it can be in this space it can be here it can be here it can be here anywhere it is there in the space in between it actually classifies them well but to be more efficient what they have come up with this you pick up a line and your metric on picking up the line is you look at the loss right so it is supposed to be the the actual y values supposed to be here to this points are above and the actual y values are to be this points are above but you're picking up here this line in a way that the overall loss with respect to actual to the predicted value is minimal that's how you pick up that particular plane as two points are on on either sides what happens is the the line going here are the line going here will have more loss so as the line moves to the center the loss will actually be minimal obvious because Bas I mean not necessarily but basing on the distribution of points and and and stuff it depends on the point distribution of point this stuff so what it does is um it actually in the spvm again not getting into the details it it tries to so if you draw a line if you draw a line

0:7:57 -  like this or a plane like this the these distances in between the top layer to the line and the distances between the bottom layer to the Line This is called These are called margins right so that's the distance between the actual values and the predicted values in a way that the loss is loss is minimum so that will be adjusted in a way that this these margins are maximum right so the line actually as I said the line can also go from here the points are above and below and the lines are actually line is Little Closer to the Y what happens the bottom predicted y happens is uh it is little bias towards this prediction right this is let's say this is negative prediction this is positive prediction then if the line is closer to this that means the lineer actually predicting values which much closer to positive or if it's going the other way it is it line is much more favorable to the um negative predictions right so either wayse is a problem so the optimal way is this margin if it is margin considering the distance between these positive points and negative points if it's maximized then it is actually like it and it is going in between right so it is not it's not overfitting to either of the class classes that are there neither of the values that are there and um that makes it a good model which has optimal fitting and um so the prediction capabilities of such a model are much higher than our actual normal linear regression process and stuff right linear regression process is given a points you just put a line in a way that it can classify them properly optimally with minimum loss in classification after classification regression you have minimum loss um with respect to it and then you we live with it but um um but we don't know whether we are actually overfitting or underfitting to the expected patterns right so the expectation is you're not supposed to overfit overfit is good for training data but what happens is it creates a loss in the test data or when you are actually predicting with unknown unknown data that comes on to the table so we should avoid or fitting at the same time we should not be in under fitting so so

0:10:58 -  the optimal way between is the target state for everyone okay that's with okay let's ask chat GPT I thought of asking chat GPT also once saying [Music] um what is [Music] the type classific class okay type regation is obviously because I not considered logistic reg he saying regression Andes classification so to find the hyper plane that Maxim separates classes or maximiz the margin between classes in a high dimensional space okay aims to find the best fitting linear relation between the independent variables and the target variables okay ASM produces discrete outputs class labels for classification tasks and outputs for regression tasks linear regression produces continuous output that represent Productions estimations or Target value because he's getting into the so-called regression to classification kind of uh comparison so can capture complex nonlinear relationship using kernel functions allowing for flexible decision boundaries models relationship linear making it less flexible for capturing nonlinear patterns okay loss function svm uses a hinge loss function for classification and Epsilon insensitive loss function for regular tasks mean Square loss function you know that which penalizes the square difference between theual values as optimization involves solving quadratic programming problem to find the optimal separ separable separating hyper plane or regression function linear optimiz involves solving system of linear equations using techniques like ordinary least squares are gradient descent okay out layers due to which focus on maximizing the margin between classes okay D regression can be sensitive to outlay as it aims to yeah this is the robustness point is actually the square difference robustness is where we where it is talking about um the the concept where I said okay I think I'm not sharing the screen

0:13:32 -  sorry guys I thought I was sharing screen I was reading so I've asked chat GPT on what is difference between SPM and integration provide so initial point it's measly focusing on classification and regression capabilities ofm whereas linear regression is only regression I should have asked the question as linear regression and logistic regression but it's okay in the robustness it says SPM is generally robust to outliers due to its focus on maximizing the margin between classes whereas linear regression can be sensitive to out layers as it aims to minimize the square difference between so as I said in in this path where you put this in between if the distribution the these points on how they are distributed and on these points on how they distributed impacts the impacts the loss function because there there are more out layers higher values here then obviously to minimize the loss it will come closer to this point so that these out layers that are there those are also predicted well or else what happens is if if you don't move towards this class the out layers will have very bad um uh loss and because that the overall loss will be higher and we will try to obviously our model will try to reduce the loss and it will go closer to these points so it it if there are out layers it would it would impact um the decision uh so whereas svm doesn't get impacted because of going with that margin between classes logic uh regularization ASM can incorporate regation parameters to control or fitting such as the parameter C in csvm can incorporate regularization techniques like I think we have not covered regularization sometime we should also look at I think regularization bias variance under fitting over fitting these are all few Concepts which are which actually um give solution to uh are actually help in evaluating how good is your model when it output is but till now the way we looked at it it's like the intelligence or predicting capability itself is a big thing but after predicting what happens people start using it in on ground and they observe that the predictions are actually not aligning with what their expectation is the the uh um accuracy of the prediction is low we should also look at how these accuracy of prediction

0:16:11 -  are calculated using different uh ways when you practically work with models that is also important I think these all these Concepts could go in a day or two sometime so that we are having a high level idea on those and when we do practically when we sit and do things we understand this terminology is one thing second is also we know how to when when we build a model we know how to uh calculate the accuracy or our laws or our overall metrics in order to classify or compare between different models let's say I have I'm using for a given problem statement I'm using svm and linear regression I should be able to have a comparison between these two to say which one is better right so how do you calculate that what is your metrix to compare right that all should also be covered okay so that's with declaration so ENT interpretability this boundaries are less interpretable especially in high dimensional spes or when using nonlinear functions linear provides easily interpretable coefficients that represent the effect of each feature on the target yeah because um we are not doing W into 2x + B in uh as in linear regression what we are doing is W into like something like 5x I of X or G of X or k of X or whatever a function that is applied on X uh at each point level to to to uh impact the points and then we are doing it so what happens is the um prediction to relation to X is not linear that's the difference so in linear aggression is linear whereas here it is not linear because the the the the Y is predicted not basing on X the Y is being predicted basing on that so-called f of x or whatever function of X kind of stuff so and in turn function of X has another relationship between between actual X and the function of X right so there are like two levels we are like doing F of G of X kind of stuff so we are applying two two functions on top of X here in SCM sorry guys okay that's with svm um I think it is little bit clear but we don't have the details uh on equations

0:19:30 -  derivations and other stuff and that'll that'll make it little complicated for sure so we're moving on um well let's look at uh the next one um as per this guy he said um okay this is about posting okay anyways [Music] um is a probabilistic classification algorithm based on base theorem and with the Nao Assumption of features Independence it assumes that the presence of Fe particular feature in a class is independent of the presence of other features so despite this simplifying assumption na base can be surprisingly effective in practice especially for text classification task so when we consider features X1 X2 X3 X4 XY and stuff we us usually tend with we look at it with certain correlation between them so that's the reason we combine them together like W1 X1 plus W2 X2 plus W3 X3 kind of stuff um and um we increase those weights W1 W2 W3 W4 in a way that they correspond to each X on how much impact it would make on Y at the same time we we put them together so that uh if even if W1 Rises on X the remaining will will be adjusted in a way that the all the sum of them actually tries to reach y but whereas in uh as for this na base uh theorem which uses the base theorem um here we deal with each feature separately and then we put them together to achieve y c kind of stuff so base relies on base theorem which calculate the probability of a class given a set of features P of Y of X is equal to P of X by Y into P of Y by P of X where P of Y byx is the probability of class y given features X okay P of X by Y is the likelihood of

0:22:56 -  observing feature X for given class y okay is the prior probability of class y p of X is the probability of observing fature X so given set of features we predict y so this is that is probability probability of Y basing on given set of features for a given the the reverse way of looking at it that is means for X2 y reverse way of looking at it is given a y let's say what is the probability that that the certain feature will be there okay that could also be calculated uh probability of overall of class y right and uh probability of X is all observing feature x n assumption is n base assumes that the feature are conditionally independent given the class label mathematically this can be P of X of Y is equal to p P of X1 of X1 by Y into P of X2 by Y and so on P of X1 by y the calculation P of X of [Music] Yasser each class based on the logistics function coefficients are not directly interpretable the impct of each feature Target uh robustness lessons features since to multi outliers can handle large feature space efficiently efficient efficient for large data sets and dimensional feature spaces uh because even there are 1,000 features we are dealing with each feature separately like feature one to the whole why some somewhere close to De trees right de trees also they deal with each feature separately but here what we are doing is we are not dividing and conquering it we are dividing the features that's all um but you are not dividing the data for each feature how does it work why works and then we are just multiplying it here [Music] so um

0:28:0 -  um okay performance was can perform well with um your small data sets and in higher dimensional space large space sufficiently okay and C say high dimension space is is meaning more number of features like same scalability of lar feure spaces and as a logistic regation what happens the calculation and stuff everything is dependent on that 1 by e^ minus Z and Z is equal to wdx + B so the more number of X the more the longer the Y will become and the calculations will be higher if the features are growing whereas here the because the feature each feature to Y is being calculated it is the equation alog together is not one equation that gets it's longer it is just that the probabilities needs to be calculated for each feure they should be multiplied kind of stuff to understand what is p of X of 5 there this is nice very nice um looking at example right example applying to small dat set we will use a fictional data set with three features age income education level and buy indicating whether a person purchases a product or not okay basing on a income and education level whether he will purchase a product or not the product is know but it's a good generalization saying it could be applied for any product right uh person makes purchase based on their income data processing convert categorical variables yeah AG income okay calculate class price P purchase purchase is equal to probability for purchase is equal to S and probability purchase equal to no um calculate likelihood for likelihoods for each feature age income education calate the condition probabilities like P of feature of purchase that is like P of age of purchase P of income of purchase and P of Education Lev to purchase that is uh for the class prior means that is p of Y okay and um for age is equal to Middle AED then P of middle

0:30:35 -  aged of purchase y equal to s y equal to no the calc income level high why purchase equal to Y purchase equal to no okay education level College purchase equal to Y and equal to no prodution P of purchase equal to yes middle age High College will be P of middle age plus into P of high into P of college into P of purchase equal to S by P of middle age into P of high into P of college of P of purchase no compare the post probabilties for yes and no and select the class with the higher probability the person makes a purchase based on its class like we calculate probability for p ofs and p of no both and then whichever has higher probability we lean towards that decision saying whether he will purchase the product or nice so we covered na base um as well with with an example on how it works and stuff primary is to understand saying when we have La linear regression when we have logistic regression why did people create or why you should think in the direction saying why did people have a need of creating a new algorithms uh when linear regression logistic regr already there people would have started using linear regression listic regression for regression and classification tasks they would have actually seen some challenges with respect to um the calculation or with respect to overfitting or with respect to um outliers data dependency data set size and and overall performance of computation all these and they're trying to optimize it to to better so in those optimization Journey they are actually picking up uh different metrics and different uh formula or different optimization techniques of approaching the same problem um and that's how I think theyve derived all these new algorithms as in svm disent trees are na Bas and stuff so that they are be more optimal compared to linear regression or logistic regression kind of stuff okay that should be the that should the way we should look at the evolution if you look at the evolution

0:33:6 -  everything will be clear um we can if you look at time lines Evolution and stuff you'll have clear you can connect the dots on P linear regression has been created logistic regression and then when svms are created when dees are created when na base is created and stuff everything has uh an underlying analysis on what is the challenges with the previous one and why we are going for the new one and how we are making sure the new ones are better fit basing on size of the data basing on the out layers basing on the need of um accuracy with predictions U all these factors will will come in um and also basing on the features like for example n clearly says it coners that all features are independent the correlation between the features is almost zero they are not dependent on each other like um this example is perfect right like age of a person income of a person education person are not directly related they are they could be independent people at higher age can also study people at higher age can also be rich they can also be poor so all combinations of features are possible and they're very independent so such kind of independent featur if they are there na Bas is a better classifier uh compared to um to logistic regression and uh also I would not say comparison between na base and svm which will perform well um but again svm and logistic regression they are they don't um uh look at features to see their independent features they they see they think that the features overall features has correlation with Y and also with the features they have correlation between between them as well which we don't know that's the reason they try to put them into same equation so that each feature in turn impacts other feature kind of stuff the problem is we treat we we don't treat saying X1 X2 X3 are related uh and X4 X5 are different we we we treat them like all of them are related and uh um having more weight to any of the feature will actually impact one way the other way the remaining features so that's how we fine tune that's the

0:35:41 -  difference that is on the table okay I thought of covering the K clustering as well today but I think we need to push it for tomorrow uh and continue cin clustering tomorrow it should not be a big deal we should be able to complete it but I I think um it will will be overwhelming to cover more things um in this flow itself okay that's all for uh today guys the energy levels look a little low um but I think I'm good um I I I think um we going at a good speed now compared to the initial days where we trying to set up more foundational understanding [Music] um I I should also focus on picking up learning new things but uh I'll I'll see [Music] um uh but my advice would be if somebody is uh uh also following are studying along with me then my advice would be to uh um get deep if you have time get deep into these algorithms not to understand the algorithm or focus on implementing the algorithm but I want you to capture the thought of how somebody would have published a paper right saying this is called svm then what is the in that paper if you study they'll give an analysis saying if you do linear regression if you do logistic regression or their current models that are there if you do them these are all the gaps and they would given a comparision to address all these gaps svm is been svm is the way to go which will actually uh address those gaps and give you a better solution that's how people will evolve and if you as long as you capture that Evolution it's not about the linear regression equation or or or the svm equation or base equation and stuff it's more about if you if you capture the evolution you capture the thinking process uh on how they have optimized that that thinking capability being induced into you you work with a model which is completely different from what we talking right now but that approach optimizing thinking are uh improving the problem that they faced and stuff those are all experiences those experiences will give you much