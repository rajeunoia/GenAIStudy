# Day 7 Study Gen AI

**Time Interval:** 00:00 - 37:25  
**Summary**  

- **üîç Overview**: The session resumes discussions surrounding **logistic regression**, building on prior concepts introduced in discussions about **AI** and **machine learning**.
- **üìä Logistic Regression Basics**: The instructor clarifies that while logistic regression is often associated with regression methods, it is fundamentally a classification algorithm, making distinctions between continuous and categorical values.
- **üí° Understanding Classification**: Emphasizing that classification involves mapping data into discrete classes (e.g., classifying objects as apples or oranges), the instructor illustrates the use of logistic regression in determining probabilities related to these classifications.
- **üõ† Equation Derivation**: The session covers how predictions are derived using logistic regression, detailing the underlying equations and the significance of transforming raw outputs into probabilities suited for classification tasks.
- **üìà Sigmoid Function and Mapping**: The instructor introduces the **sigmoid function** as a means to convert predictions from logistic regression into a probability range (0 to 1), which distinguishes it from linear regression outputs.
- **üîÑ Loss Function in Logistic Regression**: Discussing the loss function, the instructor explains how it calculates discrepancies between expected and predicted values, emphasizing the role this plays in model optimization.
- **üìè Gradient Descent Overview**: The presentation touches upon the application of **gradient descent** in optimizing logistic regression, explaining how adjustments are made to parameters in order to minimize loss.
- **üöÄ Future Outlook**: The session concludes with a commitment to further explore gradient descent and its relevance in refining machine learning models in future discussions.

# Transcript 


0:2:55 -  good hey hi guys sorry I kept it on live without talking so um today we'll just complete quickly complete the logistic regression discussion that we started yesterday um what I did actually I'm there from seven but um last few minutes I've been spending on understanding um so when we were actually discussing between friends onl topic and discussion and stuff he raised a point saying see so we I told him that um usually my approach to understanding a IML and stuff is to get to the detail um so that once you know the underlying detail your U capability to apply it is good your capability to understand uh not only what it is but you can also understand why it is in place and why people have thought through in that direction um opens up A New Perspective of how you can apply or maybe you could also think how you can optimize it basing on the um requirements are there so that's always my thinking this is this is the discussion we had and then he's like he he told me see forget about something complex like generative Ai and some blah blah blah um let's talk about logistic regression even for logistic regression we don't exactly know how those equations are derived and stuff so that was the comment I was like um um so I should I should understand it now and yesterday when we were when we were talking actually I said like um this is how the logistic regation uh logistic regressions um loss function is going to be I'm given the formula I want to get into the detail saying and I also said linear regression is regression is is both related where linear regression is part of the ftic regressions um sigo functions equation and stuff this is what we were talking right so I just thought let me look at the detail but it's it's not that much but we'll go through whatever material that I have covered and stuff so that you guys are also on the same page with with your understanding so uh this is going to be today it's going to be very quick I need

0:5:32 -  to wrap it up um quickly um we didn't write anything on ltic which is very nice so coming to logistic regression I said I said it is a classification algorithm but it says regression so what does regression mean we have already discression means continuous values right so when we say continuous values and we want classification classification is more like uh um classification so we we apply classes like class zero class one class two class three kind of stuff so where class for example if you have two classes class Z class one class zero can be apples and class ones can be oranges or class zero can be um class Zer can be disease class one can be no disease so as simple as that so it could be anything as classification right but as I told you every see machine learning um I think the way we started off as recognizing the patterns of in the data and then and putting that pattern into a certain equation and that equation in turn taking feeding in unknown values and that unknown values predicting certain values the unknown values being fed into an equation and then predicting what could be the possible outcome of that unknown values that are coming in right uh so that and and for that it's this is all covered in linear regression right now coming to logistic regression output is expected to be binary 0 one what we know is linear regression right and linear regression values will be so usual linear regression is an equation and and uh so if you if you either take a straight line or you use a curve or whatever you do on the input X values we always uh we don't know what could be the possible input X values correct so we know certain values basing on that with maybe the graph is like this but if we say that we don't know what are the possible X values the possibility is X can be or X or either X or Y both of them can

0:8:11 -  be um ranging from anything it depends like features it depends on the feature right the feature if the feature says number of Wheels it could have so you can't have obviously you can't have negative number of fields you you can have positive Wheels but you can't have 100 Wheels to vehicle doesn't make sense or even bigger numbers like th000 Wheels per vehicle is not possible so so technically it has a range it will have some range which again is not defined saying what is should be the range kind of stuff but there is logically there is a range but whereas why we don't know what y value can be right by value can as I said yesterday it could be bank number it could be size of a substance something that that in nanometers so the number could be bigger right so number could be do we know what is the upper limit lower limit or range of the numbers we don't know so considering that fact the recession values usually are not boundary or are within a range um in in general sense differs by use Case by use case but the expectation of an algorithm when we build it we we think saying it should accommodate uh no range restrict section so when I say no range it's simple it is like on the real number it is on left and right side right the left most is minus infinity and rightmost is infinity correct so the linear regressions outputs are targeted to predict minus infinity to Infinity where we fit in depends on where we would be depends on the equation that we're picking and um it also depends on uh uh it also depends on input inputs that we're providing to it basing on inputs and the equation it will vary now coming to logistic regression so linear regation YP y predicted will be or not will be could be between minus infinity 2 plus infinity plus infinity but Infinity but to distinguish minus infinity plus infinity whereas when you come to classification problem what we are expecting is logistic regression and let's say binary

0:10:53 -  classification then we say it is from 0 to 1 matter of fact it is more zero or one but basing because we are going with um probability we not just saying Z or one we we predict the probability and the probabilities are been 0 to one where we could take a call saying 0 to 0.5 0 0.5 to 1 is one so what we want to is use in our logistic regression is underlying is linear regression but what we want is listic regression is 0 to one so either we need to map minus infinity infinity to 0a 1 or 0a 1 should be extended to become minus INF Infinity so that the equation can fit in right so what that is the approach that people had so to scale ltic to scale 0 to 1 and let's say we we go this way 0 to 1 if you want to take it to minus infinity to Infinity certain functions that are could be applied on probability 0a 0 to one range numbers and um extend them to minus INF Infinity one such function that people use um is odds ratio odds ratio is nothing but um probability usually gives you what is the probability of positive outcome outcome of it and um if the positive outcome is divided by the negative outcome of the same value then that is called the odds ratio okay so that means how many how much positive can we predict um how much positive probability is there relative to the negative probability for example if you have 10 apples and um 10 oranges something else and we don't include the other classification we we we're looking at classification from one perspective right so it's like okay forget about oranges we have apples and then we call it apple or we call not an apple right so out of 10 apples if you say six apples we we have we have probability saying these are apples for the other four apples either it says these are not apples so the positive outcome is six and the

0:14:8 -  negative outcome is 1 10 - 6 [Music] right so it is um 6x4 correct so it will it will be 3x2 it's 1.5 so if the Apple predicted are zero and 0 by 1 - 0 it is 0 if the apples predicted are 10 then it is 0o sorry 10 by 10 - 10 Infinity so the value range of um this odds ratio is positive comes by 1 minus are total outcomes minus positive outcomes right if it is assuming if if if it if you put it in probability numbers kind of stuff it'll it will be like P of e by 1 minus P of e right and this will range from and assuming this is p of e and this odds ratio will range from from zero to Infinity when we say 0 to Infinity but where we want to reach is minus infinity infinity so one value one um function which will actually scale a given numbers to from minus INF to Infinity on on on given these kind of numbers right 0 To infinity or whatever is to go with logarithm okay I I'll we'll discuss more on logarithm of these numbers 0 to Infinity will be minus infinity to Infinity means log of P of e by 1us P of

0:16:57 -  e this will this is ranging from minus infinity to Infinity how if you look at logarithms log of um so to start off log Z is not possible because okay let's talk of logarithms right to understand logarithms better logarithms is nothing but if you have if you want to show given number as a multiples of another number so that means 100 can be looked at as multiples of 2 10 that is 10 into 10 right so that means log 100 with base 10 is equal to 2 right okay so similarly log um let's say log a base B is X or let's say is I I that means B power I is equal to a so that means if you know a number and you want to represent if you know a number like a and if you want to represent that number um if you want to find out um given that number what multiple of another number gives you that number like example I said 100 to the base power 100 is multiple 100 is um what power multiple what how much multiple of 10 is 100 is two uh how much multiple of um uh 10 is th000 three right so like how many multiples of 10 can be used similarly how much multiple of 10 is 0.1 right 0.1 we can write as 10^ minus 1 that means 10 power what what we give the value that is actually is nothing but so to derive 0.1 how much power of 10 should be used then we say log 0.1 base 10 means it's -1 because 10^ - 1 is 0.1 so if log 0.1 base 10 is equal to something means 10 power this value will give you this value okay not sure how how how you guys can look at it but

0:20:37 -  that's logm okay simple logm is uh simply showing that given a number um and given a number and if that number needs to be represented in multiples of another number like let's say um I think I mean this is all I'm giving you more but nothing nothing wrong to learn so it's good to learn because we feel they are complex but they are actually simple just that instead of explaining all this for this long people use a simple term logarithm but we should understand why the way and where to use it right so um I I have uh I I have like um 100 chocolates and I decided to give um or I have 100 chairs okay I think this is also complicated but okay let me let me give it a try I have 100 chairs okay I want to see um what is the best possible way that I can uh or I I want to see how many um uh how no oh this is interesting man we're trying to say where will somebody try to use uh logarithms right um what is the Practical use of logarithms I thought you'll say aren the um by 10 is okay that means stand 10 what where will you get 10 by 10 by 10 okay let's ask CH GPT once it's a good question to ask actually oh I'm not sharing my screen this is funny I've been writing without sharing my screen okay where do we use in real world provide an example okay you guys can't see uh I'll just move this real world examplei are used in real world applications finance and economical are commonly used in finan to calulate compound [Music] interest time it takes for the investment to double

0:23:15 -  um how is that for example if you want need to get 100 yeah makes sense so for example if you want to get 100 percentage and um you you get interest at 6% how many multiples of 6% um would become 100 like what power of six will become 100 right make sense does it really make sense no doesn't make sense because X is there and to 6% is there then it is into six it's not into 6 into 6 100 rupees is there you get 6% interest is not 6% 6% is 0 6 yeah makes sense that's it so 100 into 0 06 6% means 100 into 0.06 uh that's your interest or you can say 1.06 into 1.06 and um again next year you you you multiply 1.06 so you take 1.06 as base and 1.06 power that's what it is written here right 1 plus r power T So 1.06 into Power watt will give you 100 will give you like how much in how much time it'll it will get double yeah makes sense there we can use it acustics and audio engineering logarithms are used to measure sound intensity and exposit in decb so I think for representation purpose makes sense because he says the values are um like at Quake it's it's there below it person approximately 31.6 times more so somebody says 6.5 so that means it's um log I mean the numbers are pretty big and and instead of giving the big numbers to put them down to a smaller number they'll and and for people's understanding or communication between different parties they might use a base of maybe something right and then communicate so yeah that's also makes sense like something like 100,000 or something like that and then we say it's base

0:25:41 -  uh 100 if you say or base 10 if you say also it is it will say five actual number is 100,000 but you don't want for Cal purpose and stuff you don't want to use um 100,000 then it will become then you can use it as um then you can actually store it as five but actually what five means is is actually a logarithm of base 10 applied on that particular number gives you five and the actual value is 10 power 5 makes sense makes sense similarly um in chemistry to see how much moles are there and something that like any anyways it's it's all boil on to numbers right hyr concentration of 10 power minus 3 moles per liter oh okay that's what PH means but instead of telling us number like 10^ minus 3 or 0.001 um they call it three but actually what it means is 10 per minus it's actually used for communication or data representation instead of showing a very big number very small number kind of stuff here we we will put it into a common number but um underlying to get the original number again we need to go back saying okay this number is um logarithm of the real value with certain base if you know the base then if we see take the base and apply this number as the power to it we'll get the original value makes sense okay so to some extent okay real world application we need to see when and where we will do it but going back that's that's with the logarithms I think um we are deviated far away generative Ai and supposed to be llms and stuff and then we came to machine learning AI we came to machine learning machine learn to linear regression linear reg logistic reg logistic regation to log functions and now we are talking about what what is logarithms and stuff I understand but let's say today is a deviation and then we want to learn more our curiosity is kicking in and we want to learn more of it so this is what is the story of logarithms so what happens is um this is logarithms right so now going back on how logarithms is minus infinity infinity so log of um you take anything uh for the

0:30:46 -  log of x with some base B to be one the X needs to be zero correct because no no no no sorry sorry x equal to0 is not possible that's what I'm trying to say for log of x is equal to 1 is if the base is B then b^ 1 is x equal to B so [Music] um okay how does uh when does the log value become zero not possible okay when does it become one log value of something becoming one when the x equal to base right um when is it equal to zero not possible because B to the power anything can't make it zero okay so that's not possible [Music] um when does it become Infinity [Music] when when X is infinity I think we should take this down [Music] umus 10 let's say minus one X is = 1 by B minus infinity B power minus infinity that's nothing but 1 by B power Infinity when = to 1 by B power Infinity correct next is infinity not is equal to yeah that means B for Infinity that is X is infinity [Music] so logarithm becomes one when no no no not x equal to B uh [Music] when now loger the of is possible it's not POS this is wrong X = to 1 at zero it becomes one at 1 it becomes B and um at minus one it becomes 1 by B

0:33:23 -  and as the as the negative numbers increase it will go um to minus infinity but it doesn't go to Z 0 is not possible so from 0 to 1 is where it goes to negative numbers it goes down graph and um one to above it it it keeps going to Infinity this is the understanding let's look at the blog which is explaining this actually I should have started there because it would be much more clear so does it say the graph does it say okay fine so uh this one okay this is clear right that's not okay that's okay blog and stuff we can put it on so this is clear so it it is logarithm is ranging from minus infinity to Infinity okay that is clear so listic regression gives you the expectation is to get certain probability and um that probability uh which is is range of 0 to 1 but the linear regression is minus infinity infinity so in order to map these two what we need to do we should apply certain function on either sides any side is okay as say to say that both of them are matching so you predict so your values of um linear regression are minus infinity infinity now what happens if you want to map certain value in so you apply a l regression you you get certain value now basing on that value how do you say whether this particular probability of that particular X features X1 X2 X3 is um is is what in the classification so that is what we need to arrive it right simple so how how we can set it is this minus infinity comma Infinity that is there that we should map to 0 comma one how do we map it this how we can map it right so now this function that is log of P of e by 1 minus P of e is infinite minus infin now we can equate this YP y is nothing

0:36:24 -  but um actually W1 X1 plus W2 X2 so con right so w um okay w i into X I let assume some constants [Music] so um now um let's just call it Z okay but this is good but what we want to arrive at what we want to predict um this is good but what we want to predict is not Z right as if in linear equation L we want P of e right so how do you get it we can derive it using that equation right so here um we using log base for E let's say then uh as per what I explained here right log uh a to the base B is equal to I then B power I equal to a similarly log some value to the base power e is equal to this y z then what happens uh small then what happens let's take that equ equation the equation and then say of e by 1 - P of e is equal to the base is E power Z so it's E power Z correct correct and um so I'm solving this not a complex thing so um 1 minus P of E I mean I'm just trying to make it simple by I'm doing reverse of both sides so this is 1 by E power Z correct now that is uh now that is 1 by P of e minus one right so here I'm I'm dividing 1 by P of e minus P of e by P of e p of e by P of e is 1 right let's make it that way what is a big deal P of E okay I'm Distributing

0:39:29 -  here = 1 by E power Z so what happens now this is minus one so this goes here it will become + one so 1 by P of e is equal to 1 by E power C plus + one so this is nothing but uh P power minus Z + one correct so P of e is equal to 1 by 1 + e powerus c which is nothing but or sigmoid function so what we want to predict in logistic is p p of e um how do you get it the we calculate for given features xxx1 xx3 we calculate Z that is nothing but YP y predicted and uh that we map it put it in equation which which will actually uh kind of map minus infinity infinity range values to the required 0 comma one values that that we need and basing on varying the minus infinity infinity values for a given point we vary the corresponding 0 to one points value means it could be 2200 or something like that in the linear regressions YP product YP and 2200 will be mapped down to um the corresponding probability using this sigmoid function kind of stuff so and then what we do we and for given X features let's say the value is 2200 and we'll keep the 2200 YP is 2200 and we keep this 2200 here um like 1 by 1 + E power minus 2200 it will give certain value and if the certain value is not um is not matching with actual probability right so that means the value is the values

0:43:10 -  are the value is expected to be closer to one let's say uh but the value is is closer to zero then we change the weights in a way that the 2200 either should reduce or increase and uh we we tune we we uh again do the loss function we look at we can look at the gradient descent and stuff and and adjust it come up with new W values and stuff um and then we we can find unun it but the um but what happens is you you you are changing 2200 but actual value that needs to be stored is the expected value is one and you let's say you're 0.3 then your um so loss or logistic regession goes like this so expected value is one and output is um so B then loss is 1 minus p and similarly if expected value is uh value is zero and output is p then loss is P or minus P so basing on the input value the output uh or the loss is determined if the expected is one this is the loss if the expected is zero this is the loss kind of stuff so again um the these values are represented by probabilities that means these values will be one minus so like P output itself is within scale of 0a 1 correct and this will become same this will also become 0a 1 and this will become -1 0 so the value scale is pretty small um so even then here what we do is instead of uh instead of multiplying with 0.2 0.3 kind of stuff um anyways to put this loss function into um one single equation instead of having two different

0:46:18 -  equations in order to put it into one single equation what we can do is um let's say the output expected is let's say we call it as um uh pa okay P actual and this is PP P predicted so then we can actually say PA into 1 minus P plus this this is 1 - ba because it's zero 0 to become one we should say 1 - ba and then then it's minus B so this is the last function but what happens um the 1 minus p and 1us okay let's say if it is predicted are the loss functions excuse me if they are the variables right PP is the one where we have W coming in 1 plus e minus Z and z in turn has WX and all that stuff right so that's those are the variables and if those variables are in that range of 0. the overall your loss function will become uh um pretty minimal right so in order to again um in order to bring in a proper higher value or something like that here what um we what we could do is like like you remember in in uh cost function we we did squares right similarly here what we do is we apply logarithm again um in order to make the derivations and math easier um espe esecially because the the P value is 1 by 1 minus z e power minus z um and um to handle that the applying a logarithm will be a much easier way for calculation purpose that's all uh let's ask chat GPT also once why do why do they use logarithm listic Lo [Music] function numerical

0:50:8 -  stability penalization miscal miscal sorry Massif [Music] maximum likelyhood estimation M like mean square error it says maximum likelihood estimation um okay the loation model is often TR using maximum likelihood estimation M where the goal is to maximize the likelihood of observing the actual binary outcomes given the predicted probabilities by taking the negative logarithms of the likelihood function we obtain the log loss function which we aim to minimize during the training which we says it's definition of l lastic regression is what he's trying to give here but more on I think second point is um so logm uh lthms help improve numerical stability especially when dealing with probabilities that are close to zero or one taking the logarithms of probility ensures that the loss is well behaved and does not explode to Infinity as the predicted probabilities approach the extremes of zero or one okay penalizing misclassification the logarithm function is concave meaning it penalizes large errors more than small errors this property is desirable in loss function because it assigns higher penalties to large deviations between predicted probabilities and actual outcomes leading to more robust model training okay so this would be the final o function again why we do the um average and stuff you guys already know it so there's nothing uh new to learn there but all the remaining is G is equal to 1 by n sum of PA is nothing but Yi but y i into ryi log di+ 1 minus Yi into log of 1 minus p i [Music] ah this is what happens with the chat GPT you you you say that you're wrong it immediately says yes I'm I'm wrong and

0:54:8 -  it gives you an answer but at times what happens is its answer is the same okay so it's it cross validates and um but if it is correct what happens is it actually seem the same thing it's like maybe it doesn't want to hurt your ego or something like that interesting right yeah okay that's it with the the log loss for uh this thing okay let's quickly look at uh okay how are uh weights calculated in um and thens of parameters are [Music] calculated mhm it's it's the same thing I think that's the last function how good it is we need to there have the gradient this is okay so we derive the loss basing on the variable so w j is equal to WJ minus derivation of loss with WJ and what does he give derivation of loss of the W uh J will [Music] become so Yi into 1 - Yi into X - 1 - Y into y x that means that's the derivation of this particular loss function with respect to WJ it seems okay sounds good I I think we good um we've covered um lotic regression so I think you can understand so again same story as gradient descent so we pick up a random values of the one which is represent as beta here and then we calculate the loss function for the given X1 X2 using this particular beta beta one and then we see what is the loss and uh then we calculate the new beta by doing the desent and um the by keeping in calculating this values um and arrive at new beta and then we we repeat that step so the learning rate again helps in this steps that in which the particular WS will be changed and then finally we we'll come to an optimal W where the loss function is minimal that's all same logic as same gradient descent algorithm that we have applied in linear regression same will be