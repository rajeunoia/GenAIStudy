# Day 46 Study Time GenAI

**Day X: Understanding Transformers in AI**
**Time Interval:** 00:00 - 39:00

**Summary**
- **üîç Overview**: The session introduces the **"Attention Is All You Need"** paper, highlighting its transformative impact on AI and deep learning.
- **üìà Evolution from RNN to Transformers**: The instructor discusses the limitations of **Recurrent Neural Networks (RNN)** and **Long Short-Term Memory (LSTM)** models, emphasizing issues with data processing and sequential dependencies.
- **üöÄ Introducing Transformers**: The presentation reveals how the **Transformer** architecture addresses previous challenges by implementing mechanisms like **self-attention** and **parallelism**, enhancing model performance on large datasets.
- **üìä Self-Attention Mechanism**: A detailed explanation of how **self-attention** helps relate words within a sequence is provided, including the formation of **query (Q)**, **key (K)**, and **value (V)** vectors for the input data.
- **‚≠ê Positional Encoding**: The importance of positional encoding in capturing the order of words in sequences is discussed, clarifying how it allows the model to understand context beyond mere word embeddings.
- **‚è≥ Efficiency through Multi-Head Attention**: The instructor explains the concept of **multi-head attention**, which enables the model to simultaneously focus on various parts of the input, creating richer contextual representations.
- **üîÅ Future Lessons**: The session concludes with a preview of upcoming topics, including deeper explorations of multi-head attention, optimization strategies, and practical implementation details for coding Transformer models.

# Transcript 


0:2:48 -  yeah hi guys so uh today will today is very interesting because we'll go through attention is all you need paper which has actually transformed the whole J Journey um it is um this is actually the um start of of uh the whole thought process um where it's actually if you look at it it is actually a one step Improvement on top of the journey that is RNN lstm and um alums I could I would say performance Optimizer um um so Gru um but uh still we have challenges with respect to the way we process with LSM the way we um how much data that we could run using lstm and stuff there are challenges with it so um attention is all you need is a paper which came as a stone which addressed all those issues and also it gave us a confidence saying um we can train uh models with huge amounts of data uh which is nothing but the large language models um and um he they they bring in parallelism into it to improve performance and stuff right so um in of me briefing on it we'll go through we'll study it and then at the end maybe we'll have a discussion either today or tomorrow it's going to take some time but U this is this is need of the hour let's get to it CH a t to my video about the Transformer and this is actually the version 2.0 of my uh series on the Transformer I had a previous video in which I talked about the Transformer but the audio quality was not good and as suggested by my viewers as the video was really uh had a huge success uh the viewers suggested me to to improve the audio quality so this this is why I'm doing this video uh you don't have to watch the previous series because I will be doing basically the same things but with some improvements so I'm actually compensating from some mistakes I made or from some improvements that I could add after watching this video I suggest watch my watching my other video about uh how to

0:5:19 -  code a Transformer model from scratch so how to code the model itself how to train it on a data and how to influence it stick it with me because it's going to be a little long journey but for sure worth now before we talk about the Transformer I want to first talk about recurrent naral networks so the networks that were used before they introduced the transformer for most of the sequence to sequence jobs uh tasks so let's review them recover Nal networks uh existed the long time before the Transformer and they allowed to map one sequence of input to another sequence of output in this case our input is X and we want an input sequence Y what we did before is that we split the sequence into single items so we gave the recurring Network the first item as input so X1 along with an initial State usually made up of only zeros and the recurrent Nal Network produced an output let's call it y1 and this happened at the first time step then we took the hidden State this is called the hidden state of the network of the previous time step along with the next input token so X2 and the network had to produce the SEC the second output token Y2 and then we did it the same procedure at the third time step in which we took the hidden state of the previous time step along with the input State uh the input token at the time steps three and the network had to to produce the next output token which is Y3 if you have end end tokens you need end time steps to map a end sequence input into an end sequence output this worked fine for a lot of tasks but had some problems let's review them the problems with recurring our networks first of all are that they are slow for long sequences because think of the process we did before we have kind of like a for Loop in which we do the same operation for every token in the input so if you have the longer the sequence the longer this computation and uh this made the the network not easy to train for long sequences the second problem was the vanishing or the exploding gradients now you may have

0:7:51 -  heard this terms or expression on the Internet or from other videos but I will try to give you a brief U Insight on what does what do they mean on a practical level so as you know uh Frameworks like pytorch they convert our networks into a computation graph so basically suppose we have a computation graph I this is not an error network I will making I will be making a computational graph that is very simple has nothing to do with neural networks but we'll show you the problems that we have so imagine we have two inputs X and another input let's call it y our computational graph first let's say multiplies these two number so we have a first a function let's call it f of x and y that is X multip by Y mtip and the result let's call it Z is is given to another function let's call this function G of Z is equal to let's say z s what our for example does is that P want to calculate the usually have L function calculates the derivative of the L function with respect to each weight in this case calate the derivative of the G function so the output function with respect to all of its inputs so derivative of G with respect to X let's say is equal to the derivative of G with respect to F and multiplied by the derivative of f with respect to X this two should kind of cancel out this is called the chain Rule now as you can see the longer the chain of computation so if we have many nodes one after another the longer this multiplication chain so here we have two because the distance from this node and this is two but imag you have 100 or 1,000 now imagine this number is 0.5 and this number is 0.5 also the resulting numbers when multiply together is a number that is smaller than the two initial numbers it's going 0.25 because it's is4 so if we have two numbers that are smaller than one and we multiply them together they will produce an even small number and if you have two numbers that are bigger than one and multiply them together they will produce a number that is bigger than both of them so if we have a very long chain of computation it eventually will either become a very big number or a very small number and this is not desirable first of all because our CPU of our GPU can only represent numbers up to a certain position let's say 32 bit or 64 bit and if the number becomes too small the contribution of this number to the output will become very small so when the PCH or our automatic let's say our framework will calculate how to adjust the weights the weight will move very very very slowly because the contribution of this product is will be a very small number and this means that we have the gradient is Vanishing or in the other case it can explode become very big numbers and this is a problem the next problem is difficulty in accessing information from a long time ago what does it mean it means that as you remember from the previous slide we saw that the first input token is given to the recurr along with the first state now we need to think that recurr network is a long graph of computation it will produce a new hidden State then we will use the the new hidden State along with the next token to produce the next

0:17:22 -  output if we have a very long sequence um of input sequence the last token will have a h state whose contribution from the first token has nearly gone because of this long chain of M so actually the last token will not depend much on the first token and this is also not good because for example we know as humans that in a text in a quite long text the context that we saw let's say 200 words before still relevant to the context of the current words and this is something that the RNN could not me and this is why we have the Transformer so the Transformer solved these problems with the networks and we will see how the structure of the Transformer we can divide into two micro blocks the first micro block is called encoder and it's this part here the second micro block is called the decoder and it's the second part here for [Music] [Music] [Music] what you [Music] do and you pass the combination of input and output together whereas you use the combination for both of both but at the same time you run a par in par that is how you retain the context [Music] [Music] [Music] comp [Music] [Music] fore e e e e e e e e e e is the same number number as this cat here because they occupy the same position in the vocabulary we take these numbers which are called input IDs and we map them into a vector of size 512 this Vector is a vector made of 52 numbers and we always map the same word always the same embedding however this number is not fixed it's a parameter for our model so our model will learn to change these numbers in such a way that it represents the meaning of the word so the input ID is never Chang because our vocabulary is fixed but the embedding will change along with the training process of the model numbers will change according to the needs of the loss function so theut embedding are basically mapping our single word into an embedding of size 512 and we call this quantity 512 D model because it's the name that it's also used in the paper attention is all you need let me show you how we do stuff around here actually my department uses m.com so I'm Good Work Management tool let's look at the next layer of the encoder which is the positional encoding so what is positional encoding what we want is that each word should carry some information about its position in the sentence because now we built a metrix of words that are embeddings but they don't convey any information about how where that particular word is inside the sentence and this is the job of the

0:19:53 -  positional encoding so what we do we want the model to treats words that appear close to each other as close and words that are distant as distant so we want the model to see this information about the special information that we see with our eyes so for example when we see this sing we know that the word what is more far from the word um is compared to encoding because we we have this special information given by our eyes but the model cannot see this so we need to give some information to the model about how the words are specially distributed inside of the sentence and we want the posal encoding to represent a pattern that the model can learn and we will see how uh imagine we have our original sentence your cat is a Lov cat what we do is we first convert into embeddings using the previous layer so the input embeddings and these are embeddings of size 512 then we create some special vectors called the positional encoding vectors that we add to these embeddings so this Vector we see here in red is a vector of size 512 which is not learned it's computed once and not learned along with the training process it's fixed and this word this Vector represents the position of the word inside of the sentence and um this should give us a output that is a vector of size again 512 because we are summing this number with this number this number with this number so the First Dimension with the First Dimension the second dimension so we will get a new Vector of the same size of the input um vectors how are these position embedding calculated let's see uh imagine we have a smaller sentence let's say your cat is and you may have seen the following expressions from the paper what we do is we create a vector of of size d mod so 512 and for each position in this Vector we calculate the value using these two expressions using these arguments so the first argument indicates the position of the word inside of the sentence so the word your occupies the position zero and um we use the for the even Dimension so the zero the two the four the five and the 10 ET we use the first expression so the sign and for the other positions of this Factor we use the second expression and we do this for all the words inside of the sentence so this particular embedding is calculated P of one Z because it's the first word embedding zero so this one represents the argument pause and this zero represents the argument 2 I and P of one one means that the first word Dimension one so we will use the coine giv the position one and the 2 I will be equal to 2 I Plus One will be equal to one and um we do this for the third word Etc if we have another sentence we will not have different positional encodings we will have the same vectors even for different sentences because the positional encoding are computed once and reused for every sentence that our model will see during inference or training so we only compute the positional encoding once when we create the model we save them and then we reuse them we don't need to compute it every time we feed the Fe sentence to the model so why the Alors chose the cosine and sign functions to represent positionings because let's watch the plot of these two function the you can see the P of one one means that the first word Dimension one so we will use the coine giv the position one and 2 I will be equal to 2 I Plus One will be equal to one and um we do this for this third word Etc if we have another sentence we will not have different positional encodings we will have the same vectors even for different sentences because the positional encoding are computed once and they used for every sentence that our model will see during inference or training so we only compute the positional encoding once when we create the model we save them and then we reuse them we don't need to compute it every time we feed

0:30:6 -  the feed a sentence to the model so why the authors chose the cosine and the sign functions to represent positional encodings because let's watch the plot of these two functions uh the you can see the plot is by position so the position of the word inside of the sentence and this depth is the dimension along the vector so the two I that you saw before in the previous expressions and if we them we can see as humans a pattern here and we hope that the model can also see this okay the next layer of the encoder is the multi attention uh we will not go inside of the multi [Music] po for sin 0 by 2 I th000 power 2 I calate you see but it is a static number it will be same as you saying 100 words will be the same but let's say sign of um for another [Music] okay okay for now let's so first thing that we will learn [Music] is first of all words sequences Sten [Music] [Music] Vector added [Music] to which are [Music] fixed vales Bing on potion that [Music] so okay now that's see [Music] pie [Music] not understand [Music] okay zero index to 0 1 2 3 this is [Music] quick for dimension of the output embedding space e [Music] [Music] for [Music] so if you take uh three the first number is 3 I that is six so s of 3 by 10,000

0:36:55 -  power 6 by 5 what is I man pause this position is K [Music] I is do okay okay okay denominator is always the same okay denominator is power and comma 2 i d d is the question is [Music] fix oh for sample purpose for then it is 0 0 0204 [Music] start looking inside equal to 10,000 now it's put [Music] [Music] other and [Music] okay nice very good let's copy this link did in this okay let's go next up during inference or training so we only comput the position in coding once when we create the model we save them then we reuse them we don't need to compute it every time we feed the feed a sentence to the model so why the Alors chose the cosine and the sign functions to represent positional in codings because let's watch the plot of these two functions the you can see the plot is by position so the position of the word inside of the sentence and this depth is the dimension along the vector so the two I that you saw before in the previous expressions and if we pl them we can see as humans a pattern here and we hope that the model can also see this P okay the next layer of the encoder is the multi-ad attention uh we will not go inside of the multi-ad attention first we will first visualize the single head attention so the self attention with a single head and let's do it so what is self attention self attention is a mechanism that that existed before they introduced the Transformer the authors of the Transformer just changed it into a multi-ad attention so how did the self attention work the self attention allows the model to relate words to each other okay so we had the input embeddings that capture the meaning of the word then we have the positional encoding that give the information about the position of the word inside of the sentence now we want this self attention to relate words to each other now imagine we have uh an input sequence of six word with a D model of size 512 which can be represented as a matrix that we will call Q K and V so our q k and V is a same Matrix are the same Matrix representing the input so the input of six words with the dimension of 512 so each word is represented by a vector of size 512 we basically apply this formula we

0:39:27 -  saw here from the paper to calculate the attention the self attention in this case why self attention because it's the each word in the sentence related to other words in the same sentence so it's self attention so we start with our Q Matrix which is uh the input sentence so let's visualize it for example so we have six rows and on this uh on the columns we have 512 column now they are really difficult to draw but that's say we have 512 columns and here we have six okay now what we do according to this formula we multiply it by the same sentence but transposed so the transposed of the K which is again the same input sequence we divide it by the square root of 512 and then we apply the soft Max the output of this as we saw before in in in the initial um Matrix notations we saw that when when we multiply uh 6 by 512 with another Matrix that is 512 by 6 we obtain a new Matrix that is 6X 6 and each value in this Matrix represents the dot product of the first row with the First Column this represents the dot product of the first row with the second column Etc the values here are actually randomly generated so don't concentrate on the values what you should notice is that the soft Max makes all these values in such a way that they sum up to one so this Row for example here sum sums up to one this other row also sums up to one etc etc and this value we see here it's the dot product of the first word with the embedding of the word itself this value here is the dot product of the embedding of the word your with the embedding of the word and value here is the do product of the word the embedding of the word your with the embedding of the word is the next thing and this value represents somehow a score that how intense is the relationship between one word and another let's go uh ahead with the formula so for now we just multiplied Q by K divided by the square root of DK applied the soft Max but we didn't multiply by V so let's go forward we multiply this matrix by V and we obtain

0:46:2 -  a new Matrix which is 6 by 512 so if you multiply a matrix that is 6X 6 with another that is 6X 512 we get a new Matrix that is 6X 512 and one thing you should notice is that the dimension of this Matrix is exactly the dimension of the initial Matrix from which we started this what does it do that we obtain a new Matrix that is six rows so let's say six rows with 512 columns in which each these are our words so we have six words and each word has an embedding of Dimension 512 so now this embedding here represents not only the meaning of the word which was given by the input embedding not only the position of the word which was added by the positional encoding but now somehow this special embedding so these values represent a special embedding that also captures the relationship of this particular word with all the other words and this particular embedding of this [Music] down [Music] here okay [Music] [Music] one is okay but um why is exponential that [Music] yes for happy happy please that's the dance [Music] [Music] [Music] [Music] and another let's go uh ahead with the formula so for now we just multiplied Q by K divided by the square root of DK applied to the soft Max but we didn't multiply by V so let's go forward we multiply this Matrix by V and we obtain a new Matrix which is 6 by 512 we get a new Matrix that is 6 by 52 and one thing you should notice is that the dimension of this Matrix is exactly the dimension of the initial Matrix from which we started six rows so let's say six rows each these are our words so we have six words and each word has an embedding of Dimension 512 so now this embedding here represents not only the meaning of the word which was given by the input embedding not only the position of the word which was added by the positional

0:48:47 -  encoding but now somehow this special embedding so these values represent a special embedding that also captures the relationship of this particular word with all the other words and this particular embedding of this word here also captures not only its meaning not only its position inside of the sentence sentence but also the relationship of this word with all the other words I want to remind you that this is not the multi-head attention we are just watching the self attention so one head yep you guessed it another talking head ad where somebody's talking about people making hundreds or even thousands per month self attention has some properties that are very desirable first of all it's permutation invariant what does it mean to be permutation invariant it means that if we have a Matrix let's say first we had a matrix of six words in this case let's say just four words so a b c and d and suppose by applying the formula before this produces this particular Matrix in which the there is new special embedding for the word a a new special embedding for the word b a new special embedding for the word c and d so let's call it a prime B Prime C Prime D Prime if we change the position of these two rows the values will not change the position of the output will change accordingly so the values of B Prime will not change it will just change in the the position and also the C will also change position but the values in each Vector will not change and this is a desirable properties self attention as of now requires no parameters I mean I didn't introduce any parameter that is learned by the model I just uh took the initial uh sentence of in this case six words we multiplied it by itself we divide it by a fixed quantity which is the square root of 512 and then we appli the soft Max which is not introducing any parameter so for now the self attention didn't require any parameter except for the embedding of the words this will change later when we introduce the multihead attention also we expect because the each value in the self attention in the soft Max Matrix is a DOT product of the word embedding with itself and the other words we expect the values along the diagonal to be the maximum because it's

0:53:27 -  the do product dot product of each word with itself and uh there is another property of this Matrix that is before we apply the soft soft max if we replace the value in this Matrix suppose we don't want the word your and Cat to interact with each other or we don't want the word let's say is and the lovely to interact with each other what we can do is before we apply the soft Max we can replace this value with minus infinity and also this value with minus infinity and when when we apply the soft Max the soft Max will replace minus infinity with zero uh because as you remember the soft Max is e to the power of x if x is going to minus infinity e will e to the power of minus infinity will become very very close to zero so basically zero this is a desirable property that we will use in the decoder of the Transformer now let's have a look at what is a multi head attention so what we just saw was the self attention and we want to convert it into a multi-ad datation you may have seen these expressions from the paper but don't worry I will explain them one by one so let's go imagine we have a so let's pause it here copy here one missing piece in this equation overall step this uh the V what is V Q KT is nothing but QT and then he's multiplying by V what is something we don't have we'll complete that course so this page is explaining on how step fun creating query q key K and value V vectors okay input uh to self attention mechanism eming vectors put token okay and three special vectors very key values okay from the CL process okay what is a CL process you begin with m during the training process the input data is passed through the first layer of [Music] happy you have search speee right up