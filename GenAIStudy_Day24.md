# Day 24 Study Time Gen AI

**Day 5: Study Time Gen AI**  
**Time Interval:** 00:00 - 37:25  
Summary  
- **üîç Overview**: This session continues the exploration of **neural networks**, building on the previous discussion about **optimizers** and their equations.  
- **üß† Understanding Neural Networks**: The instructor explains the structure of neural networks, clarifying how inputs transform through various layers to achieve outputs. The approach emphasizes breaking down complex equations into simpler, manageable components.  
- **üìä Transformations and Layers**: A comparison is drawn between neural networks and linear regression, highlighting how neural networks can create complex shapes by stacking multiple linear transformations, allowing flexibility in fitting data patterns.  
- **üéØ Concept of Complexity**: The instructor discusses the balance between increasing complexity in networks and maintaining alignment with various data patterns, emphasizing that neural networks can adapt to various shapes and dimensions without overfitting.  
- **üõ† Optimizations Discussion**: The importance of computational capabilities and optimizations is stressed, explaining how advancements have enhanced the effectiveness of neural networks while optimizing for resources like CPU and memory.  
- **üßÆ Regularization Techniques**: The session introduces regularization methods like **L1** and **L2** regularization to prevent overfitting, along with dropout as a method for enhancing model robustness by randomly omitting neurons during training.  
- **üöÄ Key Practices**: The instructor highlights key practices to minimize overfitting and improve model accuracy, such as using a validation set for early stopping in training, and exploring data augmentation techniques to enhance the size and diversity of training data.  
- **üîÆ Future Directions**: The session concludes with a discussion on hyperparameter tuning and plans to delve deeper into these topics in future lessons, ensuring a comprehensive understanding of optimizing neural networks.  

Overall, the instructor emphasizes the interplay between neural network complexity, optimization techniques, and model performance in machine learning.

# Transcript 


0:2:31 -  hey hi guys so today we will go through neural networks in continuation to yesterday's discussion that we had um yesterday we majorly looked at optimizers we looked at equation and how Neal neural networks equations are formed on what really happens I think anybody can calculate it as simple as that right X1 goes through different layers and finally it becomes Y in between different layers the X X1 X2 X3 are are transforming are put into different equations right that's the whole [Music] concept so instead of putting it in one equation we are splitting it breaking down and putting into multiple equations and we are using different weights for different one so like um I I wrote a Blog in which I was actually mentioning saying neural networks is nothing but um you're drawing multiple linear regressions on on top of each other and then you are achieving so what happens is if you want to get any shape all you need to do is if you draw multiple lines those small lines you can actually adjust it into any shape correct so some some different some shape like this let's say this is the shape of the data pattern that you need to recognize then all you need is the the line should be going like this right so if your each neuron contributes to identifying each piece part of it right smaller piece part of it then putting them all together what happens your whole boundary will be created correct so that's the strategy of neural networks that's the reason you you the more complex your neural networks are the more um more alignment it gets to different thing and and the key point is if if you start off if this are this or any shape right any shape of data that you have you can IR respective of the pattern as long as you you know certain pattern or it it could be in any Dimensions also you can always draw a boundary if you're using small bits of lines correct even if something like this is there then you can actually if you have small lines

0:5:5 -  that you could draw multiple lines that you could draw then you could draw it draw draw that line in every space right so you can adopt any shape that's what I'm trying to say the pattern if you want to if this is the pattern in which it is if you want to draw a straight line that's a problem a bigger straight line like this like then of the stra even if you have a curve like this is it will not fit into this data correct but um when you have a smaller lines that you can draw those smaller lines can fit in anywhere and you are drawing multiple lines like that and those can fit in the whole they they can take the whole shape but whereas a curve or anything else can't take the ship that's the concept I want you guys to understand it so there is no uh super intelligence in not you in in in in that I mean there is intelligence but I would actually say I will I will appreciate it is more than intelligence it is more on um we could achieve good things with neural networks and all that um I would say 50% is to the intelligence of people who have come up with these models and stuff 50% I would say it is actually the um compute and memory capabilities that have grown over the period of time because if you see the fundamental concept of neural network it is very close to how it was few decades back yeah so it's the same but how it evolve now how it how did the model become very intelligent no there are there are definitely a lot of optimizations that came came in which actually made the whole calculations easier people were optimizing things to an extent saying um with with the CPU and memory limitations to deal with huge amount of data and stuff people are optimizing it to a scale that it could what are the minimal amount of things that are possible so that's the reason we study so many optimizations right so great decent GD of Optimizer of all the data to stockes GD which deals with one dat one uh record at a time mini batch MB SGD mini bats stochastic

0:7:45 -  gradient descent which deals with the small small batches of it um like that um Even Adam if you look at it it it even though it does the best optimization it even it excuse me even it adopts a stockis stic model of um um are the mini batch model in Adam as well that's what you studied yesterday so all of this if you look at it it is um the focus were focus was always on adapting to the compute available that is there so uh but now that the things are changing we have better computer available better memory handling capabilities available um the evolution of neural networks is happening and you're actually able to see the results theoretical concept is there but you're able to see now now that the um more better compute is there I think we we should be stepping in to much more intelligent models than than this see um large language models or neural networks and all that stuff are just like um I don't want to say put it that way people people might get hurt but my understanding I might be wrong I will correct it if I'm wrong but it's more Brute Force kind of stuff where um you you pick up either all possibilities or you pick up you you use smallest possible piece and then you try to keep them repeat it hundreds of times so that you can take the ship that is possible by any algorithm or anything but here the adaptiveness to unknown scenarios by using that formula is is the key logic and um that's where you're able to see more Intelligence coming onto the table we'll go more into it when we reach generative way we'll dig more more deep into how things are working but this is my understanding on neural networks moving forward today we'll look at U primar let me share the [Music] screen so what we have looked at is uh optimizers all these optimizations we looked at

0:11:47 -  it uh I think we didn't look at when to use what I we did we did look at when to use what with the ster format and stuff but um Adam is the one which is mostly used uh okay fine sorry sorry sorry sorry um we didn't look at list of hyper parameters so let's go here what are the list of [Music] yeah um yeah so after looking at this I feel we'll come back to this first we'll complete the regularization Okay so techniqu think this page will work it will say I should subscribe [Music] something [Music] [Music] this guys again he'll ask for login what kind of but interesting there is something on langen very curious to start off langen [Music] but the Articles definitely analytics with the articles are very [Music] nice fitting under fitting regularization concept we have already studied respect to L regation so I don't want to repeat the whole thing I just want to see how regularization is done in um in neural networks concept see again even though it's as I said even though it's a neural networks neural networks is nothing but multiple neurons put into different layers each neuron what does it do each neuron has a linear aggression plus an activation function combination that's what gives you an output right so if you boil down to the granularity of regularization again the same regularization will be applicable you can go for L1 L2 regularization which you have studied [Music] in um what one is lasso another one is uh forgot the name again that's remember it's okay and drop out early stopping and more models become more robust and better at making accurate predictions on on okay this concept of under fitting just we have looked at it previously training versus just data error okay so the objective is to go here where the training is also good the test

0:14:38 -  data adaption is also good you can actually get better with uh minimizing the error in training data but the more you get better with training data the test data will go away so one one part of we already studied one part of the use cases we we don't have a definitive set which gives the complete pattern of the universal data so we take a part of it and deal with it so our objective is always not to achieve best performance on training data but to Reeve best performance on universal data uh which includes the test data as well so these two combination should be good I think the best combination of these two is here the M the best of test and best of training is here this is optimal model [Music] complexity so if you don't have regularization this is how it will come this is definitely over fitting because it assumes that the Red Cross lines are will be only available in these parts right but this is training data once the test data comes the red part spots can come anywhere and definitely this this will not align with it it's this is purely or fitting to this particular training data once the test data comes in it doesn't really fitting on what is uh [Music] so now what happen because of adding regation is nothing but you try to optimize it to actually take this line to to form this line but what happens is regularization we add certain additional value to the to the cost or loss in a way that you you can't go beyond a point or whatever you do you can't reduce the loss beyond that particular point so you we are literally stopping you from going to zero so the objective is to go towards zero but at the same time you should not go to zero because if you go to zero that means you are over fitting that's the whole concept of regularization so once we add the regularization terms what happens is the the line that you draw it would not tend to go that far that it adapts each and

0:17:22 -  every [Music] Point yeah this is appropriate fitting correct L1 and L2 so cost function is equal to loss function term the regulation term for L2 is Lambda by 2m into Sigma W2 s that is what is the variables if you look at Cost function it is cost function and W's is what we do and that's why we take a differentiation of cost function with W's and that's what we use as gradient descent equation and stuff right so weights are the changing ones in the cost function so what we do those weights it we'll multiply with the Lambda we will add it here so irrespective of adjusting the weights even if you increase decrease the weights those will actually [Music] be adjusting the weights will also update the cost so just by adjusting the weight you can't change the overall cost because the cost will always get ated directly and indirectly W also so L2 is w s Sigma w s L1 is Sig W that's all see this is the L1 L2 L1 is power one of w some and L2 is power two of w Square L1 L2 is clear now drop out as I said when you go to this shape each neuron tries to adopt certain shape correct of this this pattern Dropout is primarily saying you remove such certain neurons okay so what happens learning so this curve and this curve this curve this curve these all curves are adopted by certain neurons right right so that is where it overfits if I randomly remove certain neurons what happens few of those lines will be gone they could be gone here as well here here here anywhere but because of those lines not being there it wouldn't be able to sh take the exact shape as this one agree so prop out is where where it wantedly

0:20:57 -  doesn't want to remove certain lines so that it doesn't take the exact shape of the particular pattern that you are trying to achieve so that's the Dropout logic so they disable neurons like this overall like by the time you come to the end it would remove automatically it will remove certain pieces are required kind of stuff but um another question that would come out is how do you pick up what is your approach to drop out which neuron should you drop off yeah that's a that's a complex thing um we should uh look at it we'll study that as [Music] well has a different set of nodes and this okay yeah so see um let's study this is one of the most interesting also produces very good results and is one of the most frequently and used techniques in the field of Deep correct to understand drop out let's say neur networks is again in the one shown below so what does drop out do at every iteration it randomly select some nodes and removes them along with all of their incoming and outgoing connections as show below so each iteration has a different set of um nodes and this results in different set of outputs it can also be thought of an an simple technique and machine learning yeah assemble models usually perform better than a single model as they capture more randomness similarity Dropout also performs better than a normal neural net model the probability of choosing how many nodes should be dropped is the hyper parameter of the drop out function as seen in the image above drop out can be applied to both the hidden layers as well as the input layer in certain features can also be stopped this reason drop out is usually preferred when we have a large Network St not to introduce more randomness [Music] probability of dropping we can tune in further for better results using the grid search method again grid search method is something we we'll look at it when we look at hyper parameter tuning I think hyper parameters and H tuning can be cover today we'll look at it data augmentation simple way to reduce orating is to increase the size of the training data in machine learning we were not able to increase the size of training data as the label data was too costly but now it's let's consider we

0:23:38 -  are dealing with images in this case the few ways of increasing the size of training data rotating the image clipping scaling shifting Etc yeah the are techniques in which you can increase the training data by creating the data from The Source data itself for example images as as this said you can flip the data you can rotate scale whatever there are different things you which you can create the data and then there are libraries which does that early stopping that is if you see that if you run your code and then if your loss is getting to let's say don't have drop out you don't have regularization you you have uh the training going on to certain and number of they call iterations will when we see hyper parameters we'll study about it they call aox and number of aox and your your loss is so minimal that it is close to zero um then you know how many AO you are reaching there then what will do you reduce that epox to certain stage um and uh that will give you a better result but let's read it atly stopping is a kind of cross validation strategy where we keep one part of the training set as a validation set when we see that the performance on the validation set is getting worse we immediately stop the training okay so this is different approach than what I was saying so it's more a cross validation the image we will stop training at the dotted lines since after that our model will start our fitting on the training data okay here monitor denotes the quantity that needs to be monitored and value error denotes the validation error where do use patience patients number of epo with no further Improvement after which the training will be stopped for better understanding let's take a look at the image again after the dotted line each will result in a higher value of validation therefore fiveo after the dotted line zal to five our model will stop because oh okay okay so he says patience is equal to five is here sorry I didn't notice so from here fire a box he'll go forward and then he'll stop it here I don't know why he us crost validation is not cross validation one technique but as simply as that he'll see the uh he'll see where the number of appox is is going to minimum and from there he'll work backwards to get at certain