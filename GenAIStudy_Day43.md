# Day 43 Study Time Gen AI

### Summary of the Elmo Embeddings Discussion

**Time Interval:** 00:00 - 37:25

- **üîç Overview**: The session introduces **Elmo embeddings**, emphasizing that the focus is on studying these embeddings rather than discussing topics broadly. Elmo is highlighted for its novel approach to handling contextual word meanings, which is crucial for understanding language variations.

- **üìö Understanding Contextual Variations**: Elmo embeddings consider word meaning variations in different contexts, addressing how systems interpret the same word differently based on surrounding text. This session aims to explore how Elmo achieves this understanding.

- **üõ† Technical Foundation**: The discussion covers the technical structure of Elmo, including its character-based word representations and how these representations are processed through convolutional neural networks (CNNs) followed by bidirectional long short-term memory (LSTM) networks, leading to context-aware word embeddings.

- **üîë Key Innovations**: The use of bidirectional LSTMs is underscored, as it allows the model to gather context from both directions, enhancing the comprehension of meaning in sentences. The capacity to handle previously unseen words and accommodate spelling variations is also noted as a significant advantage.

- **üéØ Training Methodology**: The session discusses the training of Elmo as a language model with a focus on how it predicts word sequences based on previously encountered words. The importance of conditional probabilities in training language models is outlined to ensure fluent and coherent text generation.

- **üìà Performance Highlights**: The effectiveness of Elmo is showcased through various natural language processing tasks. The paper evaluating Elmo demonstrates marked performance improvements over previous baseline models, establishing it as a state-of-the-art tool in the realm of language understanding.

- **üîÆ Future Directions**: The discussion hints at further explorations into the intricacies of Elmo and its successor models such as BERT, emphasizing how advancements in pre-training models are simplifying the required architecture for fine-tuning, potentially leading to more efficient and effective NLP applications in future lessons.

# Transcript 


0:2:51 -  he hi everyone so we will um discuss today we we'll study not discuss I think to discuss study that is very clear we are not having any discussions or especially teachings with anyone but we go through Elmo embeddings which is very interesting and very important um especially this is the first embedding which brings in word uh variations with respect to context so word to back um fastex glow and stuff what we have observed is they they consider context but they consider um word in different different context and then they come up with um uh one single form um but um what we are talking about is um a word uh it could have different uh meaning in different context and how do we how does the system understand that uh how can it understand saying same word being there in different places and having different context should be understood as the objective so um let's see how Elmo is going to Elmo is going to do it um we'll go through some videos and blog post today we'll we'll just go through those videos and content there not not much of talking in this um let's focus on that so let's start off the this video Let's see into the rest of the model to use Coto Studio as an instrument plugin navigate to the Fair light tab add a new stereo track and rename this is not a simple embedding layer instead Elmo uses character-based word representations here's broadly the way that looks our inputs are given to us as a sequence of words we break these words up into their individual characters the characters are fed to a character embedding layer so for every character we get an embedding Vector the sequence of these embedding vectors representing one word is fed to a set of convolutional neural networks each with a different kernel width these are combined and Max pulled over the time Dimension and this resulting representation of the word is fed through two linear layers each with a highway connection which is a a bit like a residual connection but parameterized and from this we get a word representation now the benefit of

0:5:22 -  this is that our word representations are character aware so that if we see a new word that we haven't seen before and in realistic natural language settings this is a very important property because a lot of the text that you're going to encounter may be misspelled so for instance if you misspell the word accommodations it will still in terms of its characters look a lot like the correctly spelled word accommodations so the embeddings will still be closed together so this gives us our non-contextual word representations these are then fed into sequence to sequence model which brings us to the probably the main innovation of which is that it uses bidirectional lsdm so we build two multi-layer lsms one that reads the sequence forward and one that reads the sequence backwards and this because lsms are caal models this ameliorates the problem of only being able to look at the sequence in One Direction so both of these models produce contextual word embeddings one based on the uh left context of the current word and one based on the right context of theur current word now the question is how do we train this model we can assume that we have a large amount of natural language but no annotations so putting these two together gives us a full model we have a character where token representation the tokens that result from this are fed to two multi-layer lsms so now we need a way to train these lsms and that is done by treating them as language models we've seen the basic principle before we called it auto regressive model then but it's worth putting it into slightly more precise terms a language model in its most basic definition is a probability model of language it's a probability model that assigns a probability to a particular sequence of words or characters we'll look at a word level sequence to um explain the principle so what we want is some probability distribution that will tell us how likely a sentence like this is to be encountered in a given setting and the first thing we do is we break up the sentence into its tokens in this case words we treat each of them as a random variable so word one through word six are random variables and these can each take on as their value one of the words in our vocabulary and what we're looking for is the probability that the first random variable takes on the value congratulations the second variable takes on the value U and so on and so on until the sixth random variable which

0:7:54 -  should take on the value pre and in short hand we will write that like this now we don't usually have enough data to see even one complicated sentence multiple times so what we need to do is break this probability value at the bottom up into multiple component parts based on conditional probability which we can then estimate and to do this we will use this property of probability which is simply if you take the definition of conditional probability you can rearrange it to get this so the joint probability of X and Y is equal to the probability of X conditioned on y times the probability of Y and we can apply this principle to decompose a probability for a sequence of random variables into a product of multiple conditional probabilities that looks like this we start with a sequence of random variables as before we rearrange the random variables from uh last to first which doesn't change the probability and then we simply apply the rule that we saw in the previous slide first to the random variable W1 which looks like this we apply the same rule again to the random variable W2 which gives us this and so on and so on until we've decomposed the whole joint probability into a sequence of conditional probabilities now we can do this in any order but if we stick to the order in which the random variables are given the natural sentence order what we see is that we get a uh sequence of conditional probabilities where every word depends conditionally on the words that occur before it so the probability that we're now looking to model is this the probability of seeing a particular word in this case price given that we have seen the words before it in this case congratulations you have 1 a and if we can model this probability then we can multiply all the probabilities that it gives us to give us a complete probability of the joint distribution so in logarithmic terms the log probability of a sentence is the sum over all log probabilities of the words in the sentence conditioned on all the words that came for it and if you think back to what we talked about in the first slide outo regressive modeling you'll note that if we train a sequence to sequence model like this we feed it with a particular input and we train it to predict the next word at every time step by giving it as a Target value input shifted one unit to the left we train our NE Network like this we

0:10:25 -  have an output layer over the vocabulary that is soft maxed so that so that it gives us probabilities over the vocabulary we can interpret every single probability Vector that this output gives us as the probability that we'll see a word given that we've seen the words that come before us so the vector indicated here gives us the probability of seeing the word on given that we've seen before it the words the cat and set in that order so to summarize if we train a sequence to sequence model outter progressively with a softmax output layer what we get is a conditional language model now this might just seem like a fairly simple statistical model that would be easy to train but it actually turns out that if you look at what a perfect language model needs to be able to do it captures not just syntax not just grammar but also semantics for instance if we see the sentence the man fell out of the there are different ways of completing this and even a very simple language model can tell us that cycling does not fit grammatically so it should get a very low probability but it requires quite a sophisticated language model to choose between the options window aquarium and pool they all fit grammatically they're all nouns and this sentence is completed by inserting a noun phrase so language model needs to have a certain kind of grasp of semantics and common sense in order to know that a window is much easier to fall out than an aquarium and it's much more likely that people fall out of Windows than out of aquariums and if it's a particularly sophisticated language model then it can even tell you that a person is more likely to fall out of an aquarium than out of a pool even though both are so unlikely that these things have never been mentioned before in the Corpus that the uh model has never seen something like this described in the data set if it contains a certain amount of Common Sense and knowledge about the world then it can reason that at least an aqu you is something you could technically fall out of whereas most pools it's actually impossible to fall out of because they are embedded in the ground it's all of this is just to say that a language model as a task getting it absolutely right is what we call AI complete you need a complete understanding of the world and a complete intelligence in order to model the uh language completions uh correctly this isn't practically practically true but there are a lot of models these days that are coming pretty close to this

0:12:55 -  sort of thing so putting all of that together this is what the Elma model looks like we are given a set of inputs as words these are broken up into characters and fed to a CNN embedding layer which create gives us non-contextual Vector representations of these words these are fed to two separate lsdm language models one which reads the sentence forwards and one which reads the sentence backwards and both of these are trained separately as language models how do we then take the resulting model and do fine-tuning we take a weighted mixture of all the word embeddings produced in this Elmo model so the Elma model produces first non-contextual word embeddings which we've called H in it for every word K in our sequence we get a non-contextual word embedding HK in it and then for every layer in this forward lsdm we get a contextual word embedding and for every layer in our backward model we get a contextual word embedding so the way Elmo now allows you to fine-tune this is by summing up all of these contextual and non-contextual word embeddings in a weighted sum where the weights are parameters specific to the fine-tuning task so all the purple values in this formula are values that are learned specifically for the task we are fine-tuning for and then the the contextual word embeddings EK that follow out of this are fed to another sequence to sequence model that is designed specifically for the task and we won't go into the details here but the paper outlines a large number of tasks on which it evaluates the performance of Elmo and for each task a very particular model is designed to uh to solve that task and these are the main results presented in the paper so on the left we see uh a number of tasks like uh squant for instance is a natural language answering task where small snippit of Wikipedia text is given and then a question is asked in natural language about that snippet of text and the rest of the tasks are similarly hand annotated manually curated natural language processing tasks and Main result from the paper was first that if we start with a Baseline and then give that b Baseline instead of randomly initialized basic embeddings we give it the Elmo embeddings the performance improves massively and the second conclusion is that the performance improves so much that on each of these six tasks the resulting performance is

0:26:27 -  state-ofthe-art is better than the performance that was up till that point uh could be achieved so that's Elmo it's a system that does large unsupervised pre-training and that allows small scale supervised fine-tuning it contains as its basic structure a bidirectional lsdm but it does still require elaborate fine-tuning architecture so the architectures that achieve this kind of state-of-the-art performance on all of these tasks are still quite elaborate lsdm structures however what we see later after Elmo with these successor models like Bert and GPT that this pressure reduces that the better the pre-training model becomes the simpler we can make our fine-tuning architectures to the point where in most cases nowadays we can simply add one fine-tuning layer on top of our pre-trained model and that's enough already to to give us a very good performance now we'll look more into that in lecture 12 when we start investigating self attention it's very nice this guy is actually explaining it pretty well for good LM znm what okay hello everyone deep Genera hello everyone so today we'll continue with deep generative modeling and we'll move to the next class of generative models namely implicit models so a quick recap from the last lecture so we divide okay let's uh go through this ones to [Music] understand e Tex can be used to perform all for for going in that direction for in the last what you need is ameliorates the problem of probability of X conditioned on y times the probability now this might just seem like a for Let's test this do I have that's problem e [Music] [Music] [Music] I'm trying to uh run this program part model model should be download m [Music] for for for

0:54:55 -  [Music] for for SL for oh it's like he's trying to use ELO embedding and to prove that Elma embeddings are doing better than normal basic embedding um then you space What using for [Music] for for s for this one okay so go back to the system okay so uh El models are pre- trained and they available online looks like there are different variant [Music] of there somebody who should be the this guys Alpha TF Google Google is publishing let's let's see that for e e for okay so alen Institute is the m one for h I be in moreel e [Music] model has model [Music] theno [Music] for I don't I sound them for [Music] I'm trying to work with a something to Happ [Music] yeah for e for [Music] hope three e for for for e sh this [Music] this is this [Music] what for for for for for d for for for see s for for