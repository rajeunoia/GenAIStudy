# Day 27 Study Time Gen AI

**Time Interval:** 00:00 - 37:25  
**Summary**  
- **üîç Overview**: This session covers **Convolutional Neural Networks (CNNs)**, focusing on convolution operations, padding, strides, and pooling layers, including max pooling and average pooling.  
- **üß† Convolution Understanding**: The session delves into how filters or kernels are used to detect features like vertical and horizontal lines, explaining the associated weights and parameters that derive these features.  
- **‚öôÔ∏è Parameter Optimization**: The importance of optimizing parameters using **backpropagation** is discussed, detailing how to adjust weights in a neural network based on error derivative calculations to enhance performance.  
- **üîó Linking CNNs and Neural Networks**: The relationship between convolution operations and neural networks is explored, highlighting how CNNs utilize convolution as feature extraction and pooling for reducing dimensionality.  
- **üìä Feature Distribution in Neurons**: It is explained how multiple neurons are engaged during the convoltion process and how filters are applied strategically to manage and distribute features across the network.  
- **üß© Weight Reusability**: The session notes that weights from filters are reused across neurons, optimizing computational resources since they allow for shared learning of multiple features.  
- **üöÄ Next Steps**: The video concludes with an overview of future topics, including backpropagation, and a plan to implement learned concepts via coding examples of various neural network models.  

This summary encapsulates the core components and learning objectives discussed in the session related to Convolutional Neural Networks.

# Transcript 


0:2:42 -  hi guys um so to yesterday we we've seen CNN conation networks we looked at Concepts as a convolution padding um strides and um that's with convolution then there is pulling layer pulling layer has two concepts one is Max pulling another one is average pulling and um the majority of the Caps that we have seen is uh how do we pick up the um we have filters or kernels and these filters or Kernels have the numbers like you remember 1 1 1 0 0 0 -1 -1 for vertical lines horizontal lines he has explained something uh so if you call them as W1 W2 W3 w456 W9 so these are all weights uh like parameters and then these parameters we need to see how we derive those uh two things how we initially pick parameter and then m look at how we um optimize these parameters in a way that the the kernel performs in a proper way and then it aligns with whatever output that we require so um this weights are similar to the weights that we use in neuron only thing is we are not calling it calling out that it is a neuron here uh okay let's um so what do we use for calculating weights or optimizing weights in uh a neural network is most probably the back propagation um so back propagation uh can be used you order to derive the weights as per need so we look at how the back proc you know how the back proc bation will work in in a artificial Network for dtive calc W equal to W minus learning rate into derivative of loss respect to W right that's what we calculate so here also same formula will apply but the challenge is because it's not in a the convolution layer is not in format for of um not in a format of a normal neuron we would like to look at U um we would like to look at how the derivation would happen that's the only thing but if you look um look into it I

0:6:11 -  should put it on a paper somewhere it so how convolution and neural networks could be linked what happens in a neural network is give let's assume that all the pixels are features X1 X2 XX so one N byn means xnn X xn Square okay xn Square will be our last one like because X1 X2 X3 if should go so or let's say n pixels like the count of the pixels is n pixels so if you have X1 to xn if you give it to neuron what will happen input layer um each of the input layers will take all n of them and then they'll multiply but what we are doing is we are instead of giving all n to this thing we going in a different so X1 X2 X3 X4 X5 X6 X9 x 10 X1 X12 we taking a matter what so we're taking 12 features I means 12 pixels 3 3 into four image and uh now if we apply a filter which is is 2x2 on this then it will it will convert into it will convert into 1 2 3 4 5 6 it will convert into 2 2x3 Matrix as Z1 Z2 Z3 Z4 Z5 Z6 and filter let's say one let's call it as K1 K2 K3 K4 if you put it on it then what you're doing is so in um in an uh neuron we are passing all the features instead of it we are Distributing the features and we are passing it to multiple neurons that's how you should look at it but how do you distribute is basing on the kernels right so um that distribution itself I think that's that's where the it's um that's right write it in a straight line and see X3 X4 X5 X6 X7 X8 X9 10 11 12 we don't need all that but first neuron we'll

0:9:19 -  take X1 X2 X3 X1 X2 not X3 X1 X2 and then um X4 X5 that means here X4 X5 this is first [Music] neuron second neuron will take X2 X3 X2 X3 and xyxx 1 two and then it will go it will take uh X4 X5 X7 X8 X X8 that is third one third neuron and then fourth neuron will be X5 X6 X5 X6 X8 X9 fourth fifth will will be X7 X8 and X10 [Music] X11 F X8 X9 6 and X1 X12 this will six so this is how it will pick up the features 12 features that are there it it'll distribute it across six neurons if um if you have 2X two weights in kernel K1 K2 K3 K4 these weights will be reused across six neurons and matter of fact uh they will be repeated the weights will be same for all six neurons one good thing which is not good but anyways there is a after convolution and pooling there's also a feed forward Network which Isn so that's fine because there their weights will vary so which is okay but um here we are for the image it's more scaling down is the objective so makes sense scaling down or feature extraction so convolution is feature extraction uh pooling is scaling down and then Ann so the features are being distributed into the first layer that is what is being done in convolution convolution

0:12:9 -  and pulling together if you look at it it's it's can be done in one neuron it's convolution is equalent to the linear regression equation um that we do in neuron and then uh activation function on top of it is is similar to pulling that is activation function in this scenario either is Max of or average of these are the two function that could be used in the convolution so so in in short the um convolutional Network concept of convolution pooling is also a layer within the neural network only difference is usually in a neuron that what we do is we go for a linear regression plus an activation function on top of it instead what we are doing here is we are doing a um and then we and and then in normal neural network layer hidden layer we give all the input features to each of these neurons instead we are not because the size because the size of the uh input is very big we are we are splitting the um one second we are splitting it we are splitting it but if you look at the sum of this sum of see we get Z 1 Z2 Z3 Z 4 Z 5 Z6 if you look at the sum of them again we get K1 X1 + K2 I mean K1 X1 + K2 X2 um R K1 + K2 into X2 plus uh K2 K3 X3 K4 X4 like that we would get the same but some multiples of those PES are coming because if you do a sum of it so it's like altogether giving it into one particular eight itself mathematically but uh picking up a proper kernel will will help an extracting features is their concept okay but again uh I forgot see I said I one we talking about one kernel but uh the conation neur networks is not going to work on one one kernel it's going to have multiple kernels because one kernel will extract you only one specific features so if you have multiple kernels then this process that I'm speaking on

0:15:7 -  um will actually get repeated that means um okay the if they are repeated um um RB all of them that are repeated we are putting them into put putting them into single um the data set and giving it as input to correct so if you have three filters all of two and 2 by two size on a 3x4 Matrix then each of them will give you 2 by 2x3 that means six outputs and if you have three filters it is 3 into 6 18 outputs then the 18 outputs are individually consider X1 to feature 1 to 18 feature 18 and all 18 features are given as input to uh the neural network Ann then if uh you consider six one filter is six neurons so instead of making it 12 neurons we making it six neurons and then these six neurons if they are actually if you're using multiple um filters it will be repeated so it's actually more than it's it's it close to if if you have more filter actually it is more than giving all the pixels individually to your neural network I think is the calculation is uh little easier because we are considering we're Distributing the features four features at a time or we breaking down into pieces and then doing a filter one filter at a time yeah yeah I think it's the convolution overall part is is is a is to optimize it mathematically now in today's world if you have enough compute and enough memory to handle large images I think if you can give without convolution if you give it to n also it would work work in the same way this is some statement I should validate it somewhere only thing is is convolution introduced uh so Ann can do training on image and CNN can do training on image but um why CNN are being used is to optimize the calculation or optimize the

0:17:46 -  overall comput and memory requirements of a neural network that's the way I understand so we need to see um if we don't do uh especially convolution is nothing but using different kernels to extract different features which is what exactly the Ann also do they adopt to the input data and extract features understand the patterns and extract features so if they also able to do the same thing if the what is the performance of Ann uh compared to Performance of a CNN we need to see um I think CNN is more optimization of NN from the compute and memory respect to that's my understanding but let's have this as an open question let's discuss with somebody else uh to see this difference okay now we'll get to uh the back propagation that's the one that's spending I'm watching this video this guy has taken [Music] um I'll just quickly Bing you on on page so this guy has taken a 5x5 Matrix image 5x5 pixels image and then he used a 3X3 uh kernel or filter and he's calculating the Zed outputs so he got and he's using stride two so A1 A2 A6 A7 um sorry A1 to a13 and then a a A3 to a15 a11 to a23 a 13 to a25 these are four four um sets that are passed to this kernel and the kernel gives output as Z1 Z2 z34 okay that's the math and then he says instead of using umn to to simplify the explanation and stuff he he removed the bias term in z or same as if it is there doesn't matter and then he's directly calculating it with one output layer output layer with one neuron let's say some logistic function or sigmoid function or soft Max function or whatever we use that and then we take this input and predict an output

0:38:22 -  right which is possible okay NN can also have one single neuron you can still call it as so um what happen is here he I differ yeah [Music] actually it's it's good but uh thing is this is not that this gu is oh no there is not there just multiple me so this is cool so we get back pration back pration is clear we need not follow this guy with respect to so what he explains is all back propagation one two and stuff so back propagation we understood but uh thing is um yeah then CNN is done but let's look at his program good [Music] for this one for right is know for for e all I great for for I I you for what you all we to the two years 27 nov 2021 okay so that is cool um think and we're good we can move to RNN yeah we'll complete RNN we skipping the coding piece part of it which we should be supposed to spend some time on coding at least um alternate days I thought I'll pick up J in parallel but I'm still stuck here so okay once RNN is done then it will be coding and uh generative Ai and NLB so these three things should go in parallel uh we should Implement code for all the models that we have learned till now so we learned linear regression logistic regression na base de trees um

0:43:37 -  k mean clustering um I mean that's not a model but we've also learned about principal component analysis what else should we learn something we have learned we can't understand I mean de and random Forest are together uh boosting is also together with d and random for for itself um that's it neural networks CNN RN anything else that we have learned I don't think so I think we good that's all that's all we have learned we good so with that we'll Implement all these in code and see how they would work and um how the code in um tlow or libraries that are there how does it work and quick question I want to ask CH GPT is where is CH GPT now here it will be expired because I was not using it [Music] LO a list of libraries can use for building NE networks and a brief comparon okay can P CFE mxnet CH cntk okay so let [Music] this it's office Dynamic computation making it Pyon interface is more flexible St approach gring [Music] popular thats on top of to design for f [Music] experimentation cap by Berkley vision image [Music] boxing [Music] applications to appro and create table rating them from 1 to 10 10 is the best comparing on performance comp memory [Music]