# Day 21 Study time Gen AI

**Day 21: Exploratory Data Analysis and Machine Learning Planning**  
**Time Interval:** 00:00 - 37:25  
Summary  
- **üîç Overview**: The instructor marks Day 21 of a learning series, expressing excitement over the progress. Today‚Äôs focus is on **exploratory data analysis (EDA)** and steps needed before training machine learning models.
- **üìä Exploratory Data Analysis**: The session emphasizes the importance of EDA, specifically discussing methods such as **Principal Component Analysis (PCA)** for dimensionality reduction and understanding the structure of data.
- **üõ† Data Preparation Steps**: The talk outlines the necessary steps from gathering raw data to preprocessing, including checking for missing values and understanding data types to facilitate proper analysis.
- **üîó Statistical Summary**: It covers statistical measures like **mean, median, standard deviation**, and minimum/maximum values to assess how data is distributed, which helps in identifying outliers and understanding overall data trends.
- **üìà Data Visualization**: The session highlights the significance of visualizing data through **histograms, box plots**, and **scatter plots** to explore relationships and distributions effectively.
- **üìã Handling Missing Data**: The instructor discusses strategies for dealing with missing data, including imputation and deletion, stressing the impact on model training.
- **‚öñÔ∏è Outlier Detection**: Various methods to identify and manage outliers are mentioned, with an emphasis on making informed decisions based on statistical insights and relationships within the data.
- **üöÄ Next Steps**: The video concludes with plans to develop a structured approach for upcoming topics, including machine learning methodologies and more detailed explorations of neural networks and other models.

# Transcript 


0:2:42 -  hey hi um this is day 21 so very excited 21 days of doing this which is nice but I think it Go Long Way um I'm on mute like yesterday uh so what we'll cover today is uh more exploratory data analysis and so it's like when you're doing PCA which is actually Dimension anal reduction it's a it's also one of the pre-processing steps before building a model or training a model kind of Stu so I thought let's explore that in in little detail as well on what are the other things that we do froming a raw data from input sources and then to uh started training the data and kind of stuff right whatever we do in between let's um cover water all the steps maybe ask chat GPT to explain what all the steps that are usually done um that's one thing and we'll also cover try to put together plan on uh we'll move to the next stage uh officially today I'll call out the ml focused area P part will completed what initially I thought was to go with ADL first and then DL and then and then to NLP but uh NLP and and Jan is nothing but is part of an LP itself uh so we'll cover neural networks CNN RNN and other models as part of anal the starting ofp and it will also C so two steps of ML and then NLP these are two things we'll cover to get into J once we get into J is where we'll start um exploring more models frequently kind of stuff so in the end of the video we'll try to plan for it I think from today we should actually also focus on putting up certain time for each of the things so that it will be much more practical um the charging is low so uh 6:45 technically I'll have time till 7:30 let's say 45 minutes so maybe I can spend uh less than 4 should be good but I'll spend half an hour on Ed and U this

0:6:36 -  thing and the 15 minutes on planning so this will be for EDM that minutes let me share my screen let's get started um okay and I hope you able to see my screen exploratory data analysis the world looks complex but it's more on explore doing an analysis on data which explains what the data contains and stuff so that we are very familiar with the data that we trying to put in U which also helps us in taking calls what kind of a model or um what kind of features what kind of a model U we want to derive and then we will also have certain level of understanding on what is the corresponding y to given features um I mean we we'll classify whether it's a supervis follow man supervis problem like that we we can classify multiple things basic on that right so explain exp data analysising in steps as detailed as possible [Music] sure machine learning project bre on each into detail steps data collection gather ra data from various sources databases files API Etc um initial data inspection load the data into your preferred tool example python li like pandas yeah checking the size of the data set number of rows and columns display few initial rows to understand the structure and format then check for any missing values and data types of each column okay that is initial data inspection very nice statistical summary compute the summary static such as mean median standard deviation minimum maximum etc for numerical features minimum maximum and mean will mean median will tell us actually how much the data is spread uh and it's like is the data very close is the data spread like um the minimum and maximum values highly varying on scale for example if men is 0.1 and the max is

0:9:15 -  100,000 they they're they're distributed we can't say they distributed but the range is High um mean will tell us roughly saying uh if we are dealing if this minmax are actually out layers or are the realistic points so mean should be the mean if it is closer to Min plus Max by two that means the numbers are more distributed most probably not exactly but most probably and uh median also lines if minan plus Max by two and mean median are close by then could be the numbers are distributed more between Min and Max kind of stuff but that doesn't say exactly but what we can what can say standard deviation the standard deviation says um if the the mean is shifted to zero how much distribution does these remaining points have um relative to that particular zero is reflected by standard deviation so the average distance of all points from the center Z point is what is reflected so that gives a standard deviation that gives us actually how much distribution is that the higher the number is the higher the distrib distribution is that means they're further from the mean the points has a mean of certain value but these points are actually uh farther from the mean it could be it could be an indicator saying there could be outliers um the they're away from far away from mean so they are influencing the standard deviation mean median all that that will get be will be influenced maximum values also get influenced because of those values so those all can be identified by Computing the statistics for categorical features what we'll calculate is counts and unique values so we'll try to identify how many uh let's say categorical features we can actually find out how many many classes are there that analysis can be done on Cat and data visualization um so here what happens is uh even those two three steps sounds easy uh but the challenge with those

0:11:56 -  steps is it depends on the data right so it could be huge amount of data so at times we we may not be able to run or load that particular data into your session so we might not take the whole data into consideration we'll take a subset of data and do all this analysis and stuff um um but to understand this data better uh with respect to how the data is distributed how the data is varying um we would actually go with the graphical representation of this data uh to see how they are varying basing on y values kind of stuff so um we can plot histograms the histograms usually what we do is uh if there are n features and we'll go with the uh n into y n into n into one where we we don't try to put all the dimensions into one graph which it is I think it's not possible to plot a graph you can go for 2D or 3D cures but more than that is not possible we can't practically represented so at least on diagrams uning that feature what we'll do is we'll go with only um each features correlation with Y to represent how those features are varying with respect to Y kind of stuff so that will be the plot histogram and then we use box plots to identify out layers um and understand the spread of data like box plot will show you how much spread they are from the mean kind of stuff and uh they'll also represent the outliers that are there um create part plots or P charts for categorical features to visualize their distribution yeah so what we'll do in uh for categorical values is we'll represent a graphical representation of counts of individual classes so how many let's say there is a b c d as the values in a categorical feature then how many A's are there how many B's are there how many C's how many D's there it's more like group by of that particular and then taking count of the whole rows and we can also subclassify for each category what is the corresponding y value basing on that also we can actually put put that detail into graph

0:14:31 -  to see how much um correlation is there between Class like certain class giving 90% of Y POs to 10 negative this are kind of something we can look at categorical values to understand how they are behaving kind of stuff and if we see more categorical values that that's also an identification count of number of categorical values to number of numerical values because as I said the the primary in of going through all that is to uh take some calls on how do we do feature selection and how do you do model training kind of stuff uh the objective is not to train the model once the objective is to train the model um iteratively um so so as I said iterative you you make a choice of models you train them you you implement it on the ground you get some feedback or um or second thing is you you might also receive more data so now you you keep retraining the model so um the first time you set up a model preferably we need we can change but preferably um the expectation is um uh we we stick to similar model uh that means if if there are 20 features we picking up 10 and in those 10 features we are standardizing or normalizing with this SE approach we do certain steps right before training those all steps will be standardized saying even if I get more data I'll go with the same thing so it's not one time process it might become a process which will get repeated multiple times so considering that fact we need to um make a call on these points very in detail um first come first but the thing is uh before taking a call on what all transformations you do on data you should understand it better so these steps one this experted data analysis experted data analysis will explain you what those steps need to be done so we can randomly pick some choices for example you pick a range assign some classes assign something and stuff you can do it so all of them will look good like normalize everything to minus one to one or something like that you can do it it standard approach but um I you for example you can drop off

0:17:12 -  all out layers but um such certain calls will impact on your overall performance because U you might call something as an outlayer but for that particular feature it makes sense to have out layers so the calls are very subjective you can't uh generically take certain call saying okay I'll drop all out layers I'll um only pick up features which are which are having more count um or I'll just standardize the whole numbers so these are all this all will impact your output because let's say the Y is a continuous value not a thir value so then you need to see how closely you need to predict uh why so I mean of course you if if you scale X down you can scale also y down at the same time so that your algorithm gives you relative values with respect to Y but um the accuracy of the values will definitely be varying compared to scaling down a certain Val to scaling up to the actual values kind of stuff those all calls will be taken basing on the computation capability of your for your environment and also the excuse me and also the data size of um what you're dealing with is it um few gigabytes is it terabytes if is it pabes no no right anything is possible in real world basing on the domain that you have availability of data and all that stuff so data visualization uh you you create graphs and P charts for categorical values use catter plots to explore relationship between between Pairs of numerical variables plot correlation matrices to identify correlation between variables so correlation Matrix is something which we have seen in PCA where they calculate co-variance and basing on um covariance values they they build a graph which reflects um uh relationship between uh um between two features so what we do is we the N features we create n byn Matrix to see how is the correlation between all the features the as we did there dimensional reduction is the primary objective

0:19:46 -  looking at the covariance matx to see how many features are very close to each other so that the influence that they make if they're very close to each other when you train the model the influence they make on y will be similar because they're just numbers right like if it is 1 2 3 4 and the other one is like 2 3 4 5 the patterns is what we are trying to achieve the model right uh so if if two input features the pattern we we try to find is between the input features and why and within the input features if there are certain features which has similar pattern um if you find out the pattern for one number the the pattern for the other number is going to be very much similar right so uh the P identification of pattern is to find out why so if X1 and X2 have similarity to a level that they have same pattern to identify y why do you need to find patterns in two variables like you find out a pattern of X1 which will correspondingly map and give you y then you're good to go with it so that's the whole objective of dimensional deduction so after data visualization he says feature relationship analyze relationship between features using techniques such as Scatter Plots pair plots or heat Maps identify potential patterns Trends or correlations in the data handling missing data decide on a strategy to handle missing data such as imputation example filling missing values with mean median and more mode or deletion of rows or columns with missing data so um missing data means within a column certain data is not filled it is um null RNA kind of stuff so in that case um what would happen that data not being there uh in a given equation um like W1 X1 + W2 X2 + c equal to Y so X2 is null or zero or whatever let's say then W1 X1 + B will become y so it solely depends on W1 X1 whereas till that feature is W1 X1 + W2 X2 + B is equal to Y so there is the influence of X2 will be gone because of that being null so when you're training the whole

0:22:34 -  model the um W2 parameter that use output of your model right W1 W2 is actually the equation parameters which are actually the inputs of your model to run that particular equation to execute it right so um those will get the value will get influenced because of having certain values which are zero kind or n or zero orable kind so address it what we'll do is most I mean scenario that we should look at is uh um how many are there values and um again same thing for that particular missing values we can actually also see what's the kind of um relation between with the other features and stuff okay the closest features kind of stuff that will also give us certain indication on what should be our strategy to fill those values so there are multiple ways of filling those values either we can [Music] um uh um use a mean mode values to be replaced in the particular value so that they are within the range of the values and also doesn't impact the mean mean value much or in the standard deviation kind of stuff right so almost they dummy values because the the variance are standard deviation that the contribute will be zero because if you keep mean the distance between them and the mean will become zero that means the overall standard deviation or anything will become zero [Music] um they don't have much [Music] impact so handling missing data is something we looked at it okay we could also go to an extr saying these values are are critical so guessing them is a wrong idea and they are distributed very high as they distributed High they're not close to the mean putting mean there is not a good idea so we can also take an extreme call saying okay we don't know the data so let's not use the data so we drop off that data that could also be one of the strategies of missing the handling the missing data right