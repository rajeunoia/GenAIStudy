# Day 34 Study time Gen AI

**Time Interval:** 00:00 - 39:45  
**Summary**  
- **üîç Session Overview**: The discussion begins with an introduction to **embedding techniques**, highlighting the issues encountered (e.g., camera problems) and shifts focus towards the concept of embeddings such as **word2vec** and **fasttext**.
- **üåê Embedding Types**: Various types of embeddings are explored, including their significance in enhancing understanding and reasoning within AI models.
- **üìä Generative AI Exploration**: The instructor expresses a keen interest in exploring **generative AI**, encouraging attendees to understand **prompting techniques** and tools like **Langchain**.
- **ü§ñ LLM Understanding Goals**: The main objective is to understand how **large language models (LLMs)** generate coherent responses, with an emphasis on the underlying **Transformer architecture** and the evolution of models from RNNs and LSTMs.
- **üìà Response Mechanism**: The session explains that LLMs draw on a vast range of prior knowledge to respond to queries, often generating answers as if they are engaging in human-like conversation.
- **üîß Role of Embeddings**: Embeddings are critical for Q&A capabilities, and detailed attention to their mechanisms will likely enhance performance.
- **‚öôÔ∏è Practical Implications**: A distinction is made between roles requiring deep technical understanding versus those that focus on application, stressing that deeper knowledge is beneficial but not always necessary for all roles in AI.
- **üìÖ Next Steps for Exploration**: Future sessions will focus on specific embedding techniques, and iterations on different models like **BERT** and **ELMo** will be reviewed comprehensively over the coming days. The importance of understanding both encoder-decoder networks and the attention mechanism will also be highlighted.

# Transcript 


0:3:13 -  okay ouch looks like something is wrong okay anyways let's get started Let's uh look at U what we want to do um so we started with embeding and uh we looked at uh word to Vector I'm reading and uh there are other types of embeding as glow something is wrong with the camera it's pretty bad is this because of the okay I think it's because of the light that's okay so uh we have uh looked at uh embeddings as in um uh word to work glow fast text and all these multiple kinds of embeddings and um embeddings are pretty uh interesting they are actually um pretty good um they add a pretty good reasoning on what why um okay uh I'll also tell you guys is on one thing that is um on generative AI that I'm planning to explore so and which which which is one of my primary motivation on exploring gener to the detail um matter of fact if you if you just um I mean I not say it is very simple or anybody can do it but if you if you get into if you want to get into an understanding of that uh if you understand um how to prompt properly and um what are the different um prompting techniques how to find tune an llm model um and if you can understand the uh tools like Lang chain flow wise and other tools that are coming in if you can understand few of the agent Frameworks that are coming in right now like Deon DEA and multiple like there some crew AI these are all something that we need to explore the L um you're good if these understanding is you're good by but my fundamental question is that I I what I want to answer by end

0:5:45 -  of this learning is um how the uh llms are able to um respond to a given question that is what I want to answer so working backwards they say okay this is all basing on the Transformer architecture okay the Transformer architecture the basis of Transformer architecture is the attention paper that is there and um they are in turn they are there is an evolution from um encoder decoders and then there's an evolution from lstm and then there's from RNN and uh where did RNN come from so like that there is a flow uh we need to we need to understand the flow that's one thing I'm focused on second thing is how so if I ask a question it is able to tell me an answer as if humans are responding to me a question I'm sure uh it would have it would not have seen that question you the questions it would have seen a similar question but um there are certain questions which uh it would not have heard for sure like some questions which are outside the thing for such kind of a questions how is it able to respond right so and that to in a sentence that is uh properly formed grammatically and it is um it could be uh presented in um different ways so that it because there could be a question and answer in stack Overflow Wikipedia and all that stuff there could be question and answers to this so that's not a big deal but again it could um um present that question as if some person is some uh character is presenting as if a doctor is explaining as if a you explaining to a kid as if um you are explaining to a expert or any any kind of modulation is able to generate this definitely I felt is where it got some level of intelligence so the my exploration of digging deep why why is that I'm going going deep into understanding things is uh to have

0:8:16 -  um uh to to figure out that piece part of it I'm not sure we'll be able to find it is it is it something which is hidden people don't know it or something that okay so that's my intention of why I'm going very deep into each of these um topics um is it necessary for people to go into this deep I would actually say it is good for people who who are there in long term especially uh if you want to be part of a large scale implementation and stuff this level of a deeper understanding of the concepts is very good but if you are more like an applied AI where you take the AI models and apply them somewhere um um that means you're able to understand a use case and then you able to map this that saying in the AI space this is the kind of solution this solution is very AP for this particular problem statement that kind of a solution consultant or AI then AI implementation consultant that kind of a roles and stuff U you don't need to go this deep this this may or may not help you if you can go it is good but if you don't go also it doesn't stop from implementing your problem but this level of a deeper understanding will give you much um I wouldn't say but maybe even if this level of deeper understanding could be a basis for you to build your own your own llm models uh for somebody like individuals building llm models uh is practically not possible at this point of time because uh from the scratch is not possible because of of the requirement of heavy uh data that it needs um heavy compute that it needs and maintenance and stuff it's it's very pretty heavy so I'm not sure but I don't think individuals uh um Can Build capable Alum model so that's where there there are companies which are building uh which are investing heavy on this models and stuff uh to prove their presence uh smaller companies what they do is they take the current llm open source models and they fine-tune it they train further on it and are they make it specialized for a specific domain and stuff and then that's how they are that's how they are bringing up new models with new names but there is Baseline there is some model on top of

0:10:48 -  it similar to programming languages right there are the programming languages the way they have evolved is there is some basic uh languages like C C++ those all came in and people started building programming language on top of it so underlying the framework that the programming language uses like like The Interpreter um for net Java um compiler interpret for for these programming languages is underlying there is some baseline that is built and on top of it they have built other languages kind that kind of um pattern also ALS here there are some base models which which can understand language which can which can answer in specific language um which can um uh it's have a basic understanding of a language so in gen sense that kind of are taken as Baseline and those could be fine tuned to answer to to have knowledge it's very interesting for me to say that someone as having knowledge uh usually we use the word memory because there is memory and and and somebody has all the references of the memory uh but now we are talking about feeding in knowledge to models that's itself is great um okay so that's the part process but now coming back on that story what I have observed is um embedding is going to be a very strong contributor to to the so-called um point that we want to reach where it would say on um what is the how are the models able to predict uh um or generate answers for unknown questions that's the um point that we want to arrive at so eding is pretty critical so I want to spend more time on embedding um and understand how the embedding is is working to the detail so that um because that is partial part of my answer is what my my thinking goes at this point of time but uh by end of this journey I think we will uh sum these together understandings of how things are working um so even if you look at lstm and stuff so uh there

0:13:25 -  are group of words that are given as input for the given group of words we represent them as um some numbers these numbers for these words are I mean Baseline numbers are okay but the embeddings will end up producing for each word they give you a vector let's say um not just single number but they'll give you a set of numbers which in turn can be passed either to single layer of LSM or multiple layers of LSM so the vector can be distributed into multiple layers of but the the objective is that single word is not represented by one single number but um is represented by a a list of numbers and these list of numbers are the standard Baseline right for that particular embeding model if you do similar list of words you will get a similar vectors that should be it this should be validated once you have eding working session I want to see that I think it should be like that only because um it can be something different if it is what is your name the aming vectors that represent these words what is your name will actually have similar numbers that's my that's my understanding uh so but let's uh look at it but maybe if you use different embeddings the the vectors will be different like for example if you use um word to if you use um fast text if you use glow and all that you'll have different numbers for different same sentence with same words but um we need to understand that uh process um and also if if the embedding is done on glow and if the model um if you're using a current model in the model training has happened on let's say word to work how does it work so my thinking goes that it should not work the eding that the model is trained with and the and the embedding that we generate on on our own embedding that we generate if you pass it it will not be able to answer the questions as EAS as is my understanding so there is

0:26:19 -  embedding is def itely a strong prerequisite and it will be mapping to the model for example for chat GPD it has its own ambing end point um and usually what we need to do is we can't we can't directly they can't directly ask a question to it what we what it does is it takes our question and uh it it passes that question to its embedding service and then it will generate the vector for that particular sentence and then that is used as input to generate the response so that's how it is a two-step process um so embedding we have seen partially word to so but we need to cover deep into it um list of embedding techniques uh I think we looked for them this I need to move to excuse I is [Music] [Music] so I asked a question on what are used GP and so use contextual word emings that's the so we should understand what conts but cont input sequence large scale language model [Music] open source in there it's important to that [Music] well for okay [Music] for for for for for for for for [Music] these five things should be this this five things should be there has a list somewhere but this should be our Target okay once we understand embedding let's ask question I say in for some to s for [Music] go [Music]