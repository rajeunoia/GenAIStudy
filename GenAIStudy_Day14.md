# Day 14 Study Time Gen AI

**Day 5: Study Time Gen AI**  
**Time Interval:** 00:00 - 37:25  
**Summary**  
- **üîç Overview**: The session revisits previously covered topics in **AI basics, machine learning**, and **linear regression**, with the instructor aiming to finalize the discussion on linear regression and segue into **logistic regression**.
- **üìê Linear Regression Continuation**: The discussion on **linear regression** persists, emphasizing how to **calculate loss** and introducing **cost function**, particularly focusing on **mean squared error** as a critical aspect of refining algorithms.
- **üß† Understanding Loss and Cost Functions**: The session highlights the importance of comprehending **loss and cost functions** within machine learning. The loss function determines the disparity between expected and predicted results, while the cost function aids in model adjustment to minimize inaccuracies.
- **üõ† Practical Calculation**: The instructor details the method of **calculating cost** via squared differences, illustrating its effect on model accuracy and underlining the importance of these calculations for making informed modeling decisions.
- **üéØ Optimization Process**: The concept of **model optimization** is introduced by adjusting parameters (W1, W2) to lower the cost. The session explains the systematic approach to diminishing the cost function through parameter adjustments.
- **üßÆ Gradient Descent Introduction**: A preliminary introduction to **gradient descent** is provided, described as a technique to identify optimal values of W1 and W2 by taking incremental steps towards cost reduction, with an overview of its mathematical basis.
- **üöÄ Next Steps**: The session ends with the instructor's intent to explore **gradient descent** and its significance in optimizing machine learning models in the next session.

# Transcript 


0:2:33 -  [Music] hey hi um hi so um yeah good reflecting on what all we covered we think we have come to a good uh distance in in 13 days reasonably good I mean we could have done it much faster but it's like um getting in depth the few of the concepts and also covering majority of the concepts in supervised and supervised rning is nice but um few of the critical details are are missing um so I'll try to cover them today or tomorrow most probably um most of it is will last CH GP and look at answers online and then learn them in a better way because I've always been looking at it but I think there is always some some Gap um in the overall understanding on how people do it and stuff so I would like to spend more time and understand it um before moving forward I had one question when I was explaining came cling yesterday um I I thought that is the answer but I thought of cross validating it and and saying it today that is let's say yesterday when we looking on on the plane um so in the plane we were actually putting all the points around we picking up centroids and we keeping that point so in a single plane is okay that is is only for each point there is only one representation right like some one value on the plane right so I mean related to another point there will be only one point um or maximum as it's a plane we can consider two points two features X1 X2 but what if there are multiple features what will happen how do you pick up uh this thing is is my question so what happened is uh I think um uh the distance when there are multiple points um and how centroid is picked when there are multiple points um is um if there are X1 to xn features that means X is represented by X1 T and and features so [Music] one um point R one row in the data set has an fees and we need to label them with a specific so we should we should group

0:5:5 -  them into one category and label them into one one particular cluster right K means clustering you should actually you you end up creating as we have seen yesterday K clusters and then we should put that into one cluster so what we do we do it using centroids centroids are our means of segregating these into certain these many clusters so um croid clusters are um are used for segregating or clustering this whole data set right so um the centroids should if if you need to calculate distance from X which has n features to C C we can't pick C in um as one point like C1 and then we can't calculate distance that's not possible Right n featur so what we do when we pick up a centroid how many other features are there which represent the X we pick up the same number of features for centroid that is C1 C2 C3 all CN as one centroid each centroid will have n features based representation and we uh calculate the distance between c and x of M features by following the ukus distance um um which will be X1 - C1 s + X2 - C2 s + x3 - C3 s so on X nus N whole squ square root of this term is what is used as the distance right so uh that is used as calculation of distance and from distance we again as you guys already know we um calculate the distance and then we pick up which one is nearest we label it under that particular centroids uh cluster and then for the all those points we calculate a mean that means um uh if c will always be in that particular group of points right which are closest and so it'll be like let's say X2 X5 or x a XB all those are points whichin a group closest to a centroid C1 then uh these some of these points by let's say they 200 points 200 by 200 Point that's what mean is right

0:7:44 -  so you calculate 200 and then C1 becomes is equal C1 is equal to C1 plus all the X points by number of points uh plus one number of X points plus one so that's how we calculate the mean and we we derive the new C1 and that becomes a centroid and and from there we we again repeat the whole calculation that's how K means clustering excuse me works and that's what we have seen in that that website as well I should have bookmarked that website that website is actually nice that's okay we'll see and then um that's with K slust string so we good with K clustering so I kind of know how to I think we can I don't know how to write a program for K clustering also but I Feel Again C's clustering doesn't sound much like a intelligence right it's more like a um it's more like an algorithm uh it's more like an approach kind of stuff to find out which are points or nearest kind so is there Intelligence on it I doubt it but do you recognize pattern out of it yes so the whole uh machine learning as I told you is more about pattern recognization so and okay now we as we go to the next topic when we say pattern recognization what are we doing we are recognizing the pattern in the training data correct um but is what is our objective our objective is not to predict the training data right because training data we already have responses objective is basing on this data that is there in hand like the current X and Y value Val that you have tomorrow if you have an X which is unknown um say say let's say it's unknown it's not subset of this particular X um or it's not part of this particular x uh when that unknown X comes in how do you predict the Y value right so you have all these X and Y that you know here and you you got an X that X is not you you see if it is there here it's not there anywhere now how do you predict the Y of this axis you have pattern in the patternation you you are representing the pattern using certain equation right so this unknown X you put it into this

0:10:25 -  particular pattern uh equation and you derive y That's the whole concept that's how that's nothing but this is what is called complex terms they use models and uh predictions and all that will come but it's simply a mathematical equation which represents the pattern of the given data and when you give take the input and put it into the particular pattern it will um give you the Y value and that is your prediction so your objective of of you should be very clear why you want to understand the pattern pattern is not not about understanding how the data is is distributed or something like that the objective of machine learning uh and pattern recognition as part of machine learning is to P to predict a future unknown set of X that comes in uh which is not overlapping with the current x whiches that you have uh X data that you have and uh we need to predict corresponding way that's the whole objective that is where you say how much is the pattern recognition close to to what is supposed to be on y uh so moving forward in this uh topic um so we we discussed on loss right so you you thought this is the right pattern in which the X and Y are related right and uh that's what you you gave as an equation right I mean as per your thinking analysis or predictions or whatever you thought this equation this pattern is the right way to represent these points um but when you using that pattern if you for given X all the x that are there if you come up with some y um as your prediction basing on the pattern recognition that you have and if you see actual why that is provided by the team because it's let's say it's a supervis learning then the difference between them is what is you're losing right because it's supposed to you know already that it is supposed to be y1 and you predicted as yp1 so that means you are not right correct so there is difference in between YP and YP y1 and yp1 right y predicted one and Y actual one so that's actually considered as the loss right that's where you calculate the loss and then to be effective mathematically good

0:13:19 -  and stuff you calculate something called cost like loss cost everything is towards the difference finding the difference between Y and YP now let's talk of bias and variance okay let's let's ask chat GPT and let's learn about bias and variance in detail if chat GPT doesn't answer to the detail let's go to let's go to other website and Lear what is two important concept used to evaluate the performance of machine Bas me how closely the average prediction of the True Value let to simplify the underlying pattern and consistently under the target highas can lead to uniting where tot on training and [Music] test variance measures the variability or sensitivity production to changes in the training data a model with high variance so to small in the training data and random fluctuation of two patterns I variance can toting where the to memorize the training data well [Music] perform so that's the concept of bias and variance so I think we discussed it but in case if you have not discussed it um usually the data set I mean the simpler approach is um we get a training data set um like fundamental programming approach is always you write your code um to address a specific problem statement and you try to solve it through the code at same time you write some test cases which will actually test different scenarios within your code to make sure that it is working fine and it also keeps Integrity of code when you make changes to it and stuff right so similarly what we do when you get a initial training data that you receive uh with with X and Y we split it with certain ratio uh basing on the data size and stuff we take a call how do you want to split it uh it's like actually an pure um Call of the uh whoever is the model uh whoever

0:15:52 -  is doing the modeling he takes a call how much should be a training data how much should be a test data um so and how to split the training data and test data so that the training data test data has good number of um distribution of data kind of stuff those are all things which are solving right now but those are all the problems which has initially for example let's say you say 80% of my data is training data and 20% is that means let's say you have 10,000 records and 8,000 records you put it for training and 2,000 records you take it for testing purpose kind of so what might happen is let's say if you take first 8,000 records and next 2,000 records as test data the possible problem that you might see is the last 2000 records for some reason majority of them are on um let's say the output is classification and the output is apples and oranges then let's say the the test data is majorly on apples like out of the 2,000 let's assume 1,600 or or500 to, 1600 apples then what happens you you train on 8,000 that's okay which is having a mix of data but your test data it is little bias towards Apple so if the model is it actually evaluates more on the models capability on predicting Apple so um if you see anything orange it will say mostly most of the predictions on orange will go wrong because the the test data is is little biased towards apples right because test we we create the algorithm basing on trading data we calculate loss basing on trading data and stuff and we we make sure it we reduce the loss and cost kind of stuff and finally we test it to say whether it is eligible um to be published as a model which can actually predict generic incoming values and stuff so we test it with the test data and um if the test data is Biers to certain value output value what happens is the model uh in testing it fails because uh uh the expectation is maximum it tries to say it should be apples so what we do because it's failing for test data we go back on trading data and we we tend to fine tune it right so uh as we fine tune it and then come in test with test data again it it will only go through test data

0:18:25 -  when the when the predictions are much Clos closure to Apple that is even if there is orange if if if it is saying Apple for majority of the cases then only it satisfies test data and then the test data will go through and you you will be predicting properly and then publishing the model kind of stuff but what happens when you publish the model uh your your model because of this kind of a testing it is little bias towards apples and uh it would start predicting your orange and apples majority of the time it starts predicting saying they are apples so your test data is also very very important on how the categorization so what they would do is if you if you use the libraries like psychic learn or anything like that they just machine learning libraries whatever we discussing now they converted them into code and they put it there so that we need not write code for all of these these kind of actions that we do repeatedly initially people used to code everything Line to Line kind of stuff uh initially when I started machine learning long back I um we were writing actually code in octave uh to solve these problems U people used to write programming in R octave python Java multiple language in which they were writing code for this but what happened is there is some repeated code like like splitting a test data and training data um calculating LW calculating cost um deriving certain equations the lot of things that we do as part of our during data analysis we do certain things all of the what they have done is they have coded it and given it um as a generic method which the and build them all as a library so that um you need not reinvent the wheel right that's already there's already a code that is written the code again is optimized basing on different techniques that are published um so all that is adopted and stuff you need not redo all those things so they are there as part of libraries and um like few libraries like T are flow and all these libraries that you listen to tens are flow pyit learn kasas everything is actually is nothing but all these topics that we're discussing they have coded it and they have written it and this code um as it deals with um as it deals with huge amount of data and um it requires a lot of compute and stuff they optimize the um libraries code in a way they can actually process

0:20:56 -  huge amount of data and they can actually run on and run efficiently use the compute resources okay so that's the difference between different libraries why did we create multiple libraries that is the difference um like the getting into computer sources like CPU gpus and all that stuff out there right so efficiently using their capabilities and to uh run and deliver results will be the primary um concern into which the the uh different libraries are made okay coming back on bias and variance um and training data and test data uh so what happens is you you you create a pattern and the pattern uh our objective is to reduce the loss so you can reduce the loss um but the question comes down to how much you can reduce it so as we have seen yesterday um when doing K me's clustering um after certain point the centroid is not moving because the average and centroid is are very are very close at certain point you can see um how much it is moving and even the visually we can't see that it is not moving it's not that it is not moving it is it's still might be moving but maybe a little bit very uh small that we we can't even see on the screen like usually we might not able to see but it might be still moving the point is the more you you get to it the more you get to a position where it is overall loss is that small that um uh it is very close to minimal the you are actually your equation that predicts the pattern is getting much closer to the division like let's say classification um the the loss if it is getting closure to zero that means you are ideally categorizing the two apples and oranges kind of stuff right but what happens is that is what is called um um overfitting where for the for the training data so to put it simply focus on the training data um

0:23:34 -  if um uh your model or pattern that that uh is created that equation classifies um the training data the more it it it tries to the let's say the the loss is less means that means the Y and YP are very close by um and majority of them if it is if it is able to predict properly then what we are going towards is U the uh model is actually more um tuned for the train training data so then what happens if there is an unknown data that comes in especially if the unknown data is outside the boundary of training data the predictions are actually um the predictions are not might not be correct because the equation that you built is is specifically done for this training data as soon as you take it and put it into test data what happens is test data being outside the boundaries and preferably let's say that the training test data has certain values outside the boundaries of of the training data the predictions will not be good right because we we we what we do if there is um there are blue dots and then red dots we draw a line which is okay I can actually show you the image here so if you see uh yeah this is the standard example um so if you see this these points that are there I said what's your what's the pattern recognition of the given points is what we will try to if you look at it all three of them are trying to do pattern recognition the loss is basing on y to YP right so let's say these X are representing the Y that is price right so this line is our pattern which gives us YP if you look at all three diagrams you can clearly see that here first one the loss is High second one there is some loss it's not like perfect third one is perfect right so typically in programming what we do our object is always Perfection right if there are 100 test cases we try to 100 scenarios we try to program in a way that all the 100 scenarios are covered but um here in in

0:26:8 -  machine learning in the pattern recognition um we are not dealing there you know the scenarios you know the test incoming data and you you code for it here the challenge is where the intelligence comes into picture is intelligence is where you you infer the values for the future values that are coming in which might not be same are equalent to what you have in hand right now so you don't know all the possible scenarios so you you can't expect to uh fit in the current training data perfectly like on the third one and say that it is um it is correct because the future values you may not exactly have this pattern right you you might have 100,000 records like let's say you trying to predict the um uh potential probability of a person becoming a millionaire okay uh so you you you take certain features like what's his current uh credit scores what is his overall financial performance what is his um earning capabilities what is his spending capability there are a lot of metrics that you take and then you spend it but you you're doing it for 100,000 people how many people are there out there mean there there millions of people like then if you fit in your data to this 100,000 people out of the remaining million of people they could be anywhere and you're your this line that you're drawing here this might not exactly fit in for them right so those are those guys are there those guys are there could be anywhere so this is called overfitting where Y and YP are almost closed see this point is little bit outside but the remaining we are putting it like this we can tweak our equation in a way that the equation could actually cover all the points okay but this is overfitting so we don't want this to happen at the same time this is underfitting that is for training data itself if it is not if if if the predictions are that far from the uh actuals obviously on test data it will be bad most probably it'll be bad third even future data that is coming in we we are not even recognizing that we are not even getting close to a pattern of this

0:28:45 -  particular points right we are we randomly giving some equation saying okay this itself is is close to the points so that also doesn't work the second so here it is high bias so when high bias is underfitting high I is overfitting so bias is bias is where the values predicted are far from actual wise variance is somewhere where the predicted values on training data is very close to the actual values so bias and variance are the terms not not very important important but it to communicate with people like I I built a model it has I I think it has high variance so I'm I'm not sure how far it will predict the values for future at same time same same point can be said for high bias also saying I have created a model it has high bias I'm not sure how how how far it predicts the values for the future values that are coming so high bias and high varus both are problem so where should you go is low bias and low variance the the question will be how to create low bias and low variance right we'll get to that like what's good fitting kind of stuff right so good fitting comes in where there is loss but the loss is reasonably U minimal um that it it actually does predict values but they are close to the why not too far and when you when you say pattern pattern is different from finding drawing an equation for a given line points right so why didn't we see again all this math where we learned given points uh if you draw a line connecting all those points that actually gives you the pattern that's that actually gives you an equation for all those points that is possible given 100,000 points also you can draw a line which actually goes through all those points it is technically possible to arrive at an equation like that like

0:31:22 -  the way you see here right but we are not saying draw a line or recognize a equation which actually um which goes through all these points is what this not the term that we using for machine learning that is what we use in math right in math they give you five points and if there is a line that goes through this Five Points what is the equation of the line that's what they'll say so we derive that equation blah blah blah blah but here in machine learning from day one we are not telling that okay draw a line which goes through all the points we are saying find the pattern of the points so if you see you why this is a best model which good fitting is it sees that the points are starting as the size is small the price is small and it is growing in this pathway okay so that's the reason the input data set is also very important um for for getting a so for example in this points let's say if you remove if you if you remove this point right so what might happen is the line could go line could come like this like this it's also possible Right the line could come like this like could it could start from here and it could go like this and then it could it could go maybe from here straight like this that will also be very close to these four points if you remove this point so so the because this point has comeing here and if you draw a line like this which is covering these four points it will be further from this so what happens this point and your line prediction if it is goes It goes like this these two are far away that means you have loss loss is calculated basing on the overall points that are there so what you do you you push that line closer to this so that the loss is minimal right but at the same time we don't want this okay so what happens you you keep a line which is actually closure as much as possible closure to all the points um but not covering all the points so that that gives actually a pattern that's saying given this 100,000 points I think they are all distributed in space in this format in this curve right using

0:33:57 -  using different planes we say um maybe the these are all falling under these planes or if you if if you go with the decision tree or something like that which says actually these points are segregated are classified basing on these lines the set of lines not just I mean not necessarily we should recognize the learning with thees is that in linear equation logistic equation we are actually trying to fit in we trying to draw one line for all the features which are actually in multi-dimension if they are in one dimension drawing the line is easy but if they are in multi-dimension drawing a line is impossible then comes a plane um which is also possible but um even with one plane you can't you can't create you can't uh create a plane which goes through multi-dimensions right like too many you got so that is where to address as the number of training data sets that are coming in with multiple features are increasing they they understood saying there might be a need that we need to address these solve this will be a combination of multiple equations ASAS decision what happens first you you take one feature and then draw one line X1 is equal to a and then you use another line to to classified forther X2 is equal to B X3 is equal to C like that you you've seen the de tree right so that's how we can use multiple equations together also to solve the problem so because it's it's it's never restricted that you should go with one single equation you can have multiple equations the objective is to recognize the pattern so there if you look at it the the lines are like this if you put all of those lines together it's like you can should you should see all of them together forming like one one line or one pattern kind of stuff so finding the pattern is the major thing but not finding the line that goes through all those points so this is very very important Point um if you are there on the right side or on the left side you your model prediction capabilities to Future unknown are lower that's that's the way you should deal with unknown data right so here what they are doing they they know that there is loss they know how to fit in further but still they are saying okay this is good enough

0:36:27 -  this equation is good enough that it is closest to the points that has been given at the same time it's generalized generalized the on like you um AGI is the famous word in generative know that people are open and everybody's focused on going there the the full form of it is artificial general intelligence that is this AI or machine learning as I said it's more towards this data for a known data you fetch intelligence which is specific to that particular data like this and you predict unknown values basing on that that means that is how it is but whereas generalization of this particular model is where it not only solves the problem for this particular data it solves the problem for different data as well and it also is generalized in a way that if there are new points coming in it can actually accommodate the loss is not only minimal for this but if you calculate loss uh for the new point that is coming in for that also the loss is minimal so that's where you go for generalization the more generalized it is the more it could actually solve more number of problems imagine I create an equation which draws a line not only solves this data but it could solve data sets it's generalized that it can solve multiple data sets then it is the more it is accommodating multiple it is more generalized right like model that's solving this particular problem it's solving multiple problems kind of so means um I mean it doesn't work like that but I'm just saying there is a there's a model that gen that that predicts between apples and oranges and as we generalize it its capabilities are it can actually classify any fruits that's one level of generalization it can classify any fruits and vegetables second level of generalization it can classify any fruits vegetables and um maybe cooked food so your model as it tries to adopt more and more capabilities of classifying or predicting values it is more being generalized that means um um it can cover actually Higher Ground

0:39:2 -  with respect to either classification or regression predictions or anything like that it can cover Higher Ground the more it covers so now with ch GPT and gen and all that we we talking about is it's not specific to your use case previously it's like if you trying to predict something in medical domain there used to be models are specifically built for medical domain if you are trying to do something with education it's trying to do something with respect to uh um safety security vulnerability detections fraud fishing detections all this for everything there used to be models which are specific to the particular domain and the models used to be fine tuned to be very um um have high accuracy for the specific domain and stuff so now those are not sufficient because we ended up saying if you if you go online in in the model repositories and stuff there are there are almost lacks of millions of models which predict something and they are very specific to certain topic but the challenges in in a given topic also there are lot of models so um how do we maintain these models again let's say today we we build those models and then tomorrow if you get more more data this 1 million models either should be automated to who retrain themselves on the new data that is coming in or there should be some people who are addressing all these and they should manually update all these models to learn these things uh to predict something right so um it's uh a tougher um if it's a tougher exercise to maintain all these models to do certain predictions and and with evolving data like let's say you you train with the data that is available till 2000 and uh and now now if you use the same model and try to predict is is far different from 2000 so your model predictions will obviously go off right so you you should actually uh retrain that's called retraining your model um time to time so that your model is up to date with the latest information and it will it will um learn more from the mistakes it has done and also learns more from the new like like so this is the are the points and now what happens there are points like like there are a few points that that come in this level also then then what happens that means the prices are the size is

0:41:36 -  even though it is Big there are prices which are going lower as well then what it what happens to this curve is this curve actually bends down and goes in between these two or it might go like this and assuming this is the only order the rest of them are are are coming close then it would it would bend down because people are not interested in buying big siiz houses maybe and the prices will go down there um because the property tax in that particular State they would have increased it there could be a number of factors why but the it could change basing on adding the new data the the the equation will definitely change and U that will help us so adding test more test data is always good but generalizing uh retraining is continuous exercise but again retraining is a is a big exercise and it's a lifelong process it's not just one time when we sit and then say okay you do K points you cluster it and stuff is not good enough um it's going to be a lifelong exercise where data keeps coming and you need to keep rebuilding people got uh fed up it's saying how many models are there and how many models will we train and and how many people will take ownership of training them all the time because um let's say there's some more model that people are all relying on they would have put it somewhere and then published APA and stuff and then he is not retraining it you you are actually predicting basing on stale data that's not it's not accurate right so people were being pushed on building this generalized models which has multiple capabilities within it so um that's where the so-called AGI concept is coming in where um it has generalized intelligence in a way it can actually talk of multiple domain able to answer questions in multiple domains it adopts to those multiple things where we we need not even um worry about asking different different models for different solutions kind of right that's that's a but coming back on this uh highas uh High variance and underfitting overfitting kind of stuff so the next logical thing to understand is what would we do to address this right so usual uh thing is you if there is high variance then what they would do is they will actually try to add more data set data to it when once you add more data

0:44:18 -  to it it uh what happens is the equation by itself will um adopt in a way that it uh um that it tries to fit into the remaining points and it will it will move move away from these points right so that's the logic right so then it would it would more come towards the low variance side second on high bias what happens is so you you deal with variance using data uh there could be other techniques as well but whatever I remember on top of it I'm I'm giving you coming to high bias if it's very generic like straight line that you're drawing to gather points are highly distributed like that not like this but more distributed like this then we will actually try to modify the algorithm instead of going for sing Single degree first degree polinomial you go to multiple degree polinomial you will try to add more new features like that what you do you you you focus on improving your equation or pattern recognition equation in a way that it actually is gets closer to the points by using loss and stuff so that would move towards low bias so left Center and right center is the so this explains variance Bas high or fitting underfitting kind of kind of stuff so let's ask chat GPT also once and what are the different techniques used to address overfitting and under fitting are bias and variance addressing over fitting and under fitting uh are balancing andols various techniques in improving generalization we talked of generalization right performance of Eng models here are some common techniques regularization next regularization we we thought of talking about regularization right so regularization techniques adds a penality term to the model's loss function to discourage overly complex model so what happens we are doing Yus YP so we are trying to take Yus YP to closure to zero we don't want it to be zero correct Yus YP should not be zero

0:46:48 -  that Yus YP is z is overfitting y- YP too high is underfitting so we know how to go to zero we don't know how to stop some if we want to stop before underfitting and good fitting is there but we don't know when we are in under fitting when we are in good fitting right so um what we do we we take a small term we add certain term to the loss where whatever you do the loss is always there it can't make it zero right so what they do to the loss function itself they'll add certain additional penalty term right so the penalty term will say that boss IR respect of whatever you do the penalty there's always some value to it so people have don't have a choice how much ever they tune it they they can go to Yus YP um I mean they can they can overtune it to a way that uh it becomes Yus YP is equal to some negative value to take off the penalty as well but uh um that there's a different story I'll I'll say how the penalty also is is is is varying basing on the equation kind of Stu so the penalty is not always static the penalty that we add here uh is also varying how it varies and stuff we'll look at few of the regularization and most probably we'll cover them tomorrow because we are little out of time different types of regular and stuff those are also interesting thing and we should learn why there are different types of regularizations why why not penalties plus C loss function plus certain constant value see why not how we have different we regularization we'll see it so we have something called L1 regation L2 regularization LGE use techniques these techniques help prevent overfitting by reducing the magnitude of the model parameters okay now cross validation so cross validation technique is such as a kfold cross validation spr so what we do in Cross validation is we split the particular data set into uh smaller chunks of smaller data sets and when we train the data we don't train on the overall data so that is the pattern recognition doesn't work on the overall data you you put you break it down and uh we train the model for each of these uh points uh at at one go like you break

0:49:21 -  it down into five sets and you train the model for five sets different subsets of the data and we evaluate performance on five of them okay um there more reliable estimate of the models performance it helps detect our fitting bessing how well the model generalizes to unseen data that means um in of having trained set data and test data you are actually training itself you're doing in multiple data sets so for for one First Data when it comes it says okay this is it might be the line and if you that's an equation right so it doesn't fit it doesn't lie only for that points in the previous exis where you've seen the point pattern you've seen you have seen the line like this but what what you should see is it will go like go below like same equation it can it will go down and it will also go up for for you to look at it they have drawn the line only for the piece where you have equation but what happens it can go down go up so but what we are doing we breaking down those points into few points we we draw a pattern for each of those points but those that line they they are generalizing it they they try to re retrain it so that instead of going like this it might take a curve because of the new data set that new sub data set that is cross validation is breaking out to K fold right each fold comes in and then it bends that line in a way that it is closer to that particular subset kind of Stu so and everybody all the K points try to tweak the model in a way that it it is towards it is closer to all those points uh but separate it is not closer to to the overall data when you have 100,000 records if you want to draw a line which is um closer to all 100,000 the the iterations will also be very high and um in the when the iterations are high you you reduce the loss the more you reduce the loss you're going to overfitting so what they have done they have broken down all those points and now the iterations are low but you you you you train with all those subset of models and um you try to get closure individually to each subset so you don't get into this overfitting problem for all the points you you overfit to subset of the points but not

0:51:56 -  the overall points kind of stuff okay so that's cross validation I think that was more explanation but when we do it you can understand but cross validation is one approach to address overfitting under early stopping early stopping involves um the the early stopping again early stopping is like if you if you draw a graph and if you see how the each Iration is improving or reducing the loss like like the cost function you remember when we do gradient descent we start from one point and then you go down right we keep going down until you reach a point where from there if you go any direction the loss is increasing but not decreasing so you keep going down in a path where you where the loss is decreasing but if you reach a certain point that you move on any direction the loss is higher then you are uh good but what we could do is to loss is minimal or closer to zero that means you are overfitting correct and so what you need to do is you need to go two or three steps behind backwards right then you know that is where you are more generalized you're not overfitting so that's like early stopping you don't go till your lockal optimal you you stop like three or four before reaching the local optimal so model performance on validation during training and stopping the training process when the performance starts to degrade uh it helps prevent or fitting by avoiding training the model for too many EPO thus preventing it from memorizing the training data got it so um aox is nothing but the iterations the number of iterations in which we train that particular um the number of steps like in gradient distance if you see we do number of steps in K main lustering also we do number of um Loops right in which we move keep moving the centroid these are all are in future talk about NE networks and stuff but the iterations in which we do to retrain the training data set in a way that it it the pattern recognition is much closer to the actual pattern right feature selection uh technique came to identify and select the most relevant features for the model which discarding the relevant or random

0:54:28 -  features by reducing the dimensionality of the input space the feature selection helps with the risk of overfitting and improv model generalization okay emble models emble models combine multiple base models to form a stronger more robust model techniques like bagging random Forest boosting like boost gradient boosting and stacking combine this bagging boosting and stacking techniques are very important so I think um we should learn them clearly each of them what is bagging technique what is boosting technique and what is tag and we should also um see a practical example on how bagging improves performance how boosting improves the performance how stacking improves the performance second important thing that we need to learn there is when to use what when should I use bagging technique when should I use boosting technique and when should I use stacking technique when you go for emble models emble models if you recolor it's uses of multiple models of using one model and then like you you have some predictions to do like you like we have a health problem we go to one doctor right um instead of relying on the doctor's response you go to you cross validate it with not cross validation but you go to multiple doctors just to make sure that you the prediction is same if there's if there is differences then you should address why there is differences and you again reach out to these people or something like that right so it's like that like talking to multiple experts on a specific thing before predicting so each model let's assume that it is an expert so each model's classification let's say this model says Apple this model says Apple this model says orange then you you go with majority right or you have some incremental value you get predict the incremental value take an average of it or something like that on how much you so the model selection is also very important then pruning is technique used in Des is to remove unnecessary branches or nodes from the tree um to help preventing Orting so de trees what happens it's it's if you do it if you if you create a tree which has all the features in it then what happens it is it is perfectly fitting there's no way everything should be classified for the training set it will be 100% match but what we want is we don't want 100% match to happen so if you don't want the 100% match to happen What you need to do you need to remove certain branches that is the only way but how which branches to