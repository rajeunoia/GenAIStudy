# Day 30 Study Time Gen AI

**Time Interval:** 00:00 - 41:50  
**Summary**  
- **üîç Overview**: The session explores the fundamental concepts of **Recurrent Neural Networks (RNNs)**, emphasizing their application in sequence prediction and generating outputs based on input sequences.
- **üìñ RNN Basics**: The instructor explains the functioning of the base RNN model and discusses the significance of understanding variant models, focusing on how they process sequences of data (text, time series).
- **üß† Sequence Understanding**: Key insights are provided on how RNNs handle various sequences, including words and numerical data like house prices or temperature variations over time.
- **üîÑ Identifying Patterns**: The discussion includes recognizing patterns in data sequences, where the relationships between inputs (e.g., time series) drive predictions, explaining various sequence prediction examples.
- **üìù Problem-Solving Approach**: The instructor displays how to approach sequence prediction problems, highlighting the need to identify correlations and patterns, and suggesting practical exercises for better comprehension.
- **üîç Next Steps in Learning**: The session wraps up by indicating the importance of applying the learned concepts through practical coding exercises and upcoming explorations into handling specific RNN implementations.

# Transcript 


0:2:38 -  hey hi we have seen RN we looked at the looked at videos in RN I understood what is [Music] happening key things are um the base RNN model we I think we understood it well but we need to look at those variants how they work and stuff we should actually look at a sample output so that we understand better so like um sequence to sequence models or something where um the input is given so input usually is is taken from a series of um um input is taken from a series of values it could be text or not series it's time series are sequence of values like what I am saying now that those are list of words right similarly um house prices from last 10 years um average price of every month 10 into 12 120 prices um like that if there is a sequence of like our um human birth rate or whether temperature from last 20 years like that if you take the information [Music] so that information will be most probably whatever examples I said are time series so they'll they'll have sequence of values and and they show how they are varying and all that stuff right so what we need to do is uh these are actually more aptitude questions that you see online saying given sequence of numbers what is your next possible number uh similarly like given sequence of prices of a stock what is an exp posible stock given sequence of weather temperature what is an exp posible weather temperature or um given sequence of words what is the next possible word um like that the so what does it depend on is it depends on um um it doesn't depend on all the values it sorry it doesn't depend on each value individually um it actually depends on all the values and also the pattern

0:5:16 -  in which the values are varying like yes becomes b b becomes c c becomes d d is followed by E is followed by F and and stuff right so how does that uh what is the what do we solve that when we solve such kind of a problems what do we do we see a correlation between AB BC CD EF and we make sure that you the correlation that you assume between a b is applicable to I should I think we should actually try to solve uh some problem online which will be let's see number sequence problems for next number prediction find the next number see if you take something like this um find the next number in sequence 3 7 15 31 63 okay so what we will do to solve this problem we look at what are the numbers that are there first we look at three okay three so from three we can infer little bit is it's an odd number and um third number the number series and stuff okay then we have seven now if s is also an odd number which is okay and um how is seven related to three it's not a multiple of three okay um is it an ed addition of three yes 7 is comes after three is you added 4 to 7 then if Sorry 4 to three you get seven [Music] um and uh any other way we can represent division no multiplication no so all those are not possible so one observation that we had is um it is four varying okay very good now we move on so we we'll remember this saying three and seven are there and the variation the pattern could be addition of s now you go to the next step 7 3 7 now 15 comes in so what we will do we'll look at what is the difference between 7 and 15 7 and 15 if you look at it it is um there's no multiplication no division

0:8:17 -  nothing what is the difference if you see it is um is it four no it is not four then what is the difference the difference is is N9 sorry it's not 9 it's 8 so 7 and four 15 is 8 very good so 3 and 7 is 4 7 and 15 is 8 now let's get to 15 and 31 15 and 31 difference is 16 and but again same rule is not there and 31 63 the difference is 31 32 sorry so as you can actually see as you look at everything that you understand it is 4 and then it became 8 and then it became 16 and then it became 32 the next number will be 63 + 64 most probably it will be 127 so if you see the [Music] answer where is the answer yeah so I think this is these are all very good U but I don't know there is an answer for this [Music] find block is this the oh no the numbers changed um okay fine doesn't matter uh for solution steps click on the link below below ads below ads of 36872 okay uh 3 6872 is um now if you see 3 and 6 is multiple 2 6 and 18 is multiple 3 uh 18 and 72 is multiple 4 and um 2 3 4 72 5 is [Music] um 3 um 350 10 360 360 is um the multiple so 360 should be the next number let's see this guys okay take your time man take your

0:10:57 -  time oh previous there solution below I didn't see it properly okay so yeah 360 is the answer as I said so what it did three took and he has looked at is this two then it is like 3 is multiplied by two then six came so it has taken that factor see it can also look at like saying 3 + 3 six that could also be considered okay that's also one way yeah a solution one yeah it's here see that there will be multiple patterns within the series of numbers so we need to find out which series is the one which we need to use that's all so ecsn 3 into 2 is 6 and then um 6 into 3 is 18 18 into 4 is 72 17 into 72 into what some number should be there on the next one if you say think so then they look at the second level pattern that is 2 and three multiples what is the difference one three and four one we will we need to look at these patterns until we have a similarity across all those numbers like here you see 1 one11 one across the place so that kind of a pattern once we find out we can actually say okay this is what my next number is other solution this Ser is using a different differences results result correctness okay so 3 + 3 is 6 6 + 12 is 18 18 + 54 is 72 then we need to know find out what is the number here 3 and 12 difference is 9 12 and 54 difference is 42 9 and 42 difference is 33 so now if you assume 42 + 33 is 75 75 + 54 is 129 21 so X number for given series maybe 21 so I think possibility is technically that's also possible but the solution actual solution is 360 because of this pattern so maybe they should give saying there should be this pattern or something like that entered four separated numbers so again for forget about this but the intention is uh uh the the primary intention of this whole stuff is there is a pattern in which these numbers change in sequence and um

0:13:42 -  basing on if you give only three and six the pattern could be multiplied by two + three multiple things right so uh the pattern can vary but when you have sequence of data you you can pick multiple patterns as as as I said and those patterns we can should validate it when you go ahead and then see okay out of these four patterns that I thought of two are applicable and one is not applicable like that if there are multiple patterns you make changes accordingly okay so that is with the sequence of numbers pattern and all the stuff and then how it works and stuff well this is there um the expectation from RNN is same thing whatever we did manually now looks at three and looks at six 1872 so it need not necessarily remember three as is it comes to 18 it needs to remember that 3 into 2 is 6 e this or 3 + 3 is 6 this and when it comes to 6 3 it should validate whether 6 into 2 is it 18 no 6 + 3 is it 18 no then it should look at what is 6 18 6 into 3 is 18 now it can look at what is the difference between these numbers how does this pattern change right so that's also one way so you it can't be basing on one single Factor right it should be actually multiple factors into considered into to finding the solution so good on name of this we could actually do some um number series and so but number series could be complicated as well so for example the previous one we have seen we could solve it in another way that is you can it can actually be a 37 something right it could also be in a pattern saying 2 N minus 1 something like that where n is these numbers that might also work because you see the numbers are 3

0:16:23 -  7 15 63 so there are like um 4 8 plus one of all those numbers are 4 8 16 and 64 so all of them are two powers and reduced by one so that's also a pattern so for us by thinking by looking at those numbers because we know that two power is this we we think of it and we we come up with that factor but whereas in in a computer it doesn't know right doesn't know that there's these are all two power and something like that so uh it only way it could get to the detail is to understand uh uh try out multiple combinations again which combination will work out which combination is what we don't know which combination will fit into all the numbers also we don't know and there could be multiple as we have seen in the other solution there could be multiple patterns that could when you go multi level there is multiple factors that could fit in or we can squeeze in those multiple factors into the series and also der the number whether the factor is whether the pattern is correct or not will be visible only by predicting a number and then finding out the actual number for example when you have series and then you have 63 at the end we'll remove 63 as if we don't know basing on the first three numbers we predict 63 entry see if you're able to predict 63 then your your pattern is good then you can apply and predict the next number so that's how you can train the model and start predicting the future unknown values okay [Music] um so this is all good but now what we need to understand is for a given sequence of numbers or words or something or even image how does it train by taking input one at a time how can it predict the output for example especially the most interesting that we want to observe is when it when there is one word and you try to predict in sequence to sequence if you look at it you remember the diagram right so there are multiple neurons in

0:20:24 -  sequence if I give what is my name and give what or what is your name and then I give you what it's saying basing on what it will actually predict something could be my name is Alex something like that but um how does it know when when I give what it knows that I'm going to ask what is because what is your name will be my name is what is the meaning of something then my will not come so how does it know in sequence to sequence how does it know um which word should be predicted basing on the RNN um flow so that's um that's very important um so we should look at an example to see how it happens so let's try to see if we can find any example online that is the lazy way of learning things the actual way of learning things is to go write some code print the output in between and then see how it how it works right so then it is much better RNN Network an example sample data flow this could be this could be power of network [Music] let's look at [Music] it and then you'll go to diminishing gradients exploding gradients implementation using python we will try to build a text generation model using of a character given the preceding characters so we sample an X character from predicted probabilities and repeat the full Center this implementation is from and ra building a character level r n here we will discuss the implementation step by step initialize weights matrices U V W from random distribution and bias BC with zeros for to comp predictions compute the loss back prop to compute radians that is a and process same right let's assume we pick vocabulary say 8,000 and is 100 so input XT is uh [Music]

0:27:6 -  8,000 what is 8,000 again X I'm assuming X is input I think we would have explained them on the top but we didn't see hiden size is 100 100 neurons way output will be 8,000 implementation is to predict the next character there should be only one character right it should be like sequence to Vector implementation kind of stuff but let's look at it is leg get [Music] [Music] next character character [Music] Lev see B the man [Music] [Music] I to exp flow simple powerful model in practice it is hard to tr among the main reasons by this model is UN widely are Vanishing while training using BPT the gradients have to travel from the last cell to the first cell the product of these gradients can to zero or increase exponentially gradients problem refers to large increase in the norm of gradients during train R to the opposite Behavior faster Zer making it impossible for the okay text okay predict okay so this is sequence to Vector so this is okay we need to look at how sequence sequence sequence see how R is learning after few box ear Hope was on trained my first neural network okay and after here predict gives what is predict start Comm image the choice of activation function for hidden the output looks more like real text with word boundaries and baby RNN has started staring staring learning and toct the next few [Music] implemented in case you h is

0:37:52 -  [Music] sh not good very diff them it's for for on that's for for you should look at it in a big screen so so the input is going to be one Z One Z output to be 0 1 01 so this this is the network and it has uh hidden States or hidden size is three so the initial input zero errow weights will be there there for these are weights for uh L input way is X hiden byas STS okay we zero across minan State previous is um Z meaning of hidden state is actually uh the output of the previous state is hidden State hge okay and this is for output hidden State output plus these weights based then it will give output okay you use for actual weights of the input 14 range length of inputs for each input next [Music] here to [Music] one so they make character one basis zero and P of tan heads of input weights U and access T hidden weights into hidden so ux plus w h plus b by very nice that he is putting it on T tan is his activation function at this level and uh that will give these values 0.05 0.06 minus 0- 0.02 because it is three three size there will be three outputs okay now um Next Step

0:44:43 -  output output basing on this three this multiplied by this plus output bias these three will give output as this one so the probability Target is zero so loss here is um 0.69 665 so because uh 050 0 this three will be input here oh he's taking two variables and is representing which one is the value so first if zero is the value it will make this one as one and if one is the value this you will make this one as one so that's how he indicates which which one is one okay so that first value because Z is the value it will make one Z if one is the 01 okay okay the hiden layer output is um not good and that being multiplied by this this will give output this one Lo is more 1. 38501 interesting now 0 1 because one and then ux + W + V this will give you output is 0.06 t h and uh this into this plus this 0.01 0.02 [Music] [Music] again this so this is clear first pass through all layers what is the output and stuff not that it's output properties of roughly then this animation looks pretty cool but you probably won't [Music] uh for so this guy has done back propagation and he come up with some weights so U and W are different see U and W here are where where where where where unw here are .050 562 115 to 1652 whereas here it is different see numbers are different so with this updated weights if and Bas also has they add some Byers as well and V

0:51:0 -  output also they have is still zero here why is he saying L zero is default to zero okay in of making hidden layer 0 initially they're starting with 0.95 09 is one then it become 0 one it is one Z that mean zero super you should have listed the prediction also here to be more clear nice 0.190 the all properties 100 also 1 - one one one - one oneus one that's correct I can prend but is interesting to see in fact the more we start thinking about the more it's interesting that hidden underscore State learns anything state is ME to useful information [Music] about if you wait if you want hiden to be useful we'll have to give it problem where it's actually needed we'll modify our slightly and the next character okay now Network the opposite of the it have to take Special K to first or second oneic theut below 11011 okay animation progress okay so same thing uh code is all same weights may be different bi STS are also added okay one for one it give one for the next [Music] one it will give zero when 0 zer this time hidden state is actually uncoding useful information we can observe directly whenever the network is one in the cont network is the second [Music] one like this alls to Output the proper probabilities of each step desp same excellent man I love this this should be there somewhere you should remember this no no no [Music] edit am very excellent work this guy has done excellent work this is exactly what I wanted to do and he did it without putting any effort without of writing a

0:57:0 -  program to see all that but uh so u w v these three things are the weights input this is for hidden layer and uh this is for output layer when you [Music] go see remember this 562 1652 69 562 1652 69 see same thing is being continued even bias 0 0 0 Al same thing is continued so literally we are we are having one network that is being repeated multiple times for each input also taking in the to construction output of the previous input this is um exactly what we do what we do with the the time Ser I number prediction example of your but interesting thing is the weights of it is not multi-layer NE Network the weights are not changed at each iteration is the same weight multiplying the output of the previous one and the input of the one it is able to prict if you you can't this if you put more I mean visualization also not work if you okay is it to see how it will work for multiple layers like this this is amazing work amazing let join us at a live programming demo with this is just code M 2016 MH Cho here we go nice good job man good job yes okay anybody else who did uh [Music] better all pictures no demo nice this lastu okay we'll for today here we didn't even go to lstm RNN only we looked at it one more time it's very interesting now how same [Music] M if you look at

1:60:18 -  it tan h of hidden into now input into weights plus hidden into weights plus Bas and the whole all of it being put together into an activation function that's in turn given to next equation where it is again multiplied with the [Music] h0 let's try [Music] to did you look at it so the incoming for RNN uh let's pull a diagram for you to understand it better [Music] yeah B directional R interesting that's something we can see later on but uh single Direction where is it where is is it where is it where is diagram diagram diagam diam here yeah even if you take here so if you take here we can go to his page is that's excellent do you need all this nonsense we don't need yeah go to this diagram so X weights are u h from the previous hidden State H are W and output weights are V correct so forget about output weights for now let's talk of the what will happen activation function tan h of X into U + H into W + B but let's make b0 then it will be X Plus HW what is being considered and once you that is at this layer level first layer okay so let's call uh X as uh X x0 H as h0 and U our X will call as one because h0 is coming from the previous step X1 first input t h right U and W will be same now this will become H1 correct yes this will become H1 uh and H1 given to V like V H1 + uh B

1:63:34 -  um V and B bias being zero this and again maybe they'll apply some tan H or something soft Max something on this and this will give us um output but this is specific to one iteration or one time step right output is equal to Output weight into hidden State plus output bias okay he he's not even doing any uh function soft Max or anything on top of it just directly doing this okay which is okay doesn't matter for us probabilties np. exp of output by NP do some whatever okay um now coming to this layer it will become tan h of X2 into U + H1 into W if I replace H1 here this is H2 X2 into U plus H1 is tan H into X U plus h0 W X1 uh into W again because tan H is there we can't send this inside but if you look at it it will be a loop so for third the fourth it will become x 3 into U + tan h of X1 X2 U plus tan hit into X1 U plus uh h0 W into W uh W and again into w so into W for this and then into W for this and into W for this three it's w Cube U is U is only X3 into U plus tan h of X2 U + tan h of no W Cube will not come because there is a tan H outside this this bracket is tan so w can't come outside tan h of w so w into if w t h x y it's not like that yeah you

1:66:26 -  can't say t h of X Y is t h x into t h y I don't know let's not go there so this is the equation if I calculate once instead of it rating also if I Cal this once I'll get the direct output of the third iteration the third time step similarly if I go iteratively it will become so if you see here we can clearly see X1 is there in this equation for third iteration um I would like to show you guys this but I don't know how to show it it is on my book but um so I'm not sure if you guys can see it but uh this equation what a big deal now let me send the photo so if I is it screen share it's not sorry if I go for screen sharing let's pull this same if we look at it what we are doing third step see the the first step is here H 1 is equal to tan h of X1 U plus u is weights for input ho that is initial input that is given H4 usually can start with 0 0 but final prediction that we see that it is not ideally to be 0 0 into W plus b hidden that H bias will be there but let's say we assume it is zero then it is XU plus h h X1 u+ h0 W H2 will become tan h of x2u + H1 into W where H1 is tan h of X1 U + h0 W so if you keep it it will become like this tan h of X2 U plus tan h of x X1 U + h0 and you if you keep going like this if you see X3 tan h of x3 U like this plus H2 W where H2 is this one and in which H1 is this so if I keep here this one in place of H2 this is this into W so what will it have is tan H any time x and we'll

1:69:2 -  have tan h of xn minus1 into U plus tan h of x - 2 into U plus like that it will go till h0 so what what is going to happen is you have X3 X2 X1 and h0 how many of steps you go those are common and uh U and W are also but the only thing is they're multiplied inside t h function so we can't pull them out but U and W is also same so U for a given inputs we are actually putting all of them into single equation so each step the output that is there we can also predict it um in one go using these equations that's also possible one thing is we instead of doing all those math in one step we are dividing it and conquering saying we calculate one tan H first and then tan H inside with so that means we calculate this tan H first take the output H1 you put it here into another tan and I mean tan is activation function but yeah here let's assume another tan H and this you take it H2 you put it into another tan h of like that if you you keep going to multip levels uh and you derive it but always the equation of H is dependent on inputs X1 X2 X3 X4 X5 those U weights of x w weights of hidden layer and um and the ho the initial input uh ho hidden lay output that you give is imaginary there is no step before it it's zero step but we give some value there which could be zeros or whatever um and that only those are influencing what is going to be the output of the particular step which shows say there is influence of all the inputs on every steps output and also the ho initial input that you give that's how it works nice man I love this love this RNN okay I can I did stop sharing yes I did stop sharing okay I'll end the video now