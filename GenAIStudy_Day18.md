# Day 18 - Study Time Gen AI

**Day X: Study Time Gen AI**  
**Time Interval:** 00:00 - 37:25  
**Summary**  
- **üîç Overview**: The session continues the study of various machine learning techniques, including SVMs, decision trees, and ensemble methods like random forests. It focuses on metrics associated with classification problems and introduces regression metrics.
- **üìä Classification Metrics**: Key metrics such as accuracy, confusion matrix, precision, recall, F1 score, and ROC curve are discussed, highlighting their roles in evaluating classification model performance.
- **üìà Regression Metrics Introduction**: The discussion shifts to regression metrics, specifically mean absolute error (MAE) and mean squared error (MSE), and their importance in quantifying errors in predictions.
- **üîç Understanding Mean Error Metrics**: MAE is explained as the average of absolute differences between predicted and actual values, while MSE focuses on the mean of squared errors, amplifying larger errors to provide deeper insights.
- **‚öôÔ∏è R-squared and Adjusted R-squared**: The session explains R-squared as a measure of variance explained by independent variables and how adjusted R-squared accounts for the number of predictors in the model.
- **üîä PCA Discussion**: The session touches on Principal Component Analysis (PCA) as a dimensionality reduction technique to improve model performance by identifying key variations in high-dimensional data.

The session concludes with an emphasis on enriching understanding in upcoming lessons on PCA and its significance in the model evaluation and feature reduction process.

# Transcript 


0:3:4 -  hey hi everyone so in continuous continuation to our um study um towards geni we have covered the flow machine learning umch learning linear regation l d trees random forest svms and um we also covered um I should gra my and also close the door [Music] better [Applause] second so um we have covered uh um svms forest and then svms what else did we cover May base probability based distribution and um that's it I guess yeah that's what it said and then we looked at variance by under fitting over fitting and then we look at metrics on how to measure these um oh sorry we we've also looked at unsupervised K means clustering algorithm and um we looked at um uh yeah metrics to measure this uh model accuracy uh we looked at classification problems and for them there is accuracy there is uh uh confusion Matrix expression recall F1 score specificities which is nothing but um it's a one minus recall of negative and then uh we looked at Roc Roc graph and uh a you see area under curve um so that's that's the thing metrics that we have classified for classification so today we'll cover the metric for um regression problems and how do we cover it most probably we have we looked at it already so it's quick recap that is uh uh there is something called ma which is mean absolute error which we have seen during linear regression times um that the absolute values are being um taken mean of the absolute differences in between Y and Y predictor and stuff um we also have looked at MSC mean

0:8:51 -  square error that is absolute uh difference squares mean of uh absolute different squares okay so let's look at them and then study okay let me scare the screen for [Music] okay let's look at um there a blog post man did I say it or not maybe I would have closed it it is actually a good one explained all this metrics okay now let's look [Music] at in absolute error that's when discussed this is the form y of [Music] y mean square is sum of 5 - y Square by [Music] n root mean square is square root of [Music] mud same as the target varable often preferred because it is more interoperable than Ms as it is the same units as Target varibles okay okay mean absolute percentage error may be measures the average percentage difference between the predicted values and the two [Music] values [Music] a me of the relative accuracy of prodction and is useful the different skills coent of determination r¬≤ h r squ this is important coefficients of determination r s r squ measures the proportion of variance in the dependent variable that is explain some of squared errors and sum of total sum of squares by total r s Val ranges from 0 to one IND getes a perf fit and z indates these matx [Music] r s coefficient of determination useful [Music] explain Google also same time okay [Music]

0:15:57 -  sotion dependent variables means how well the independent variables explained the variability of dependent variables the ratio [Music] exp s squares from 0 to [Music] 1 dependent variables independent should understand what it is meaning that it does not provide information about the absolute model Bas model hold on [Music] hpal [Music] brael brael okay let's see now um top Val a common type [Music] of directly jumped into top RSS ISS value and TSS isus y Das s where Y is actual value and Y dash is the mean value of that is like X1 + X2 plus X3 plus X use doesn't even make sense what does it mean is that being used as a metric to score to get skill just like in classication impl no there is something wrong so actual values predict values here he says this is are actual values these are predicted Val this is fine is the mean value of the variable SL feature other features where is the mean value of the features here I don't even have mean value [Music] features we should get to the math of it R sister R square R equation [Music] below the mean value will be considered the Baseline value the Baseline Arrow as the square differences between the actual Y and the mean value oh does it mean it is like uh

0:22:21 -  mean value is mean value is y + y i by [Music] [Music] [Music] 2 yeah it's just like [Music] R sare this equation [Music] samples number data [Music] samples here R squ just R square sare mean mean absolute error standard error mean absolute deviation Maxim res error root relative square error Bas information CP the corent [Music] this is good this is [Music] good coefficient ofation sare start of the finding residuals point the predicted y value plugging in the corresponding x value equation okay is um [Music] [Music] negative res is [Music] positive okay [Music] res Square to Sigma to find the Y first to the mean of Y [Music] values 2 4 6 7 me of Y value c means we of actual values means we are looking at each values distance from the mean how far are they distributed from the mean values minus one minus so Mouse residuals by squares mean of actual that's what he supposed to [Music] write this is nice very [Music] clear some of squared some of square

0:32:11 -  error residuals differences by sum of um squared actuals minus mean of fact that's called um R sare last time also we have seen this forgot we forgot to next time we forgot that something called is there itself it is not nice R square and adjusted R square R [Music] square okay I think we' covered everything adjusted R square we can see if you want to you only need [Music] H is the number of points in your data sample is the number of dependent this will penalize you for adding independent variables morees to the data as you think of them will be significant but you can't be sure that significance is just by chance they adjusted Square Ro while are usually POS now here it is clear now so if you look at this individual values is okay [Music] data for number of predictors be sample size means if there's an equation for example example of a let's look at example intercept intercept 100 done data sets e um adjusted R square maybe yeah just to be clear that's why why to leave okay let me share the screen it better not the question hospit model 0.5 0 100 observation you want to multip B such as size location let's say number of features that's all so same number of number of all features that's all X1 X2 X3 number of features the number of independent variables prct in the model what do the r sare k represents the number features or inputs in

0:35:22 -  machiney okay variables krond to the number of coents mod content to model so he says number of variables and number of questions are all the same okay okay represents number of features equation regardless of these variables could be numerical features categorical variables [Music] registered that's a more accurate ah okay now we good um without we thought we could cover um we thought we could cover PCA also PCA is an important topic so let's try to brief pcnc but to repeat on what we have done with respect evaluation of regression models is we looked at um residual is nothing but Yus y or error with respect to what is predicted to what is the actual value minus Yi um um ma is mean absolute error that is absolutes of the Y Yus I so that the negative uh differences doesn't impact the positive differences and Versa and um uh mean Square errors MSE is where you have mean of squares of differences residuals uh um that actually helps in um amplifying the ones where there is more differences uh root mean square error is done rmsc is to uh take a square root in a way that it it actually equates the values closer to the um actual y values um so the diff the the differences are closure to actual y values we are amplifying the difference but at the same time we reducing the value and we are saying the value that is root square error and um then R square and adjusted R square R square is uh where we have 1 minus um sum of squares by um

0:38:20 -  actuals minus mean of actuals Square sum of um actuals minus mean of actual Square um why are we doing um coefficient of determination is what R square is called but the question is why are we doing this this Yus IUS mean difference between the actual value and the mean of the actual values will give us more like distribution right how much it is distributed from the mean how how much this values are distributed uh to that of uh PR errors now becoming dependent on the number of data sets and stuff right if you do uh distribution let's say you add one more the mean will will become more and your base will become more and then if your base becomes more uh this becomes lower and your accuracy will increase so just by adding more data to the data sets we can actually increase the R square error um you adding more features will also increase your actual prediction um sorry you adding more features will also increase the uh y value not necessarily right more features doesn't mean this could be negative it can reduce also I don't know how number of features will influence the number of features that's the only way it number of features see y actual y actual minus mean of Y actual all of them are standard values they are not variables they are non variables right the variable one is the um why predicted because y predicted depends on features features can change and the weights can change so way predicted can change predict is the only one which will change when we change the number of features uh are the corresponding weights the by predicted will change the rest of the equation will always same so n- 1 by nus Kus 1 so number of features is okay uh but number of features

0:42:8 -  minus uh sorry uh number of data set value minus okay andus one this is making me curious we should listen to an explanation from somebody on R square and adjusted R square okay I'll try to find out more information and I'll let you know guys if I have anything interesting to share tomorrow but meantime if you look at uh PCA principal component analysis it's very called system and okay principal common analis dimensionality reduction technique widely used in machine learning and data analysis to transform High dimensional data set seem to be a lower dimensional space while preserving the most important information in the data PCA achieves uh this by identifying the directional principal components along which the data varies the most and project varies the most and projecting the original data onto onto this components is a detail explan of PCA motivation in many real world data sets especially those with a large number of features there may be redundancy or colinearity Co linearity I don't know if L second L is important there but among the features making it difficult to visualize or analyze the data effectively PCS to address this issue by reducing Di data while retaining as information as possible okay there are too many features we are trying toce that's all simply principal components principal components are the orthogonal vors that captures the direction of Maximum variance in the data the first principal component captures the most variance Follow by second principal Comp PC Maxa patterns in the dat when you sayal between the okay e typically starts with standardizing the features to have zero mean and unit variance this ensures that each feature contributes equal to the analysis and pres L scales from Domina comp okay so