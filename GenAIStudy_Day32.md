# Day 32 Study Time Gen AI

**Summary of the Transcript:**
- **üîç Overview of Sequence Models**: The presentation discusses various sequence models, including sequence-to-sequence and vector-to-sequence architectures, and their applications, highlighting a scenario where inputs and outputs may vary.
- **üåê Translation Challenges**: It emphasizes the complexity of translating sentences between languages, noting how different languages can result in varying numbers of words while aiming for equivalent meanings. This leads to inquiries about how AI models manage these translations.
- **üìö Understanding RNNs**: The speaker addresses challenges in using basic Recurrent Neural Networks (RNNs), particularly regarding exploding and vanishing gradients, which affects their ability to retain context. It recommends Long Short-Term Memory (LSTM) networks as a solution for better context retention.
- **üîß LSTM and GRU Differences**: The session compares LSTM and Gated Recurrent Units (GRUs), noting that GRUs are optimizations of LSTMs that require less computational power while providing similar capabilities.
- **üß† Memory and Output Mapping**: There are questions about how models understand and remember words during translations and how they generate outputs based on complex input sequences, emphasizing the necessity of understanding the embedding process.
- **üîç Deep Dive into Embeddings**: The speaker advocates for a deeper exploration of the embedding process as it is crucial for effectively training AI models. The embeddings are not fixed but can vary based on the context of use.
- **üöÄ Next Steps**: The session concludes with a plan to spend additional time exploring the intricacies of embeddings, as they serve as the foundation for understanding and implementing effective AI-driven language models.

# Transcript 


0:2:55 -  hey hi guys so we have looked at um we have looked at the type of r as in sequence to sequence Vector to sequence and sequence to Vector kind of stuff and different applications of this The Fourth Kind is where you have combination both where we have sequence and um it takes a certain uh steps takes only input and certain SE so it's um kind of many to many where need not necessarily be one to one like sequence to sequence with equal number of inputs and outputs it could be different varying inputs and outputs which could be used for translation kind of use cases right there could be four English words and could be two Spanish words which represents the meaning of it those kind of things right so we go there we will look at we'll try to understand more but the open question that I have that we need to understand is um so given a let's say with um certain words if you want to translate it do a certain sentence in a different language you can't so there are two ways right you can actually translate it word by word like what is your name then it translate what and is your name so it will give two three four words equalent words in Spanish you put them side by side that doesn't mean they give you a right meaning in Spanish correct so um maybe what is your name in Spanish as diff as I don't know maybe two words or or so in a different language it could also have more words so you can't say it has equal or less or greater words right so um how does the AI model if you if you take it through RNN what is happening it has two I mean RN in the sense let's say because base RNN will not be used by anybody that is understood discussing because it has a problem with respect to exploding and diminishing gradients kind of stuff so the capability of using the historic data or context data within the flow is being impacted so the way to go is lstms right

0:5:42 -  so long shortterm m is Gru is there but um um it's um they are more optimization of el both are same Gru and lstm are same but except the difference is Gru is more optimization uh to reduce the computation um need of lstm Stu so all combined together you can actually look at it as lstm cells um so so when I use RN I mean lstm is the way to go is what we you will use um but what we are doing we are um the flow at a given point of time in a Alm we are calculating even if there are 100 words in sequence like um we are converting it into from the output perspective we are converting it into one it's 100 outputs for each lstm cell will give you an output and also it is it will passed on to the next one um and also you get one output at the end um so the question is how does it map these words words English words let's say I'm I'm I'm for example I'm taking the task of translation but how does it map English words to the relevant other language how does it learn how do does it remember saying okay this is the sentence what is your name or like how is your how was your day something like that if there's a question how does it remember and um give certain answer right so we need to see whether it is laying because the end memory wise how is it how does it store does it store um the intermediate level details during training or how does it store the last number basing on which it gives the output or how because when you when you process the whole thing and give it to the next level you you might not be giving the whole intermediate output right you'll be giving the final output let's say you have five LM cells at the end whatever is coming output you can give the intermediate also but let's say

0:20:41 -  if you're giving the end output to it then it will get one number even though there 100 words you get one number representing those 100 words one number how does it tell saying that one number represents these 100 words right that's one thing second thing is uh um basing on that one number how does it know that what is the translated sentence does it so we need to get detail saying does it remember for each possible combination of sentence no it's not um so then how does it work is the question to understand this better first we need to understand how the embedding process is being done that we'll look at it then we'll look at the the problem in hand is sequence to sequence right how does the sequence to sequence generation happens with the different inputs these are the two things we'll quickly see we'll start off with word aming okay we are for these videos I've already seen stat Quest is doing a good job so we'll go with stat course to understand um how the word uding is working so start Quest vide for for e oh for for [Music] p [Music] [Music] [Applause] om for [Music] [Music] [Music] [Music] that [Music] for for [Music] you for for for for sh for [Music] for for [Music] you for oh for so um embedding is something that we have seen long one wording he has explained wording the I mean I've been

0:23:13 -  thinking word ending works like you take dictionary of words and then you embed each of the word with certain number and those will be used as the representation of the prod about that's was thinking but looks like the embedding process that he has explained is specific to each text itself it's not but um if the embedding is specific to each text okay even during training they'll they'll take the current text and apply the same embedding methodology but uh the the number representation of the particular word could be multiple will vary from training to that of the examples that we pick up because the sentence and context could be different [Music] so looks like it's complex it's not just simple saying all the words are given some numbers and those numbers are fixed and um the model will basing on the numbers understand the sentence and it will try to predict the output this is not that simple so the embedding words could be different basing on context for a given word one observation second we are not only talking about one embedding for one word there could be multiple embeddings for a single word and all of them will be ConEd at the same time and then we process the output this is where it is making it complex and that is where by making it complex is where it is being able to deal with um more scenarios okay sounds good so I think the embedding needs more deeper dive s Quest is given a high level overview but we need to get deeper into understanding how the embedding Works what are the different types of embeddings and all that stuff and how they work and stuff because embedding is definitely un deniably the backbone of the whole processing and if embedding is not clear um and we are not able to embed the data that is coming in for our model properly we will not be able to um either train or predict proper outputs as required kind so we'll spend more time on embedding to understand it better that's it for today we'll meet again tomorrow