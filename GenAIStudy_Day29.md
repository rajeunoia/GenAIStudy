# Day 29 Study Time Gen AI

**Time Interval:** 00:00 - 54:12  
**Summary**  
- **üîç Overview**: The session revisits the previous day's discussion on CNN and transitions to a brief introduction of RNN. The focus will be on understanding RNN's foundational concepts and its significance in deep learning.
- **üìö RNN Fundamentals**: The instructor highlights that RNN serves as a crucial precursor to more advanced models like Transformers and generative AI. A video explaining RNN, which provides necessary context and depth, will be reviewed.
- **üß† RNN in Context**: Emphasizing the importance of grasping RNN, the session discusses various combinations within RNN structures, including sequence-to-sequence and vector-to-sequence transformations.
- **üéØ Importance of Context in Models**: The instructor explains the necessity of context in RNN models, particularly for tasks like time series prediction, and discusses how previous outputs influence current predictions.
- **üõ† Practical Examples**: The session outlines plans to demonstrate different types of RNN cells, such as LSTM and GRU, and their practical applications, including feature engineering before inputting to networks.
- **üöÄ Next Steps**: The instructor anticipates deepening the exploration of RNN, its variations, and coding components in future sessions, while acknowledging upcoming scheduling challenges.

# Transcript 


0:20:47 -  hey hi guys so yesterday we covered the remaining questions or queries that we had on CNN um today we'll look at um RNN uh RNN briefly we have seen yesterday but we have not got the details in it so there's another video that I've seen which we'll watch again now um which actually explains RNN pretty well and also takes it to the next level kind of stuff on what is the continuation topics that we need to understand and this definitely is a very good prerequisite for the Transformer architecture and in turn the generative AI models and stuff so RN definitely should be said that it is the first step of all those models so the stronger we understand R the better we can understand the remaining pieces part of it okay so let's get started with RNN I e than I for what I'm C sorry I yeah you and than C I I you sh than wa wa for h okay I I right yeah and I [Music] good I for I I [Music] Ser is c [Music] x in and output is now so it's being to see he is not there at all everything is going to hit so X hit combination plus the all three are being packaged so being a single value it will become a list of values that's the

0:24:25 -  difference Interesting Man very interesting but I should look at it with an example both RNN and G I should look at it an example good oh I think we should move on so we didn't cover much today not much um length but um we definitely understood saying what is RNN and uh what is lstm within RNN variance sequence Vector combinations all combinations are possible sequence to sequence sequence to vector vector to sequence all three combinations are possible so those should be looked at on how they work we should look at them as with an example Transformer and coder is nothing but um a combination of uh [Music] uh it's a it's a combination of sequence to vector and sorry sequence to vector and sequence to sequence combination kind of stuff both combination is where we yeah all three matter of fact is what Transformer encod network is so sorry encod decorder network is not Transformer now lstm and gr are where [Music] um they're trying to just in the basic RNN we are just passing the output of the previous value but um what happens over the period of time is the the initial output as it goes further because of the multiplication and stuff that gets faded out and we forget it so we should have a context right especially we are dealing with sequence sentences or um time series values and stuff you should have context on what El is happening for a while and we should remember it for some time so how do you remember in a transactional system if you look at it how do you remember is You Remember by caching the previous values for example if you are um calculating for every uh minute every one minute if you're calculating some value of a stock price or something like that but you

0:26:59 -  want to remember how it is actually uh growing over the period of time so that um you you know um you still have a context of how it has grown for Last 5 Years so that you could do your prediction for next uh next minute so we need to have that uh context so how do you have the context is the model has been trained on the fiveyear data for every 1 minute so that context needs to be set somewhere so you should remember what is essential in this the best is to remember everything and um other way we may be we may not remember the essential part of it so remance of the previous Parts Plus the output of the previous part so the combination of it if it is there we are having a proper prediction so like like in chat models where we have context and then you ask a question your question is not only answered basing on knowledge plus also the context so that's how we answer right so similarly here they're bringing in the concept of context here which is very interesting so and basing on the context and the input the output is given so and the implementation of is done in lstm and Gru again we should we should get to the details to understand very clearly how those how those work um so this is now I understand when I ask questions for people um about Transformer architecture and stuff and attention mechanism and all that stuff when people were saying you should you should actually have a very sound knowledge on the basics of RN and kind of stuff so I went in impression saying okay I should not hit the RNN directly I should actually hit the whole thing from the starting but late realization is if you hit RNN directly also you'll be able to understand but nevertheless the uh understanding is pretty good now uh with respect to Ann's CNN now RNN so but we should see look at examples of RNN on different types of RNN different cells within RNN as lstm Gru RNN normal RNN cell what are the other types of cells that