# Day 38 Study Time Gen AI

**Day X: Study Time Gen AI**  
**Time Interval:** 00:00 - 52:42  
**Summary**  
- **üîç Overview of Word Embeddings**: The session reviews previous concepts of **word work**, discussing two types: **bag of words** and **skip-gram** models, including optimizations like **negative sampling**.
- **üß† Introduction to GloVe**: The session shifts focus to **GloVe**, highlighting its method of generating vector representations for words and letters. Comparison is made with **fastText** and traditional word embedding methods.
- **üîÑ Cosine Similarity**: The instructor discusses how cosine similarity is utilized to assess the effectiveness of embeddings and their relationships to one another.
- **üõ† One-Hot Encoding Explained**: A detailed explanation of **one-hot encoding** is provided, outlining how it uses binary representations to encode characters, emphasizing its role as a dense representation technique.
- **üí° GloVe vs. Skip-Gram**: The session examines the differences between **GloVe** and the **skip-gram model** with negative sampling, noting similarities in their functions but variations in execution methods.
- **üìä Global Context and Local Context**: The instructor discusses how GloVe relies on global statistical information while skip-gram utilizes local context, providing insight on their different approaches to deriving word embeddings.
- **üîÑ Next Steps**: The video concludes with a plan to explore additional models like **Elmo** and **BERT** in the following sessions, continuing the journey through embedding techniques in natural language processing.

# Transcript 


0:2:55 -  hey hi everyone so today we go through what I'm meing we already covered word work which there are two kinds when ISO continuous bag of words and uh skip gr um and Skip gram also has a optimization using native sampling sorry this we have covered already so we'll go to um we'll take a look at glow I think the primary thing about um his edings is uh a vector representation of words or letters um oh yeah we looked at fast Tex also fast Tex is nothing but the same variant as word to um SCP gr but um uh scrip gr with um at a l level is is fast text whereas script not Word level is uh word to work that's the difference now moving on there are still glow Elmo and BT are I mean there are multiple Amic models but these three are the major ones which we need to understand before jumping into uh the next NLP details okay so let's cover glow today uh we'll be watching some videos mostly I'll not be doing not talking I already quickly looked at this video he was explaining on um one hot encoding one hot encoding is a very important concept uh which nothing complex let's say I say take 26 letters 10 numbers and space long so using binary system zeros and ones what we do to represent a we'll keep one at one's place in this 37 characters and then similarly we'll keep going adding ones at all of these right so like e is like a b c d e so e placees one a placees one like that we'll we'll keep one at a corresponding place for all the letters 26 letters are 10 numbers 0 to 9 and a space okay so this is called um a dense representation ofing sorry other way spse uh representation ofing and then this is called a dense representation of eming which we can derive through word work which we understood the other day right using to work or fastex or

0:15:53 -  anything we can represent it's yeah not only about size but it's all also about the representation where we are in spots we are representing other words other letters or words to represent this word whereas in dense it is specific to the word um we representing on on part of the features or something that look it up for sh for [Music] for for [Music] for for for [Music] for for for for e right for [Music] [Music] um so for glow we have and we looked at word to work on um the skip gram with Native sampling also we have done the same thing where we have taken a cosine of these words when they representing I think the cosine similarity of the words is the common metric to see how the uh embedding is working fine because we know what is the similarity of certain words and those words being closure is the thing but what I didn't understand is what is the difference between glow and um and the uh um um skip gram with the and skp gr with negative sampling I think both looks similar I should look up on it to understand what is the difference between both of them um okay let me look up on that let's ask CH GPT on what is the difference okay here what the [Music] he CH must are two popular Techni [Music] [Music] forting for okay okay so let's go through each points objective predict context words given a capture word cooccurrence Global context oh okay so the cooccurrence there we use for evaluation but that's not the driving metric for

0:23:20 -  the the model using a Nal Network to derive it but GL is simply getting to the co-variance where there is no um actually there is no neural network as search and glow GL doesn't let look at it training to language factorizes the word coer matx so it's just simple math um context local [Music] purpose so this with negative sampling I guess this is this issue is [Music] addressed uh Global context ENT Corpus C statistics it's okay [Music] factorization output dense word vectors optimiz [Music] for nor statistics back for nice J so both are doing the same thing but the different ways of doing okay so glow is also clear um Elmo and B are pending um I have an issue with my eye there something wrong so if I if I focus more I think it is there's a little bit of pain so I'll end this stream today quickly um we'll continue on uh Elmo uh BD tomorrow and complete the embedding Journey once we complete the embedding uh is the plan there's a plan somewhere look at it F text [Music] bir what I'm creating texx been stagging it's back what do l [Music] as [Music] [Music] [Music] concept PA tagging let's list this tagging B tagging