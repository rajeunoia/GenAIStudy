# Day 20 Study Time Gen AI

**Time Interval:** 00:00 - 33:30  
**Summary**  
- **üîç Overview**: The session focuses on **Principal Component Analysis (PCA)**, exploring standardization, variance, and standard deviation in datasets.
- **üìä Importance of Standardization**: The discussion covers the need to bring diverse continuous values to a similar scale using standardization techniques. Standardization involves adjusting values to have a mean of zero and reducing the spread of values relative to the mean.
- **üß† Variance and Standard Deviation**: Key metrics like **variance** (the spread of numbers from the mean) and **standard deviation** (the average distance from the mean) are discussed. Understanding these metrics is crucial in analyzing how data points relate to the mean.
- **üí° Covariance**: The concept of covariance is introduced, explaining how the relationship between two variables is assessed. Positive covariance indicates that as one variable increases, the other tends to increase as well.
- **üîó Correlation Coefficient**: The correlation coefficient, representing the strength and direction of a relationship between two variables, is defined. It ranges from -1 to 1 and provides insight into how variables are related.
- **üìè Principal Component Calculation**: The session describes the calculation of principal components by analyzing covariance matrices, outlining the process from standardization to computing eigenvalues and eigenvectors, which help identify principal components.
- **üõ† Dimensionality Reduction Techniques**: Various dimensionality reduction techniques beyond PCA are discussed, such as **linear discriminant analysis** and **autoencoders**, emphasizing their importance in machine learning fields.
- **üéØ Next Steps**: The session ends with plans to delve deeper into data exploratory analysis and other dimensionality reduction techniques in future discussions, highlighting the complexity and importance of understanding these concepts in data analysis.

# Transcript 


0:2:50 -  um hi so um today we'll go through the PC yesterday we started with PCM we were getting in detail we looked at uh standardization that is uh the X fees values are usually continuous so such continuous values could be ranging from a wide range right right like be something like umus 100,000 [Music] to plus 4 million something like that big numbers right could be varying and then dealing with such variable numbers and um for example something measured in centim right it will be big huge but where does the same thing someone some other dimension which is actually in um like currency something like that it's relatively small right so what happens is the numbers are not relative with respect to size um size in the sense multiples um one could be in 100,000s another could be in 0.2 0.4 0.5 something like that right so so to bring them to a similar scale and also to get an even distribution so distribution as per what we studied yesterday distribution is um the way we look at it if we take a set of numbers take a mean of that particular number like set of numbers this if you put it them in a straight line and you take a mean of that numbers somewhere in between distribution is the way we look at it on how much they are spread from the mean kind of stuff right that gives us like um how these numbers are how far are these numbers from this particular mean basic the difference from the mean is called variance and basing on variance we difference alone not difference alone sum of squares mean is variance square root of sum of squares mean of differences between the mean and the point is standard

0:5:39 -  deviation how it is significant variance and standard deviation are a mathematical calculation but um logically on um how do we what is the purpose of calculating variance and standard deviation I don't see a logical explanation but those are one metrix of a series of numbers like like you have a set of numbers you can calculate mean um you you you try to see instead of looking at all the numbers the mean will give you what's on sort of a mid value not even mid value what sort of a value that is there um where these numbers are distributed across those numbers similarly variance gives how much these numbers are um spread from the from the mean um similarly standard deviation will actually standard deviation will give you more accurate how much they are spread on the mean because um variance is a mean of the squares of these differences variance um but standard deviation being square root is closure to the actual values on Yus y mean kind of stuff and this VAR is with the points there are some out layers then the variance are the distance between mean and the these points will be higher something will be very close will be smaller so considering all those differences between the mean and the actual points standard deviation is calculated as a average of the differences um between the mean and other point so that will give you a rough idea on how much the distribution is that's about variance and standard deviation so first step that we looked at is stand I couldn't find that block for some reason I think this is the block is it okay I'm not sharing the screen might okay let me share the screen yeah I think this is the block that you're following it had steps no yeah for component analysis yes this is the one standardization step one standardization

0:9:33 -  is value minus mean by standard deviation um we calculated on how it is so what happens if you do this is value minus mean once you do it the mean of these numbers will become zero you remember we asked CH GP to calculate and it's not calculating properly because of rounding of the numbers and stuffff so value minus mean by standard deviation um standard deviation gives us the average distance of the points from mean is what we discussed right so what happens each value if it is divided by that average distance that now what happens if you calculate um stand deviation the standard deviation of these numbers that's a good point standard deviation of these new numbers what is there from high values about the mean of the result values and the standard no previously mean he calculated 0.0 to something oh my God this guy is in this U cross validation mode so he's making sure so what happens mean become zero then sum of square square root of sum of squares by n previous standard deviation is 11 that means the distribution is 11 now it has been brought down to 3.62 by dividing the standard deviation we are actually bringing it much closer to the mean that's the whole point besides uh the numbers are so the mean is like this we Shifting the mean to zero and these numbers we are moving them relatively by doing a minus mean something like the number mean is here and the number is here like 10 now mean is 4 10 we moving mean from 4 to 0 and 10 - 4 6 we moving it closure here so and previously it is

0:12:23 -  um four and then imagine standard deviation average standard deviation across all the numbers uh not average standard devation average distance from mean we considering as input for standard deviation right so that average you're taking and dividing so that relatively all the numbers will be sized down SL divided by standard deviation will bring it here somewhere right like 0 to it became four now um what did I say 4 to 4 to 6 is it if it is 4 to 6 4 being moved to 0 it will become 0 to 2 2 again will be divided by standard deviation let's imagine it's 10 10 deviation then 2 by 10 will become 0.2 so it it will come very close to zero so the distance from the mean it will actually be reduced that's the whole objective okay and deviation previous numbers if you remember it is yeah it's 11 from mean 36.5 we bought it to zero and standard deviation from we bought it to 3.6 that means the spread of the numbers we reduced the spread of the numbers related to mean so we're not making it same for all the features the Fe all the features the S deviation will be different but what we doing is instead of having a high range of numbers we are reducing it to we minimizing it okay then going back to the blog standard deviation is done standardization is done not standard devation standardization is done using this formula C Matrix computation this I tried to see So Co variance of X comma Y is nothing but X um Sigma X Sigma x i = to 1 to n assuming X and Y are belonging to same data set will have equal number of values and right so it will be x minus x mean x i - x mean into y i - y mean by n why do we multiply x i- x mean and y- Y mean I don't know and why do we divide it we I mean you're calculating a mean

0:15:43 -  of those that particular multiples so we we taking the differences from mean of all the points of X and Y we are multiplying them with each other the distance between the mean and the actual points and um so the distribution of X with respect to its mean and distribution of to its mean those values are being considered and and those values um are being multiplied and then we're taking an average of those numbers and that's what we are representing as covariance compute the oh this is how the cence will work is what they were [Music] saying soans some of are mean of multiplication of differences from mean if you look at it uh from the mean if both of them oh yeah both of them are positive and both of them are negative at the same time possibly with few exceptions then mean of the product of the distances between mean and this number and mean and Y number will be positive because negative negative multiplication is positive positive positive is positive Coan will become positive whereas if they are going opposite then what will happen from mean one will be going towards positive direction other one will become will going will be going a negative Direction so positive negative negative positive so both will become minus most of scenarios mean of those numbers will become negative that means most of the cases they're going in opposite directions but there's a possibility that half of them are going in same direction and half of them are not going in same direction then also the possibility and also basing on the ranges let's say the by Y is very small the distribution

0:20:9 -  from zero it's just 0 to one values whereas X is [Music] from - 10 to 10 for some reason then the m multiplication will actually even after standard deviation let's say still it is - 10 to 10 range of the values then X will dominate in the covariance but actually the value doesn't matter right the sign matters sign matters but what happens is the negative values and positive values of X will start influencing the overall value one big negative value of um X can take away all the positive values of Y and still make it negative so the calculation is of coari bi for sure okay then anyways that's the maximum amount of time somebody can build on go but okay let me still I want to go through a little bit more Co and Define the of X and Y do some information about how X and Y are statistically related that is okay x minus x mean minus why mean by slash and he has removed okay fine what does he mean by E okay maybe itself means mean okay h e of xire yeah why e of X Yus e of X into e of Y me mean of the product minus mean of X and mean of Y that's what he's trying to say the coari between indicates how the values of X and Y more related to each other if large values of X tend to happen with large values of Y then um x- X Y is positive on average in this case COI is positive and we say X and Y are positively cor on the other and if x tends to be small in this case the co variance is negative and we say negative of X Y is equal to variable of x x comma

0:26:40 -  x if x and y are independent then Z how can it be zero I don't know X yal Y X a into X Y the results can be Prov directly from the definition of Co for example FX XY X1 y1 + X2 Y2 + on X and Y N by n minus X1 + x X2 + X3 + xn into y1 + Y 2 + Y3 by [Music] n how can both of them will become same man I don't [Music] know that's okay that's all about the correlation coefficient denoted by some some character I don't even know what is by normalizing the coari normalizing the covariance in particular we Define the correlation coefficient of two random value variables X and Y as the coari of standard versions of X and Y Define the standardized versions of X and Y as standard deviation is never negative so M dividing by um standard deviation of X and standard deviation of Y is uh nothing but square root of RX y so say looks like same value but just WR it seeing about the correlation coefficient is that it is always between minus one and 1 uh this is an immediate result of CI inequality is discussed in section 6.2.4 one way to Pro - one nice to use for any real numbers product of the numbers beta is less than or equal to so 2 Alpha Beta alpha alpha square + beta Square minus 2 Alpha Beta is greater than or equal to Z that that is because alpha- beta is whole Square it is always greater than or equal to Z Mak sense then Alpha Beta is less than oral to Alp Square beta Square by 2 okay make sense given the equality holds only if alal to Beta for covariance is no correlation coefficient is coari divided by standard deviation of both and standard deviation of

0:33:47 -  co-variance is sum of product of the differences by n standard deviation is square root of sum of squares of differences by n but the square and square n come NN cancelled that is sum of products of x y and uh square root of sum of squares of x - x i oh okay what he's trying to say is [Music] um square root of square number some of squares um as long as it is square root of as it's positive and it is always squares is always greater than sum of squares is always greater than product of squares oh here itself so Alpha square + beta Square by Alpha Beta or Alpha Beta by Alpha squ Beta squ it is less than or equal to 1x2 to that that means um but bu there is a square root on top of it okay let's see for for uncorrelated there's no relation between X and okay this is too much of non for enough let's go back to this of the next step of Co variances step one standardization step two calculation of Co Varian compute the AG and vectors and and values of the co matx to identify the principal components okay that is are linear albra Concepts that we need to I'm don't know can [Music] do so basing on coari you calculate agent vectors and agent values what agent vectors is we look into it but what you do is you calculate by ranking aent vectors in order of their asent values you order by agent values highest to lowest you get the principal components order of significance okay [Music] okay

0:48:6 -  as which means that the Comp PC is V1 after having the principal components to compute the percentage of variance accounted for by each [Music] component for for for name is the fizza oh thisal B analysis looks very comp complicated but okay steing backwards something agors and values watch I do you for this I'm I what I I for that's I Maybe for e so it's complicated that's the sum I vectors A values and all that stuff end of the day what he's doing is calculating all that AB is dering AB and ab what he's doing a into x - x mean plus b into y us y mean a are values that he is derived as the Aon vector and basing on that basing on these features if calculating the first component just similarly if we um the first Aon vector and if we take second Aon Vector which is calculated basing on the second a value Lambda 2 then use using those that second a vector we multiply and derive the second principal component [Music] so getting into the detail on what this principle component is and stuff I didn't understand this graph he has drawn and stuff which goes in positive direction direction project the values and stuff that is not clear but um principal component analysis is in turn but principal common analysis by looking at a into x- X means plus b into Yus y mean kind of stuff where a are calculated by some math ion values ion vectors whatever um that being considered um so it's a combination of

0:50:57 -  multiple features together [Music] um it's equation principal comp analysis is actually a equation which actually is derived from the remaining features which we believe is the right thing to do or on on reducing the P Dimension kind of I mean I'm assuming if there is X Y and Z then it will become a into x - x mean plus b into y - y mean plus C into z- z mean um and that will give us the first principle instead of a BC if you use Lambda 2 it will become DF then it will become D into x - x mean plus e into y - y mean plus F into z- that mean will become uh the second principal component analysis feature like that we form certain features and on these features we we do the modeling and um take it forward and this features calculated basing on multiple features as a combination is effect is what you saying got it okay so with this we'll stop the principal component analysis not 100% satisfied and complete understanding but it is definitely complicated at times we need to step back and say bus this is not your cup of tea so we should agree the defeat uh but yeah so standardized values are being taken and then the covariance is being taken um post covariance they are calculating the igen values and then ion values they're calculating igon vectors from ion vectors they're calculating the principal component by multiplying the vector with the standardized vectors transpose into standardized values transpose and that when give us certain principal components um so you can use different um ion vectors multiply the to derive multiple features and these set of new features that we derive will be used as principal components 1 2 3 and four kind of [Music] stuff Lambda 1 Lambda value one and ion value two ion value three how are they

0:54:7 -  um making sure that they are giving us the best principle component one one two and three and stuff I didn't understand that concept as well there are gaps in the understanding but I think um we should live with it or maybe if we if we read somebody we should um ask them before we close off for today let's ask CH dbd what are other Dimension production Mak beside PCA besides PCA there are several other dimension reaction techniques commonly used in machine learning and Analysis some of these techniques include linear discriminant analysis we have wrote so many papers on this because it's a very important concept especially dealing with multiple features and stuff they would definitely get into this question on how do I reduce the this thing right linear discriminant analysis this is a supervised Dimension technique that expens theability between classes reducing the Dage of data T distributed stochastic neor embedding is a nonlinear dimensional reduction technique commonly used for visualizing high dimension data and low dimension space together okay Auto encoders auto encoders are neur network used for unsupervised learning that learn to encode High dimensional input data into low dimensional space and then decode it back to the original Dimensions by training the aut encod to minimize the Reconstruction eror meaningful representation of the data can be learned ISO isometric mapping map isine damage reduction technique that seeks to preserve the shortest to low di is particularly useful for locally linear embedding is a nonlinear technique that local relationship between Comin near okay sparse coding representation of data [Music] finding independent component analysis blend Source operation techniques that a m use for St means like X1 X2 X3 X4 are there X1 plus X2 X2 + X3 x3+