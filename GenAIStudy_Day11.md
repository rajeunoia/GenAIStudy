# Day 11 , Study Time Gen AI

### Day 11: Blog and Machine Learning Concepts  
**Time Interval:** 00:00 - 37:25  
Summary  
- **üîç Recap and Blog Introduction**: The instructor shared that they had been working on a blog covering previous topics, including a discussion of machine learning algorithms through analogies related to war situations.
- **üìñ Blog Content**: The blog detailed aspects of machine learning, outlining that while the foundation relied on brute force methods, it included optimizations to reduce overfitting and improve feature selection.
- **üë©‚Äçüè´ Focus on Machine Learning Models**: The session emphasized understanding core concepts in machine learning, specifically **decision trees**, **random forests**, and **extreme gradient boosting**.
- **üß† Algorithms Highlighted**: Key algorithms discussed included support vector machines (SVM), logistic regression, and others that utilize ensemble methods for model performance enhancement.
- **üñ• Kernel Methods**: The instructor explained kernel functions that allow SVMs to manage non-linear data by mapping input features into higher-dimensional spaces, differentiating between linear and non-linear scenarios.
- **üõ† Soft Margin SVM**: The session discussed the concept of soft margin SVMs, where a balance between maximizing the margin and minimizing classification errors is critical, especially in handling outliers.
- **üöÄ Next Steps**: The instructor plans to delve into kernel methods and their implications in machine learning optimization in the next session, emphasizing the importance of these techniques in constructing effective learning models.

# Transcript 


0:5:53 -  hey hi everyone so today is um day 11 um we have completed um thees and stuff um yesterday yesterday I couldn't come live but what I did is uh I worked on a Blog on Deary that we have covered till love um I mean the blog is more um uh uh uh it's more on explaining Deary with respect to an an analogy comparing with the war situation and stuff and how do you deal with it uh uh and stuff our video has a much more detail explanation but um the blog was also interesting let me share the blog with you guys as well is it here okay yeah I'm sharing my screen so this is the blog I wrote entry and um my thinking is going saying it's it's little uh it's it's not 100% but I would say Deion Tre if you go deta into the detail 70% of it is it sounded like more like a Brute Force it's actually 100% Brute Force but 30% because of the optimization on on how we pick up the features how we pick up the values and stuff and um uh how we can uh reduce overfitting other things that came in which I added a little bit of intelligence R for me uh but the core understanding it sounded more like a Brute Force algorithm so what happened is I was actually trying to put this um the the initial version that I prepared it was more draft and uh and I was actually going a little negative on it so um topics okay we'll publish this blog machine learning asry and [Music] uh and [Music] [Music] uh yeah so this I okay let me share it on LinkedIn too it should not ask me for login all I'm bu that f now okay this is done um we

0:8:30 -  good we'll get started with um okay I I'll copy it I I'll just publish it uh soon this is not good to be recorded I'll go back to the recording um so de Tre uh you guys can go through it I'll not go through it now because it's waste of time and we don't have much time today so um we will today's Focus will be we have completed the uh Deion 3p is part of it I think we've explained about information entropy Information Gain zi index I think these are the three metrics basing on which we make a call on each of the nodes and then we build the DI trees um I also explained previously on what is random Forest what is uh um X GBM that is uh extreme gradient boosting uh algorithms and stuff and we we understood a little bit about assembling models and all that right uh that being done there are few things few additional um things that we need to understand from The Core Concepts pie part of it so today we'll walk through with chat GPT Chad GPT says um these are few things that we need to study it recommended saying these are few other things besides linear logistic regation is random for what are the other things to do is my question and said we should we can cover support Vector missiones na base classifier K nearest neighbors and then these are all boosting and optimization and all techniques they are not real B networks is okay um that's that's what it uh said so let's study let's try to focus on these uh five things if we can cover um as much as possible let's try to understand U them but I'm not going to get to the details on derivations and how it is working and all that as we went for linear regression logistic regression Dees and stuff to the detail but we'll understand what is what in it because um after and the direction which we going is Genera way and St nothing wrong with these models or nothing to say that these are not being used or something these are um these might be used by people basing

0:11:18 -  on the use case especially people use them not today people don't use many of the machine learning models in individually for for a use case they usually use embling approach and then embling approach where they use multiple models together they pick up uh they pick up models um um of different of of heterogeneous kinds and then they try to see how it works and stuff so they might they might use this for that purposes and stuff but okay let's go to svm svm is support Vector machines and support Vector machines um they cover both variant um I mean okay today it's more reading is what I'm going to do uh so it has both regression and classification capabilities uh okay let's ask explain me about um about Vector machines how they have AED and uh why did create that's algorithm even when we have linear logistic it'll explain now about support Vector missions I just read out what what is there in the things support Vector machines are powerful supervis learning algorithms used for classification again we still in supervised um they were first introduced by since become widely used in various Fields including computer vision as a language um processing and bioinformatics SPM formulation for binary classification tasks and linearly separable data spms aim to find the hyper plane that maximumly separates the classes while while maximizing the margin the distance between hyper planes and the closest data points okay I mean I think that's that's the key point so um okay we'll read through it and then summarize at the end instead of talking kernel kernel trick one of the key Innovations in sum was the introduction of the kernel trick this allows sums to

0:13:55 -  handle nonlinearly separable data by implicitly mapping the input features into a higher dimensional space where the classes become linearly separable okay soft margin svm the original svm formulation aimed to find the maximum margin hyper plane which might be sensitive to out layers the introduction of soft margin svms allowed for some misclassification errors by introducing a penalty parameter C which controls the tradeoff between maximizing the margin and minimizing classification errors what is margin what is classification error is something we'll look at it but let's understand so soft margin ASM C have introduced to handle out layers okay mtic class classification um designed for binary classification techniques like one versus one strategies have been developed to extend ASM for multi this is same way thing that we learned in loation we we iteratively apply the model on multiple models where we say that this is apple and then we say these are other fruits and then we take the other fruits and then say okay out of it this is orange and remaining other fruits so we go incrementally to do the multiclass classification but these are not doing one shot multiclass where the output directly says what kind of fruit it is kind of stuff so it is like you classify as not Apple not orange and then maybe you go toward something like banana kind of stuff so that's kind of approach svm regressor svms can also use for regression task svr support Vector machines be sub Vector regressor regressor VR aims to find a hyper plane that minimizes the error between the predicted and actual Target values while maintaining the maximum margin kernel methods so svms are closure to Kernel methods which generalize the idea of sus to other learning task kernel methods used use kernel functions to implicitly map data into a higher Dimension space enabling nonlinear decision boundaries kernel methods is something that we need to also so there are points that are important and skipping I'm just make a note of those kernel method

0:16:49 -  okay and uh what are the other things uh soft margin SPM and uh Kel trick and kernel method should cover it hyper plane multic class classification as methods okay uh effective in high dimensional spaces ASM are effective making them suitable for tasks many as IM classification classification withal functions can handle on linear boundaries through the use of various kernel functions including linear polinomial radial basis functions and sigma kernel kernel methods kernel functions so much is there on the kernel we should understand what this kernel is really uh robustness to overfitting assumes are lond to over fitting especially in higher dimensional spaces due to they able to maximize the margins between glasses I understand maximiz margins but but little bit it's better to revisit and understand Global optimal optimality svms aim to find the hyper plan Max my only separates the classes leading leading to solutions that are often globally optimal okay Effectiveness in higher dimensional spaces spms are particularly effective in high dimensiones Others May struggle you computational complexity robustness to to out layers to the margin concept making for such with noise are unbalanced classes flexibility with kernels the use of Kernel functions allows spms to handle nonlinearly separable data providing more flexibility compared to L so nonlinear seable data is handled well in svms compared to linear anog mods Global optimization to find the hyper Max same thing is repeating to Solutions are often to become for various machine learning offering advantages such as handling di data robustness to layers and flexibility with nonlinear de boundaries through Kel functions when they say it's nonlinear but they pick up hyper plane which is actually

0:20:49 -  linear I I think the hyper planes are linear in spms but it's a good point svm hyper plane are linear good question to ask okay steps here are the general steps to build data cleaning feature scaling feature engineering splitting the data selecting the CM Kel based on answer of the data and the problem at hand common choices include linear polinomial radi basis function training model tring training tuning evaluation and moning that is actually common I think the whole key step is to understand about colel so let's understand what is water pels Cals are a functional fun Al Concept in machine learning particularly Mach and methods mathematical function that computes this similarity or between PS of data points in high diens space oper [Music] hand [Music] computational efficiency of explicitly transforming Thea into into higher dimensional space compute the or feat space of more efficient flexibility in choosing this linear kernel used compuse the dot product between the input feature Vector X and Y the feature space okay polom Pol computes the dot product rais power of D optionally adding a constant term C okay radial basis function RP of col E power minus GMA of x - y Square RF measures the similarity between radial basis it's for capturing complex non so there are t boundaries could be nonlinear also inm comp the hyperbolic tangent of

0:27:13 -  a a linear combinations of input um feature vors option adding a constant to it I feel I should rely on um watching a YouTube video as well [Music] l h e computation yeah okay so what they're doing is um they they are applying they're taking all the data points then because it's a supervised running they know they know the output that is there basing on output um that's there they are actually um so basing on the output that is there and the input combination that's the pattern right like given input this is the output that's the pattern right for the inputs they are applying a function in a way that the those input points are um are take are either increased or decreased in One Direction uh related to y so if um Y is some value um if the Y is that's a zero it goes in one direction if the Y is one it goes in One Direction so using that um what they're doing is that they're actually segregating the uh points um instead of keeping them very um in a layer in a plane where they're all mixed up what they're doing is they're taking them to the uh they're changing the values are are literally they're like they're pushing them away from each other so that we can actually put draw the problem of drawing a line is if if all the Blue Points let's assume they're blue and red red dots and the blue and red dots are um are already uh visibly separated uh with respect to to their values and stuff then it is very easy to draw a line in between right so like even the algorithm can draw a line in between challenge comes is where they are very mixed uh there are blue points in the inside the red Point Zone there are red points in the blue point Zone like that if it is there then you can't draw the line so what they're doing is they're

0:29:49 -  applying a function which will actually um move the feature points values in a way that um that the the blue all the blue go in One Direction all the red goes in One Direction the blue and red classification is dependent on y so basing on the Y value they are moving these X values um they're moving them um by applying a function that function is called kernel function uh once they move them and and and to a and and in a way that they can actually can squeeze in aition boundary there um that will actually help us in U coming up with a solution which will uh help in either classification or whatever right so that's a that's a good idea I understand support Vector machines uh better now so kernel concept fee um understood to some extent I I I am now very curious to uh understand this kernel functions more but um limitation of time I we can't go to that level of detail to understand each and every uh kernel function but basing on the data segregation and stuff because even though we apply a function that doesn't mean the functions boundaries that it will create would not be linear always it could be um nonlinear with a curve and all that stuff so multiple are POS multiple ways is possible so that's the reason they have multiple kernel functions that he has explained in the video and stuff which is okay uh um but we didn't um see important Concepts called um in in the kernel uh support Vector machines kernel functions how do they calculate the um how do they calculate the loss key thing and basing on how do they reduce the loss and then how do they fine tune it [Music] um oh so kernel functions are applied and that will give us so he said instead of going with X comma Y for the training set to create the model we'll go with f ofx comma y f of x is the kernel but after coming F ofx comma y uh using f of x comma Y what kind of a I mean it should again