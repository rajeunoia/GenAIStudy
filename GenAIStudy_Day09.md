# Day 9 Study Time Gen AI

**Day 5: Study Time Gen AI**  
**Time Interval:** 00:00 - 37:25

**Summary**  
- **üîç Overview**: The session summarizes previous topics covering **AI basics, machine learning**, and **linear regression**. The instructor aims to conclude the discussions on linear regression and introduces **logistic regression**.
- **üìê Deepening Linear Regression**: The video elaborates on **linear regression**, focusing on calculating loss and introducing the **cost function**, with an emphasis on **mean squared error** to refine algorithms.
- **üß† Understanding Loss and Cost Functions**: Key concepts regarding the importance of loss and cost functions are presented, highlighting how they assess the differences between expected and predicted outcomes while guiding adjustments in models.
- **üõ† Practical Calculation**: The instructor illustrates how to **calculate cost** through squared differences and discusses its relevance in enhancing model accuracy, stressing the need for comprehension in making informed model adjustments.
- **üéØ Optimization Process**: The video explains the optimization of models by adjusting parameters (W1, W2) to minimize costs, detailing the systematic approach to reducing the cost function.
- **üßÆ Gradient Descent Introduction**: An introduction to **gradient descent** is provided, explaining its role in determining optimal parameter values by reducing the cost function through iterative steps in the right direction.
- **üöÄ Next Steps**: The session concludes with a preview of upcoming topics focused on deepening the understanding of **gradient descent** and its significance in optimizing machine learning models.

# Transcript 


0:2:33 -  hi guys so um we have come to decision Tre so we started with introduction to AI intro to machine learning linear regression logistic regression and um we started with Deion trees um yesterday and um uh matter of fact it is actually um yesterday I didn't make a video um I mean I was actually more studying um without making the video the video the actually it's it's supposed to be study time where um I'm supposed to um set up the video screen share and then keep studying but what I'm doing is actually I'm I'm setting up the video I'm I'm just talking I'm not studying real study study right so I'm more on providing information or giving out what details what that I like to provide so what I'm focusing on is um yesterday it was more on um understanding things because um I I myself had lot of questions I thought like I started off yesterday saying I'll clarify all the questions and then I'll I'll get into um um I'll get getting to video but uh there are there are few things that I didn't understand and once there are things that I I didn't understand I I I am behind it to see how it is done and stuff okay that being said let's get back to D entries right so I I told you guys when we studying linear regression and stuff most of the algorithms that at least I what I have seen they are one version another version of linear regression um is what I see so uh same here dries also for you guys if you clearly understood um linear agression I'm just quickly Rec capping it so if you take a plane and if there are multiple points in the plane um which are actually features and all of those features combined together they try to predict a y right um so it's like this and then you have a lot of points these points are nothing but X1 X2 X3 X4 whatever so and those all try to predict why correct so um that's linear aggression right [Music] so here in De trees what this guy is really

0:5:16 -  doing is see uh when you draw y equal to W1 X1 + W2 X2 + W3 X3 plus b or something like that it's it's it's it's not really a straight line because obviously right it has multiple features until unless you to put it because if it is y WX + B makes sense it is UN feature but whereas considering X1 X2 on two planes and why then it can't be a straight line right it will be it will be a three dimension so you can't dra like you can't draw a line in two three dimension three dimension should be a a right so so what happens here but that's that's complicated to imagine but that's how it really works so we in all the tutorials are our learnings what people do is yeah just let's just talk of W1 x1+ B and then try to explain it but the real use case on the ground is never like that right there is there's never uh um one feature and predicting why because if there is one feature there are a lot of is we can predict it instead of using all these algorithms and stuff you can directly predict what why is close by at least maybe not be that that accurate but the challenges we talking about real world the features could be many there there are features there are hundreds of features also is possible um and still we need to deal with things okay now getting into um the Tre right uh using de tree de Tre can be used for both it can be used as um um it can be used as classification algorithm and also regression algorithm okay that's good how do you do classification with dentry correct let's start with that what dentry does is there are multiple features in our instead of taking a complex linear [Music] um equation and where we don't know what degree of polinomial it could be what combinations we should pick up and all instead of making it complicated the more it is complicated the more it

0:7:54 -  is complicated with respect to Computing it with respect to deriving uh that equation and all that right and another underlying assumption is that all these features influence by directly at a given point of time but the reality would be there 100 features and your y specific thing we very much know that there are certain features which influence y majorly and then other feature which influence low so the how is it determined that is determined by1 right so if something is influencing more and something influencing low what we will do to to reduce the impact of that particular feature on y we reduce the weight like something which is made 0.1 into that particular feature um assuming the feature is somewhere in hundreds or something that once you multiply 0.001 it becomes very small right whereas another feature which is very important which is actually is directly correlated to Y uh very closely then what we do we increase W like we make w 100 right assuming X2 is in some 20s 30 something like that so what happens when you 100 into 20 becomes 2,000 so the other weight you're multiplying with 0.1 and this weight this weight you're multiplying with 200 so the basing the just by adjusting weights we um by adjusting weights we are actually deriving um the impact like we are predicting why correct but what happens we we we are picking up all features in one go and then we're trying to predict it which is where even though you start with one single I as we discussed we we pick up random W1 W2 W3 W4 right let's assume that usual practices we pick up the same number for all of them maybe W1 W2 0 something like that so that's also a good discussion on how how to pick up what is the W1 W2 how to pick up learning rate these are all these are all good things that needs to be learned I didn't cover them in detail this but those are the practical things you learn when you really try to solve an problem statement using the algorithms

0:10:29 -  because this when you study right you all you need to know is what is linear regression how w1x WX + B is calculated how loss function is calculated how cost function is gr how gradient is happens that's all but the actual decision making happens in the real world right where you actually try to solve a problem then you then you'll actually face Sim questions is like okay all this is good but how do I determine what is my first W1 W weights okay how do I pick up um a good learning rate right um how do I make sure that I'm not stuck in local Optimal Solutions right right so there there are a lot of things and how can I fine tune um my how can I fine-tune my data in a ways that the calculation gets easier there there are a lot of things practically that gets into picture because these challenges online they'll they'll also to to push you towards a specific solution or to push you to make an efficient solution they'll say they'll restrict on what kind of um how much time you can run the algorithm uh for a given data and um how much compute you can use um and um I think that's the thing restricting on libraries is very rare but yeah that should be fine as long as you have one good Library it will cover all basic uh required details and all that stuff right so okay that that's um that's actually important that's actually a practical now coming into coming back to De why I'm explaining all that is um to to to simplify the whole thing right people say okay let's simplify then they thought like let's say instead of considering all features at a time how about I go um with simple one feature at a time okay and then one thing second thing I want to go with um um I want to go with linear regression but um I I want to go with something which is um simple like uh um just X is equal to a um kind of allines and um and I mean when I say x equal to a yeah X is equal to a consider x equal to a but the the decision Tre is

0:13:13 -  as it's a decision tree it happens to be like X greater than a kind of stuff right but actual equation that you draw there is x equal to a you draw a and then you you classify things on left and right side and U so that's an easier linear equation right so it becomes X byal to uh oh interesting if I say x equal to a that's a different subject but if so what I'm trying to say is X greater than a okay so because what happens is when X is greater than a you split that particular data set that is there you you take a line you say x greater than a so what happens X greater than a covers makes always into two parts whatever data set you have um but the two sides of it may or may not have depending on a right because let's say there are these all points and then X is a at the end of it then there is only let's assume there's only one point on the left side or there's no points on the left side so you have all the points on one side that's also possible so the x equal to a when you have a graph like this with all the points xal to a can be anywhere on the x-axis correct considering that case the the point segregation is different so you that's that's we'll get to that like how do you pick that a right that's the important thing when I say x it could be any of the features but preferably they pick one feature X1 X2 X3 whatever so what they do they pick up one single line and BAS basing on that line they classify the data um they split the data basing on that particular equation they split the data and then see um how efficient is the split and uh so how efficient when I say how efficient is how closure does it take us to outs for example we we we are we we are discussing about decision three with classification correct so what's our objective our objective is to classify given set of data we need to classify and say how many of them are um that's say simpler as binary classification how many of them are apples and oranges um bigger example how many of them are

0:15:51 -  malignant or benign uh cancers how many of them predict saying the machine has a problem doesn't have a problem like that that could be a number binary use cases then similar there could be multiclass classifications where maybe you say how what is the probable color output or um what is the uh probable uh age group into this age group of the person for which the data is available it could be it could be multiple things into which multiclass classification can also work right so um what happens we we we take we start off with one single line okay so on a given graph there are points all across you draw one line okay then what happens the line the points that are there in the in the graph if I draw a line there are points on the side there are points on the right side correct so it's more like a divide and conquer right so instead of drawing a line which is closure to all the points which is difficult if they are widespread and stuff right in line integration that's that's our major problem that's where our loss is right say you try to fit in one single line or single curve that actually goes that predicts corresponding y for all X and so what you can do but let's say let's say let's think of you you draw two lines and two of them will cover some part of the points like for these points this is the line and that line gives me prediction for these points this is the line and this gives me prediction and then basing on the point where it is I I decide I take the our algorithm so then what happens um if you if you imagine that concept what happens you you start um uh um you you start dividing the whole prediction of given X like the data sets and Y basing on the range of X values you can divide them into um multiple smaller chunks and then you you you solve each of them separate and all you need to do is once um once the incoming data comes you you class you should classify them and say you should segregate saying which bucket of X does

0:18:23 -  it belong to if it belongs to this x then you put it it there and there you have a model specific to that X and you solve that particular X alone and then that gives you y what happens the the range of X is limited obviously the Y also I'm assuming could be narrowed than the broader scope of the complete data set so you're breaking down to smaller data set and they trying to solve problem um similarly so and you agree with the fact saying the more I segregate the easier it is to classify and more accurate it is because you have the whole data set of 1 million records and you have one line trying to um predict why for all of the 1 million records two I break down the whole 1 million into 10,000 10,000 chunks and in 10,000 um if I um if I try to predict for each 10,000 separately then it's much easier right um to classify so classify or anything but let's talk of classification so it's easier definitely so that's the whole approach of these decision trees and other algorithms that are there and stuff so what happens in decision Tre is um it tries to break down the whole point so I start off with one single line and from that line I break down the left and right okay the first thing that we need to do is to pick up this line like what is the right line to pick up but we'll come to that later on how do you make a choice on which line to pick up and stuff okay that's one thing second um thing is once you break up you you get the points what you do you deal with them separately because you have you know how to break them down right so you broken them down now you after breaking them them now you have two different data sets in your hand which you need to deal with and you deal with them separately right because you know that given a problem statement that comes in you can actually take it to through to these two directions where there is no overlap in between two of them agree so you deal with this problem separately now again if it is still you think it is data set is huge and it has the mixed classification you break it down how much do you break it down is you you need to reach the leaf nodes what does a leaf node mean like like in a binary classification problem you take mix of all apples and

0:20:52 -  oranges on the top you break it you draw a line and this is the left side this is the right side you you put left side and right side do you see left side and right side what is the mix of it is it all apples is it all oranges uh if if any of the leaf node has has all apples or oranges that's the end node you you you stop there because your objective of classifying is reached now on the other side if you have apples and oranges you break it down again still so how much do you break it down um how do you make a choice of the lines how do how much do you break it down how much do you break it down question is um you break it down until your Leaf nodes has um you has only the um single class items that means only apples and only oranges should be there in the leaf nodes until that point you you keep breaking down the tree like from Top all apples oranges break down maybe apples oranges on both both sides also possible yeah and then you you break down try to achieve apples alone oranges alone on one sides and then you keep breaking down until you get all apples and all this kind of stuff so now what has happened is instead of um instead of dealing with the whole data set and instead of solving the problem from complete data set level I've broken it down to smaller pieces and now I'm I'm solving small problems and then small problems are easy to compute faster to compute and stuff and that makes it easy now again so what I'm doing is I'm drawing lines to break it down right so this line is nothing but what it's linear regression um or logistic regression if it's a classification problem it's logistic regression right so I draw a line then I say this is my algorithm okay let's imagine I'm at the root node right top node of the Deary I draw one single line and say this is my logistic regression algorithm um what do we do straight line Z is equal to WX + b um then you calculate logistic regression 1 by 1 + e minus Z the sigmoid function and um what happens to it what is the loss function focus is to reduce the loss that's all right so the objective of always or them is to reduce the loss so that the Y and Y predicted is close to each other so how do you calculate loss the

0:24:9 -  one which we studyed let me share my screen in St showing my face share and then open the book so how did we [Music] do um log loss oh okay this excuse me sorry for that so what I'm going to do is I'll move forward with it considering on assumption you guys understood the log loss and all that stuff but I'm going to make another video um which should be day 10 in day 10 I'll revisit logistic regression I'll revisit um this ENT this this overlapping concept of what what is this um loss function log loss function um binary cross entropy all these complex terms that are there what does it what do they mean and stuff I'll get to the detail that's what I spent a lot of time yesterday and I want to share with you guys also and what I'm what I was doing and all that stuff right so uh uh coming back so what I'll do for given x i is equal to one term I can actually um one second sorry guys sorry sorry I'm just I'm just trying to see how how could I um tell you guys on how um uh we are doing dealing with this clearly right so if the if the all all the points are taken and you you calculate the loss um function binary cross entropy it will give you certain value now you segregate it into two pieces and you calculate the binary cross entropy that is the loss function um uh for each of those points and then you see how much you yeah fine so so you take you you consider a data set where where you have all the points um available and um you you predict ltic regression is nothing but you draw a line and then um I'm just thinking guys give me a minute um you draw a line and then you you map that particular lines points to probabilities to say for each point on the line you you you try to map it to using a sigo function you try to map it

0:26:58 -  to a certain value and that certain value is is what we we cast it as probability of that particular X1 X2 X3 values at that particular um value using this algorithm will predict the probability of classifying that particular thing as xal to 0 or x equal to one kind of stuff right that's what you calculate and then you you your loss function shows um whatever you try to predict uh for that particular Point how far is it uh how how yeah how much difference is it having relative to the actual prediction that is the actual classification that means one minus the probability of predicting that it is one um that's what we do and then we calculate entropy entropy will tell us how far are you from the actual classification your algorithm says the classification is this and then the actual classification is that correct so if you if you start with um [Music] um if you probability of drop your loss is high it should be so anyways yeah so you start off with um you start off with having all the points available okay so then you calculate the uh the the entropy right for all the points then what happens if you have all the points the probability of each point being predicted means you you imagine you draw a line and all the points are on one side of it right so what happens then all the points are available uh that's that's that's also not the yeah yeah you can consider like that also but what happens the entropy at that point of time is you know you have every point that is there and then you you calculate the probability the probability will become um one point being let's say it's apples and oranges one one one fruit being um uh one fruit being an apple compared to all the fruits that are

0:29:56 -  there right that's how the probability Works um that's how you predict the probability for all the um apples and then that's how you calculate the probability for all the orange one orange being uh value being an orange compared to whole fruits that are there so it's more like you put everything into one bag and then you pick up that's probability right yeah so uh starting off from there you calculate the entropy of the overall or apples and oranges that are there in the uh bag so what happens your probability um are the overall entropy loss is um calculated by minus 1 - 1 by n i mean I'm referring to this this one right now that is actually um logarithm applied but it is it's actually a binary class spefic binary classification that's the reason it shows 1us Y and 1 minus p and stuff only two two things but whereas if you generalize it it is you can write like let's say the second one is PJ pi and I and J are two classification then it will become Yi log pi plus uh YJ log PJ correct so basing on appication it will change um uh um oh sorry I'm I'm very much distracted I'm very sorry the bad thing is I'm I'm explaining things on air and then I'm I'm just trying to see how far you guys can understand but let me step back on that I should not be worried so much on on on making sure how far you are understood because uh I'm not putting things on somewhere I'm just explaining in the air so I can't help it uh that's how it is so let's let's move forward and um see how do you calculate um entropy and uh Information Gain and all those things with before getting into those Concepts I'll actually want to show an example saying given all these features um then what you do you pick up some line like this and then what happens you once you draw a line you can actually classify let's assume there are uh to to understand uh uh it uh much

0:33:39 -  better there are um 10 apples and 10 oranges this feature comes in and then you have um four apples on one side and you have 16 apples uh 16 fruits and which are 10 oranges and uh six apples on one side right so initially the point is 10 apples 10 oranges right so what happens the probability of picking up an apple is 10x 20 it is 0.5 the probability of picking up an orange is 10 by 20 again it is 0.5 so what happens it is 0.5 if you put it into that formula it will become um um 1X 20 into 0.5 into log 0.5 multipli by 20 so 2020 is 0.5 Log 5 log 0.5 log 0 uh 5 uh to the b 2 will be approximately 2 to the power how much is 0.5 is um if you look at it um let's look at this thing L okay let's what is there what's the big deal let's just pull up and calculate it in thinking so much right okay not this one yeah thought I don't need I just so 0.5 into that's also just just say log into log of base two of [Music] 0 5 isy minus one okay um assuming that two right so yeah so that two okay what will it will become is [Music] 0 I I'll write the problem sum of entropy is sum of uh uh Pi log Pi

0:36:48 -  P into log P that's it okay it will be a negative sum minus of sum of log P into so what happens here I should put it in our [Music] document these are where few days where uh the disturbance is there and then there there is there are people pinging on different topics and I'm just thinking about it I'm sorry about it it takes a lot of time for you guys uh because of that but that's okay happens so this is the formula right so let's start with root node the root node if you calculate this it will become so minus 0.5 into log 0.5 for apples plus - 0.5 into log 0.5 so this is equalent to log 0.5 is -1 correct -1 into - 0.5 is + 0.5 it's 0.5 + 0.5 is equal to 1 so root node the entropy the loss is calculated as one okay now coming down let's take each of them separately there is if there are four apples what is your log loss right um the prediction with respect to oranges if you look at it it is zero right 0 by 4 0o and so that that that calculation okay on the left node is equal to um probability of apples is one one into -1 into log one y [Music] zero uh yeah let's see and not plus minus here isus probability of um apples is of this set is 4X 4 One correct probability of orange is 0 by 4 0 so it will become

0:40:29 -  minus 0 into log 0 log Z is undefined you can't um any power of two will not become zero right undefined but doesn't matter the uh it's multiplied by Z that is not considered and uh uh log one is uh zero so that is log 1 is 0 so 1 into 0 is 0 so again the value overall value is zero right node is equal to the probability of um ores is 10 by 16 correct so let's use again this thing to calculate it so 10 by 16 into 10 by 16 dude I know 5 by8 man okay fine uh 0 84 88 5 40 8648 uh 6 0 6 uh 2 5 0.625 so 0 625 into log of log 2 of 0 625 will be oh I said it's wrong actually I could calculate the whole thing in one go instead of for doing all this nonsense this 625 plus uh 0 uh so 10 by 16 is what we did here and then now we can do 6X 16 because remaining are 6X 16 but other way if you look at it is 1 - 0.625 uh that will become 375 0 375 into log 2 of 0.375 okay you do whatever man just give me answer - 1.03 1.03 is what what it's trying to say it should not be the case because the entropy is supposed to come down right yeah there's something wrong with that chat GPT its calculation was not correct okay it's um

0:43:8 -  95 okay 0.95 was minus is there I apply minus across 0.95 uh where is this document yeah right node is um to to get you guys uh one I kept it for Apple so let's keep apples on the first so that we are consistent it's 4 by six it's is 6 by sorry 6 by 16 into into log uh 6 by 16 plus or minus uh minus here and here also minus minus 10 by 16 into log of 10 by 16 is equal to 0 94 okay 95 95 so this is what is your entropy that is there but if you look at it the the objective as I said entropy is on the top is actually one right and uh we bought it down to 0.95 here after this if you if if you split it now you have 0.95 okay that means to the the loss of um this particular thing is 0.95 previously we had calculated saying everything is on one side and then the the entropy loss is one now it is 0.95 but you can't um just add 0 plus 0.95 and then say it is 0.95 that's also not a right thing because um it it could be like um the the number of points on the left node are only four and the number of points on right side 16 points so the loss of uh predicting values for four points and loss of predicting values for 16 um could very much be different right so what we do is we um add a weight to it so that it makes more sense right so anyways this is this is entropy calculation but entropy just tells us for each node what is the loss that entropy value that is there which

0:46:27 -  is in turn nothing but the overall log loss to represent what is the loss if you if you go with this kind of a classification correct we to in order to take a decision on what is the right thing to classify because this is four apples and 1016 but let's say I have another um option of splitting it that is I have um uh let's say I have five apples and uh three oranges on on left node and I have um uh reminding right like 10 - 5 five apples and seven oranges here now what happens to the math right so we again need to calculate for this is left node this is right node so for left note what is the value right so it'll be uh Apple's 5x8 into log 2 of uh 5x 8 plus um 3x 8 into log 2 of 3x 8 right so this is something we need to calculate and then uh right node will be node is equal to 5x 12 into log 2 of uh 5 by 12 plus um 7 by 12 into log 2 of 12 so let's calculate uh we should we should apply minus [Music] interestingly this also um is this correct then the same value is coming again and again maybe isct let's look at it Z 0 95 but the calculation I mean the way reading it's reading is correct 5x2 plus log 2 5 by 12 by 12 log to S but okay so this is 0 point 97

0:49:10 -  now if I submit I can actually clearly see it is it is not correct thing um also just left node plus right node if I do also it is the second classification is very high whereas the first classification is lower kind of stuff which is um okay so but also looking at our objective the uh objective is to reach the leaf nodes on all trees and the leaf nodes are nothing but the ones which has more uh uh single classification right so the first one has that that's the reason I think that is uh having a higher value but um again as I said I can't add 0.95 0.35 but example let's say there is another use case where there is three apples and uh four oranges and uh seven apples and uh six es right so and you can't add them directly because the number of here there are four apples there 16 46 here it is um 8 12 so the distribution is also different so to weigh in those distributions and then make the calculation um much better what they do they calculate something called Information Gain right the term is they call it Information Gain IG uh Information Gain this is nothing but to see what is the overall entropy um they they sum it with the weights that means um with their corresponding root node for example for this uh Information Gain for first division first division is equal to um it is the two values right left node and right node this is left node left node is four apples so it's 4 by 20 or into 0 plus right node is 16 right so it is 16 by 20 into 0.95 this is the information gain on the first one the second one Information Gain is uh second division is equal to um so this is 8 8 by

0:52:31 -  20 into into zero 8X 20 into 0.95 plus and this is 12 12 by 20 into 0.97 so let's calculate this this is zero already so all we need to calculate is only this 0.76 and um Second Division 0.962 962 so now you you see Information Gain ways if you look at it the the weighted enop is sum of fated entropies is nothing but Information Gain that's all so because if you add it the if the numbers are bigger the values are bigger and and would be bigger and you should not um add them straight so we we go with weighted entropies and weighted entropies clearly give us the decision saying the first classification is the better classification for decision Tre and we should go with that particular classification correct so that's the that's how you make a choice and then you we we move further down on each part so again 10 oranges in six apples again you you draw a line and then classify it and maybe you get five oranges one side and five oranges and six apples on one another side again you then you pick up um five oranges five six apples on one side you if you can draw a line assuming six apples on one side and fire on side you you're good right you you have all the leaf not which has only apples and oranges then the new use case comes in you you Travers through this particular de Tre saying Bing on X1 is it is greater than one X2 is less than three something like that you you Traverse that particular dis Tre and then you reach certain Leaf node and the leaf note gives you prediction saying for given these X features X1 X2 X3 X4 the the classification says it's an apple or it's the classification it's an orange so that's how the de Tre would work I am assuming this is clear um and you also understood how uh it is coming but I'll also quickly show you guys from visualization perspective three lines graph and then show images come on man yeah something like this right so so uh see what what this guy did really is