# Day 25 Study Time Gen AI

**Time Interval:** 00:00 - 38:56  
**Summary:**  
- **üîç Overview**: The session begins with an apology for a brief pause in study. The focus is on **neural networks**, particularly the structure and function of multilayer networks and how layers interact to interpret data.
- **üß† Fundamental Concepts**: The instructor reviews how neurons contribute to forming smaller parts of a larger data representation. It emphasizes the importance of adjusting weights through **backpropagation** to align the network output with data patterns.
- **üìà Optimization Techniques**: Various **regularization techniques** and **optimizers** are discussed, including **Adam**, **gradient descent**, and others. The instructor highlights the significance of selecting the right hyperparameters like the number of layers, neurons per layer, and learning rate.
- **üîÑ Hyperparameter Tuning**: The necessity of tuning hyperparameters such as the number of layers and the initial weights is mentioned. The session stresses the importance of finding the right balance that ensures optimal model performance without increasing complexity excessively.
- **üß™ Results Analysis**: A practical demonstration shows the impact of different configurations on model performance, such as training and test loss. The instructor reflects on various activation functions and their effects on convergence and classification tasks.
- **üöÄ Future Directions**: The video promises future explorations into optimizing neural networks and effectively employing different hyperparameters to enhance performance. The session concludes with a reminder to practice these concepts with coding exercises and potentially use **grid search** for parameter optimization.

# Transcript 


0:2:43 -  hey hi everyone so sorry there was like couple of days I didn't study which is not nice uh preferably we'll try to get every day okay now what we have looked at is neur networks how the multilayer neur networks is formed how each layer contributes to the next layer and um we also we should saying the neurons um in each layer in down in turn help in giving us a small part of the big line and um and those smaller parts are actually um as it goes through multiple layers what happens is we we adust the weights in a way that the small part of the line which is derived by a sequence neurons is actually aligned to our data pattern of the data so all these small small small lines will actually get adjusted to the pattern of the frame um which is actually driven by adjusting the weights and the weights are adjusted through back propagation um weights are adjusted through the same gradient formula that is W = wus Lambda by 2 into derivative of loss by by W but in back propagation what happens is because the W in the layer to get the derivative of w in the first layer basing on the loss of the final output we we use for calculating it we use back propagation that's all okay that we have seen on how chain rule of derivatives and stuff how it works to to Der the value of derivatives and stuff um we looked at activation function we looked at uh regularization uh techniques we looked at um U optimizers like gr indent and other optimizers um Adam Optimizer gradient descent and then rmsep prop and and we left out last Topic in neuron netx if we thought of covering is hyper parameter tuning because in neuron netx as we

0:6:33 -  discussed there a lot of things where we had choice to make right for example number of layers number of of neurons in each layer uh I wouldn't say number of inputs and number of outputs because you know what x is what Y is so you you would classify X and Y accordingly um the activation function that you use for within the these neurons if you use regularization terms and regation terms has these factors that we use to multiply in L1 and L2 you you remember Lambda W1 sum of w Lambda sum of w Square are the formulas right so you have one is sure what do you call it is Lambda another one is Sigma something like that okay the two variables with which we multiply the sum of w or sum of w squares for regularization so so those two terms are there and um when you do um gradient descent and when you calculate the W values using back propagation we will use the learning rate so that's also hyper [Music] parameter what else is there let me let me ask uh this thing Char GPT and then [Music] don't watch one do [Music] nice [Music] and for what each J okay let's see most of them are what we have discussed already uh number of hi layers that means um so here as we said we can increase the layers right so the number of hidden layers that we need to keep I mean as the number of hidden layers increase the complexity of neural network increases as the complexity of neural networks increases the adaptation to pattern increases but at the same time the compute and uh memory utilization all of this will also increase at the same time so we should be um we should be picking up the numbers of these layers which are meaningful um as per the data

0:9:23 -  that is given um as per the accuracy expectations of neural networks um and all those elements we should pick it up okay this that hidden layer number ofden layers detes of the neural network eachar extracts and learns features from input data yeah the way I what happens is um let's say if you give an image to a neural network the understanding is it it the neural networks the pattern that has been observed is it learns a part of it like you give a human image right human image then when you train a neural network through multiple layers what we understand is it will learn a specific part in each of them in each of the layers and uh and and those those specific parts that it learns saying okay so for for example certain neurons will help in finding out that this image has hair okay so the hair's pattern it will as you fine tune it the hairs pattern So within an image anywhere if there is hair after learning on multiple images if there is hair this particular neuron will detect it by the time it reaches the outer layer or outer layer minus one output minus one hidden layer it will know that it will flag off saying this this image has hair similarly some Flow by the time it reaches the output layer it is sure that it has a nose like that it will it will figure out small small pieces within the um image that we have given in and those small small pieces uh could put together as we see on another day right like so if you if you take this is one feature this is little finger is one feature another finger another finger like each finger might be taken as one feature and it will remember saying when you give an unknown data something like this it will actually try to find out each of this part of it and it'll figure out if okay this side is there so if the side is there it it will that will detect

0:15:29 -  this side or it will say because the size is partially there it will say that the size is partially there kind of stuff so um within neural networks we are Distributing the pattern that we observed is it usually distributes the thing um I know this will un be taking more time here but uh neural networks feature fion so here somewhere the image will be there where the image will show how for a given input of specific image is it we made uh permission outputs is the point is on output distribution of particular image okay go to the [Music] [Music] playground actually we didn't run this um if you run it we he can see how it would adopt but instead of this data show test data dze output we can't use a problem classification problem we can't use an image data something oh here we can [Music] have for if you see this is not even taking anything and orange it is actually taking everything on Blue only I think it is it is truck it is there only it is not going Beyond very slow so now if we increase the learning rate yeah it is jumping out so reset 0.1 see these two are doing the same thing like this like this like this like this and then this is only doing this this and this output is this and this these two are only blue Shades that are coming oh this is blue shade this is orange shade okay Orange it's showing us blue here

0:19:1 -  actually is [Music] orange now this didn't work so now we need to figure out how do we should we add more layer or should we do one thing how do we decide how many layers should we add these are all something we should learn today there are some metrics I forgot actually on uh they are not even metrics they are more like suggestions on what is the how do you pick the distribution of the neurons and stuff we will will learn today all those things okay let's try this now I added more hidden I add an additional hidden layer and I also [Music] added this thing I'll see if outputs if we go with one and two what will happen what is this line oh loss okay it's decreasing but it is fluctuating up and down loss is going higher as the number of iterations are moving up it is almost close except for this piece part of it is almost closed and from here see the test loss and raining loss they become straight line so that means it is not working anymore there that's the best picture possible with this particular classification and it looks almost uh good right so but we'll look at what's happening with each of these stages this has picked up why does it say orange color click anywhere to edit bias isus 1.8 okay bias 6 do oh okay like the bias term that is there we can edit it if you require since so most everything is being L1 L2 regularization rate we didn't use any regularization by the way which is okay ratio of training to test data 50% [Music] so okay let's put it this way 60% nice okay we'll learn about noise also B size 11 we don't know the data set size also but we are soling it but it's okay so we reset it right the loss actually ch both the losses increased it went down for some time and then it

0:22:54 -  increased way up I think it jumped the optimal point is somewhere here below it jumped it because of the higher learning rate so we should um decrease the learning rate let's go with optimal usually learning rate we start off with the our starting point is 0.01 and then we'll move it up and down basing on the need kind of stuff okay let's see how it goes now okay it's smooth the test loss is going down significantly the training loss is still up okay both of them are going down very nice yeah I think they're almost straight [Music] line maybe that's the minimal loss or fews this orange these two orange boxes May touch each other I guess oh they already did I think this is I think this is the minimal loss that we could see because see almost everything in Orange I don't know why training set loss is 0.05 okay it's minimal maybe one point or something is there but if you see for nak D everything is almost everything is there maybe somewhere here if there is an overlap it might have there but everything is here so this is definitely overfitting but let's see how it is doing it first layer for I think in the whole set this piece of orange is picked up by this one okay good [Music] now second piece of orange is picked up by this so it's actually classifying by orange not by Blue both of them are picking up pieces of orange so this is so if it it divided the whole data into I mean two segregations and the first neuron is helping in in picking up the first one and second one is now what happens if you pick up more neurons here then it will divide into more smaller parts as I said in the image classification divides all parts so what happens another image comes in another data comes in it it if it is part of this this neuron will help in detecting

0:26:33 -  it if it's part of this this neuron like that it would go but let's run the training once again you can actually see the test loss and training loss here and because we have more output neurons I think it's working faster let's see which output neuron will actually pick up which part of the output oh these two are overlapping with each other and uh I should have noted that box the Box previously are actually good this is taking more time and it's going up and down the previous one is more stable and it's going down continuously see the the test loss is mounting so this is this going to convergence is harder so having three is actually the output is creating more problem okay let's stop it because it's not converging that soon let reduce it and see if we have one what happens I think one will not work because a single neuron can't take off all the shape yeah it's almost like we're putting a one single equation right so it will come like that single neuron let's see what happens oh it's opting [Music] actually you see this piece it shows it is struggling to pick up this piece part of it because one in single thing it's trying to put the whole uh neuron and it is not able to do it so that's reason okay let's pick up two two any we have seen it is working out well [Music] um one here instead of RA activation let's say if he take sigmoid what happen and we should use the same one so let's use the same neuron go oh was much smoother sigmoid is not sigmoid is flat now itself you because Sig is 01 it's more 0 1 so this problem statement will not be sigo is not a good choice

0:31:4 -  okay linear sigmo will not t I'm not sure how it would work let's go test loss is going down training losses then it is [Music] slow that's going good this trying to assume that this is blue or because of this it is taking it shape but yeah slowly Orange has pushed it out whole blue [Music] thing this Gap it is trying to see if it is blue okay it's struggling oh fluctuation so 0.192 is the training loss 0.21 but whereas reu when we went it went to 0.055 and from now it's there too many iterations and it is actually started fluctuating the loss this whole loss is not stud so we cross the boundary where it would really fit in it's just flickering there itself not going down any further okay back on just repeating on what we have learned see the test loss is low the training loss is high initially at least okay this that that steep is low for less number of iterations you see 700 less than thousand and we are already we have adopted to the the loss is 0.033 and and stuff it's very good those are all waste see optimal point is here so now it's it's keep going and there is fluctuation of data but we already there so if you refresh and we notice somewhere let's stop at U 500 or so M again the behavior is a little different here interesting now it's struggling previously below 1500 so itself it is got to a good State now even after very slow I think every time I do a refresh the bias term something that is changing so the behavior is different now see this time at 400 itself it's almost not almost it's there except these two points or something like that 0 078 that's 0.055 was the best score we've taken last time and uh now it's very

0:34:42 -  good so it's now getting fluctuated 500 yeah see it became straight line that means this can't be further improved this point is outside other than that thing else so we pause it this is picking up the whole blue this pick up this part of the blue this part of the blue this part of the blue interesting what happens if you increase the bad size make it 18 let's go mhm no not really converge but it's taking more number ofox that's all yeah oh it got to a good distance it has spiked so till here it is good and after that it again went to a spike again it is going down kind of stuff that's interesting and learning okay fine so this is this do good so we'll go back U to this so number of hidden layers you have seen how it impacts the overall performance number of neurons per layer also you see seen how it impacts the performance activation functions we have seen how we change different activation function and what it means if you go for logistic I mean sigmoid Tru tan linear and all that stuff um soft Max learning rate control the size of the step but learning with regation we didn't use exp but when you do vectorization here like let's say if you keep L1 regularization and then you make a choice regularization rate uh regation rate and learning rate is the same no no no no see declaration rate is Lambda or sigmoid or something that in multipli by W1 W2 learning rate is sorry learning rate is W1 minus learning rate by m into the derivative right that's learning right all steps that you're taking with respect to W that will impact running rate a lot 0.03 is optimal 0.01 is very

0:38:4 -  slow 0.01 is too fast [Music] see it went very fast but after reaching certain state it actually slowed down second going down down down 0.10 yep so 0.1 is working fine and it is bought down in pretty decent number 0.023 Ah that's almost like the best till now 0.023 running rate incre it worked actually but what if you go to 0.3 see the test loss and trading loss so the the local optimal level where it is supposed to go it is skipped that and then it went somewhere else and it is traveling there so 0.3 is a bad choice 01 is good can see that descent is pretty good that means these are all a areas where it is where we are playing with unnecessary um parameters and those we have avoided pretty quickly and we minimize the loss every time you run it has a different Behavior I think mostly because of uh when you refresh or something that it is changing the initial weights are bias something like that so oh it's not even converging now that's very bad it is converging but it's going very slow so this all piece is left out or this all piece is not even considered it see everything is left out whole thing that's interesting struggling that's mostly because of the AL regularization reg is not there if I take make it none and rerun it yeah see because the regation is not there it is picking up 300 a box and it's already in shape almost in shape sh yeah L is already 0.055 all thing it's not going below go high it's starting it it's not going towards convergence or his minimal law 0.0 to9 to is with theum but it's because of that but

0:42:0 -  n with L2 what happens one is w L2 is W Square so w Square will actually de it more and the learning the overall learning and will be slower because you are giving more you're adding more to the uh loss it's struggling because uh the remaining part of it it is regularizing from adopting to it so it is only adopting small piece of it only if you see both of them both of them are targetting getting only here they're not going Beyond this piece that's because regularization and L1 is both of them are inside previously L1 let's see L1 is most probably outside yeah outside if you L1 and L2 together it is oh actually matter of fact L1 declaration is still supporting and you're getting to 0.02 1 020 is okay again every iteration it is is different okay going back on the activation function learning rate size determines the number of samples processed before updating the models parameter larger bat size sizes may result in Faster convergence but require more memory okay let's observe the pattern also bad size 18 that's a larger bad size start yeah you see the already loss is Clos close to training L is 100 uh but it's there struggling 0.143 yeah from here it is struggling but it said the con bad size if it is the convergence will be faster I think oh okay okay one second let me [Music] see okay struggle struggle struggle struggle okay too much of struggle okay L is already 0.0 it's going below but it's approximating at the same time it's left out this pce the regation is not there it's this I don't why [Music] Mach

0:47:25 -  30 let's see what happens number of yo 300 iterations and it's already below 100 see the iterations are here and the loss is here training loss is here it's going up again bigger bad size I think it's going the actual results if you look at it the smaller bad size has has better results better accuracy than this is 0.058 it's not going below that when I say it only going there 0.0 what's going down 0.27 I I should see both of them 0.019 I think this is this is really good let's see optimize anything below I don't think practically has optimized the best possible ideas see this became straight lines so the loss is minimal see both 50 50% both the graphs are same almost the same 800 and it's it's at 0.05 now it's became stable except these points I think almost the orange points in Orange I think not reduce let's see what happens now 70% train data and 30% test data the training loss should go faster test loss should be higher the r going in similar place because the test data set is 30% and higher it is not coming down previously if you seen the test data was going down Turing data is still up but now it's the other way you have the neurons if they're not picking up this area see this hole yeah this one is there this one is also here this is it comes here these are the two things that you're picking it up the the low this one is not impacting it like let's say 0.12 does it affect at run time or how is it I don't think it'll affected long time that's very difficult even though we don't add it is kicking in right somehow s

0:50:48 -  the test data is very low the training data is being high the loss of training dat is very high it's not coming down it doesn't have enough data to train that's the problem now it can't centralize gu very difficult because it has very 40% of the data and 60% of dat it doesn't know so predicting the data for 60% is impossible it doesn't even have the whole curves that are required kind of stuff if you see see all of them are opting certain part of the data only okay so bad size we have seen epox we have seen basing on the number of eps one EPO present account complete pass through the entire data set during the training specifies the number of times the entire data set is processed during training me entire data since entire um training data set me test data set can also be considered but test data set is more to look at how the loss is happening optimizers Adam SD in the diagram that we have we don't have a choice of picking up Optimizer I think so there he's picking up Adam is what I'm thinking this is the output from one neuron output mixed with varing weights of the lines test data discrete output okay optimizers are not but optimiz you already we have studied adms gbms prop how the models are updated during training this one hyper that is example mum DEC rates okay Techni as L1 droping by penalizing the larger weights producing noise during train dropout rate specifies the probability that a neuron will be randomly dropped out during training more presentation okay weight ination how the initial weights are assign like what is the initial weights of the w i c the networks parameters proper initialization learning rate schedule specif how the learning rate changes over time during training and techniqu

0:54:50 -  such as learning rate Decay warm restarts and adaptive learning rates can be used mean instead of going with standard learning rate we go with adopt adaptive learning rate and adaptive learning rate is itself is the Adam to learning rate and with optimization not optimization I think the gradient Des all this corresponding is either bats or learning rate this adaptive learning rate is a combination of both adaptive learning rate change and also um Adam is and also adding batches both B of size some value and then both combination is so gradient clipping limits the magnitude of gradients during training to prevent exploding Radiance as stabilized training especially in deep networks and the current neural networks okaying how to pick initial values this start with the small number of layers and increase complexity problem Arch sorry use reu variant like example L reu as default choices for layers okay use softmax for output layers in multiclass classification tasks and sigmoid for B classification okay multi class classification soft Max output layer alone then internal R the best is reu see Adam optimiz is the best Act is the best learning rate learning rate in the range of 0.1 to 0.01 General role of Thum Los is [Music] diverging that that size should be basing on the compute that you have decrease the bat size lower compute increase the bat size higher compute okay powers of two GP ample 5050 and monitor the training validation L optim Adam is a popular choice of optimization du to Adaptive learning rate properties start with Adam and exp other optimizers example SD momentum RMS

1:61:6 -  performance and validation okay but Adam is the best parameter1 as a default Choice toting okay wait use random initialization small Val turn from a normal distribution example mean equal to Z 0.1 or uniform distribution 0.1 0.1 okay learning R sched start with the constant learning and learning example gradient clipping set the threshold values one to clip the gradients to prevent them from becoming too large okay okay 3 person okay I'm going to die we also Google search on no no no so this is number one um let also give Pyon code to M or to [Music] compare make a choice of best to compare different hyper the best combination see best combination using python you can use techniques such as grid search random search cross validation PSAR for simple classification task so what they do is they create a param grid with the hidden layer Style sies like 50 hidden layers 100 layer layers activation solver CD initial classifier perform grid search with cross validation best parameters GD search. best parameters when you saw grid search what happens they'll go with all combinations possible ABC with and then they pick up different combinations of those and um they make a choice of which one is working [Music] fine us good CV okay 16 minutes 9 minutes 10 minutes from for I okay didn't get much but okay fine I'll end the video here it's already too long and we need to start off with um other things we do I'm not sure if my voice is audible today because I was speaking very low um that's okay fine