# Day 33 Study Time Gen AI

**Day of Study on Sequence-to-Sequence Models and Embeddings**  
**Time Interval:** 00:00 - 37:25  
**Summary**  
- **üîç Overview**: The session revisits concepts from prior discussions on Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRU) in the context of sequence-to-sequence learning. The focus shifts to understanding how sequences of varying lengths are processed.
- **üìê Sequence-to-Sequence Models**: The discussion explains the mechanics of sequence-to-sequence models, highlighting the handling of differing input and output token counts, emphasizing the importance of embeddings as a foundational step for these models.
- **üß† Understanding Embeddings**: The instructor delves into embeddings, clarifying that they are more than simple word-to-number mappings. They are dynamic and context-dependent, leveraging AI models to effectively represent words based on their contextual use.
- **üõ† Practical Implications**: The dynamism in embedding values facilitates the input of multiple values to networks, which can enhance processing through parallel layers, akin to techniques used in image classification.
- **üéØ Complex Layer Structures**: The conversation elaborates on multi-layer structures in encoder-decoder models, which utilize different embeddings for the same sentence or word depending on context, thereby increasing understanding and efficiency.
- **üßÆ Contextual and Positional Encoding**: Various embedding methods are discussed, especially highlighting that the same word can lead to different embeddings in different contexts, which is crucial for accurate next-word prediction in language modeling.
- **üöÄ Future Exploration**: The session wraps up with plans to dive deeper into the functioning and importance of various embedding techniques in artificial intelligence, particularly in relation to large language models like GPT and their capabilities in context understanding.

# Transcript 


0:2:40 -  yeah hey hi um yesterday we were looking at um after looking at RNN lstm and Gru um we were looking at uh we looked at sequence to sequence um to understand how it works especially when there is different size of input uh the number of tokens in the output input uh and the number of tokens and the output are different um so how do we deal with it that's like that's how does it how is it covered in sequence to sequence um we covered it so we next step to get into is uh having an understanding on embedding which actually is a prerequisite to this sequence to sequence model also and um there will be prerequisite for Auto encoders uh Auto encoders decoders Transformations um which are actually the basis for the large language models part of gener embedding the thought process was similar to one H en coding I assumed saying we just represent the words basing on with with certain set of numbers for all possible words we represent with certain number is my thinking but yesterday as per what we have observed few things we learned is it's not only about a word and a number mapping it also has it actually is um little Dynamic with respect to embedding whenever you call embedding and then embedding is not a um is not um uh programmatic uh function they are actually using um kind of AI model um like an neural network to assign a number for the for given words and um so it's going to be very Dynamic and it depends on uh the context and the sequence of words that you're providing

0:5:21 -  so it's not just U independent besides uh the embedding doesn't uh restrict providing one single single value to a given word there's a possibility that it could provide multiple values as well and these multiple values in 10 can be fed into the network which might have parall layers like parall Alm layers or something like that and those layers can process them in parallel and um and they would together add input like for the n coder there could be multiple layers and the multiple layers might have different embeddings to the same sentence like what is your name [Music] um and they will process them in parallel these layers and the output of these layers is in turn given as input to the decoder so the the back propagation and weights optimization and stuff happens across all these layers and then so similar to image going through multiple layers would actually uh extract U like it had to have different kernels right so having different kernels uh the expectation is each kernel helps in might help in extracting certain feature within the image these different features by multiple kernel layers would in turn contribute to processing uh the particular image or identifying the image so similar concept here as well um in these sequence to sequence models are encoder decoder models and stuff instead of giving it to one single layer of sequence of LSM cells um as what is your name like four inputs and four Alum cells instead of giving it to four Alum cells the they're also having [Music]

0:7:58 -  parallel layers which has the same repeated lstm cells which will take the same input but the input might have different set of embeddings for uh this different set of POS embeddings for this particular words so that's one thing second thing is uh even in embedding um it's not just word and a number like what is your name how is your mom doing so even though e is same in what is and how he is basing on its position uh basing on the context basing on the words beside it there there are I mean like each there are multiple embeddings it seems and each embedding has its own Logic on how it generates but the the the point is the the word uh is not just map to one single number or if you take two sentences same word doesn't have same number like I was assuming the same at least during the eding process same word uh with same meaning like closure meaning uh might have same eding that's how it recognizes the model recognizes saying okay this number has come that means this word is there so basing on that I'll tell you what word should be coming up next but um that is one learning saying embedding doesn't necessarily give same embedding value for a single single word this is definitely we need to understand this process using a program um and it is very very important because embedding by itself is uh not only carrying the word it is also carrying word with the context with again relative position the lot of things that they are adding and embedding itself so the input to these models like sequence to sequence and coder decoder or transformation and stuff input itself itself is becoming very intelligent it is coming with lot of input details so that is actually adding a lot of value to uh the models and um I mean doesn't give the answer necessarily but uh it is is

0:11:30 -  uh giving a lot of context uh relevant context so that the uh so that the models are more so it's two steps so given a sentence the embedding layer actually is Bing model is helping in extracting some level of intelligence or understanding from the particular sentence maybe the position of the thing may be the positions of the other words which are beside it may be um matter of fact the embeddings that we have seen word to cerday they were actually training the model to generate embedding basing on uh the models capability of predicting the next word so what would be the right embedding that we should use use for a given sentence in a way that it could predict the next word properly that is the a very strong correlation between these words is being represented in numbers and that is being taken as embedding not just a representation of that word alone so um embedding is very um becoming is very contextual um so and it is solving the overall language understanding problem to some extent in its flow uh and it's also a critical part of the overall model so saying all that why we need to get deep into the embedding um process um okay let's get started let's it's been a while we use chat GPT so I'm thinking I should start off with chat [Music] GT for [Music] so let's um what are the different kinds of uh embedding and uh where do we use them so that should be a right question to ask so this will give us the list of um he has given amings as in word eming sentence eming sent eming gra okay [Music]

0:25:11 -  [Music] [Music] glass question [Music] sentence sentence sentences [Music] [Music] [Music] graphs graph cap structural cont information from graph structure data social network task like classification link prediction Community [Music] Direction include to Vector graph sales def graph conation NS imageing iming represents [Music] [Music] Rel [Music] transy transer dist M complex rot these are some of Main [Music] for [Music] okay what to maps St vectors space capture semantic and between words size Global Vector GL Global vectors Vector compared to one standable for TAS Global word to work with some word [Music] information for what I'm for fix that's go purpose of Tex corpora what is meaning of corpora corpor is a plural of Corpus which is a collection of text used in language you have access to last I'm might that that [Music] okay let's [Music] see just e [Music] is use pre she [Music] so this question is still open right saying whe the embeddings are specific to models are the in of model but as for this answer it looks like the embeddings are specific to models as in jpt has its own embedding layer to pre-rain Transformer um which helps

0:38:21 -  in providing specific embeddings that including the contextual information whatever be input at GPT okay so that being said i' be lots of embeddings that are there so we should understand on how these embeddings will work effectively um let's see for but e e [Music] e e for e for for e for on for e for e e so so we learned um few things with respect to um embedding especially the evolution of embedding it's pretty nice on how the embedding has evolved over the period of time as for this guy the embedding that is generated using uh compared to word work word to work uh glow um Elmo um and embedding generated Elmo embeddings which are generated based on lstms um and finally the embeding generated by Transformer models um are more efficient they have positional and contextual information more than uh the remaining ones and uh computationally they are more capable compared to computation the more efficient compared to the lstm model based amb system and stuff um uh but we still need to understand we should get to the detail saying to if let's say first we need to figure out is the word to Glow um Elmo if they are they not being used today that's one thing if they're not being used just a contextual understanding of what they are and stuff is enough like high level understanding is enough um and we will focus on Transformer based embeddings that are being there so more like maybe we should tomorrow we'll explore on what embedding process does GPT use what uping process