# Day 19 Study Time Gen AI

**Time Interval:** 00:00 - 51:45  
**Summary**  
- **üîç Session Overview**: The presentation focuses on **Principal Component Analysis (PCA)** and various data techniques, addressing how to fine-tune and format data for different models.
- **üìä Data Suitability**: Emphasis is placed on modifying data to make it more suitable for models, discussing the influence of data structure on training iterations, computational efficiency, and model accuracy.
- **üè• Real-World Applications**: The discussion highlights real-world scenarios in sectors such as healthcare and banking, where large-scale data is processed, emphasizing the need for effective data handling to optimize model training.
- **‚öôÔ∏è Model Training Dynamics**: It describes the iterative nature of model training and the necessity for real-time adaptations when new data becomes available, leading to continuous model improvement.
- **‚ùó Challenges with Dynamic Data**: The session acknowledges challenges in integrating real-time data and developing systems that adapt autonomously based on user behavior or changing circumstances.
- **üåç Future of AI Deployment**: Insights are shared regarding advancements in computational capabilities and the shift toward more dynamic, real-time prediction models.
- **üîë Standardization and Variance**: A detailed segment on PCA discusses the standardization of data, its importance in reducing bias, and achieving a balance across variable ranges to enhance model accuracy.
- **üß™ Statistical Foundations**: The final part emphasizes statistical concepts of mean, variance, and standard deviation in relation to data distribution and the necessity of transformations for improved model performance, hinting at practical calculations to be explored in future sessions. 

The instructor encourages exploring blogs and videos for deeper understanding, particularly regarding statistical methods such as standardization and their relevance in machine learning.

# Transcript 


0:2:31 -  hi everyone so today we'll um look at principal um component analysis and um other data techniques and we'll also look at um we'll try to look at how data needs to be how data needs to be fine tuned or kind of modified um in a way that they are more suitable to model so what we'll do we'll take each model and ask chat GPT and stuff to in order to fit in data to this model what's the best way see models can take up data in refer formats but um thing is in order to make them computationally easy or some models they may prefer to have data in certain format so we will explore those details to see which models has what requirements and what kind of a data is good because that will influence the uh the training iterations the number of training iterations and the overall compute basing on data size because in real world data problems some healthcare banking e-commerce um all these transactional systems will have millions of data right millions of Records so when you have millions of Records even if you reduce your computation by um 1% It is a lot lot of Saving right that to training of um models as you have seen it's ative process it is not happening once like like we do here right you remember in gradient descent what we are doing is we are for every descent we are retraining uh reevaluating the model with different weights so what happens um from from random point the number of distance is nothing but number of iterations in which we change W and uh we retrain the model with that particular um uh equation right so the computation at times could go much higher so um today as we speak long back it used

0:5:6 -  to be like models are pre-trained and um once and then uh deploying them to production and then making them available to people it used to be a lengthy process and people used to um wait for U even days or weeks for this to happen um only huge systems like IBM Watson and stuff were capable of of uh U building bigger models and um certain systems and stuff or certain server configurations are not even capable of building a model and deploying it kind of stuff so that those are the days restrictions and now actually we can do build a good size models on your local system and actually deploy them and all that can happen in less than few hours of time so the the computation capabilities have increased um but the faster is better and uh the the trend is going towards more um realtime predictions right so U like if you look at it chat system right it is real time but it it's more real time predictions right so but how about real time training right that's interesting piece right so uh you have a model and you have data coming in and U basing on the data that is coming in um and also basing on Dynamic data is coming in from multiple sources not just about you and the model basing on data coming in from lacks of people who are interacting with that particular system I say want to train the model for every hour for some reason I don't know I could couldn't be collect of a use case but u i I want to train the model every hour and then um create a higher version of the model I should test it uh with certain test cases that that return and stuff I should test it and I should deploy it and make it available to the user like U it's it's it should it should not be a uh security sensitive um critical applications uh we can actually go for applications which are more like um product suggestions

0:7:38 -  or or offer Creations something like that right um or news or some kind of a news predictions or something like that right so let's say weather predictions these kind of systems are expected to be more Dynamic right so um there are predictions for next 15 days but what happens is there will be some changes right right uh in the weather and stuff so it's Dynamic data basing on the dynamic data we should not only change today's prediction we should also be seeing how the predictions would change for the other days right for example if it if it as per production there is no rain today but there is heavy rain today obviously when there is heavy rain today which is deviating from prodiction if you go back to the system and it saying B if these are the is the way the input features are the prediction is rain if if it classif r s or no it is actually previously no it became yes and you can this being an input to the particular you can retrain it and um you retrain it in a way that for the previous features where it said no it will become yes now right that's that's where you that's the error and you correct it uh once you correct it what happens that's a new model right that's a new learning basing on the new learning for next few days the predictions might vary now if the predictions are varying uh so there is a new temper new rainfall prodiction everything would would vary right so very those differences we need to um calculate them and predict them dynamically times it we need to change them predictions maybe next few hours next hour or something like that right so so as we evolve as we have more computation capabilities in hand as we have more data available in hand the uh the need for dynamic Productions will keep growing and we will reach in future at certain point of time where you don't need to instruct um provide any details to any systems there will be individual systems that are there where um right now what's happening is data is being gathered at the uh last M touch points like like like iot devices transaction systems ATMs and all that stuff right so what happens is the uh the models uh training and everything will happen as close as

0:10:16 -  possible to your uh data point collection systems so that the are it could be incremental like there will be smaller models that that work at the end and there will be bigger models which will be trained periodically kind of stuff so so that the AI lives not very far in a server and then it builds models and gives you predictions kind of stuff but it will go more Dynamic where your ATM machine will itself will have certain models um not only they they'll have just models which predict but they also start evolving they'll start adopting um basing on the people Behavior or or basing on the transaction behavior and everything um this will be a little challenging um and initially maybe they they will'll go for an option saying do you want to do with City do you want to do with um some other BS do you want to do it with that that suggestions or do you want to do normal um way of interacting so and then slowly I think the ey will start evolving and then it starts doing things um the the part where it will be very um um effective is it reduces the humor interaction and and the and the it interprets the inputs in different ways like now we are typing in tomorrow we can actually speak and effectively talk to systems and third um like the way Alan musk is is going towards maybe they'll they'll start um putting in chips in your brain or somewhere and and then then through through actually through thoughts you can start controlling things there a lot of things um are unevitable with that's where it is going people we we we reached a point where people can just slow it down but they're not going to they can't stop this uh Evolution to happen so the intelligence capabilities are are coming in uh um the but the faster they come the if they come too fast we will not be able to handle them so all that government and teams are trying to do is they're trying to slow it down today at this point when we are discussing we are already at a stage where um a lot of

0:15:27 -  things can be automated and a lot of things can be enabled for people but U just because of uh the disruptive capabilities of these kind of a changes people are holding back on releasing those changes okay this is all General discussion we'll get back to the actual study pie part of it that is um we'll focus on principal component analysis principal component analysis I think I'm expected to watch a video uh also I think I should some blog but let's start off with reading a blog um and then we'll go with the a video if it's necessary detailed [Music] explanation of [Music] PC there are a lot of videos which can be we look at it later on first we'll look at some blog a step byep explanation principal component analysis right let's see principal five steps do many steps okay I make constr Asar combinations combin [Music] newable compress Max Maxim remaining information and so explain variance and principal components and components to try to keep the effective knowledge that is there in each of the [Music] comp losing much this is discarding component compones import to here is that the are less interpretable and don't have any real meaning since they constructed as geometrically speaking principle comp represents the directions of the data that explain a maximum amount of variance that is to say the lines that capture most information of the data the relationship between variance and information here is that the larger B the larg the disp dispersion of

0:19:26 -  data BS along it and the the larger the dispersion along your line the more information it has to put all this simply just think of principal components as new access the best angle to see the many principle comp and largest possible variance in the data set of data as shown below can we guess first per the I'm not going to play pick the [Music] projects to the [Music] orig second is calc the same with the condition that it is uncorrelated with the first principle component and that it counts for the next highest variance okay the continues until of principal comp have been calulated it's standardization the a of the step is to standardize is a very important topic and it is very useful it reduces the computation by reduce the overall computation requirement of the model training and stuff drastically so we'll look at more detail on what is standardization and stuff the different kind of we should look at them as well reason why it is critical to perform PC is that the letter is quite sens the Vari of initial variables there large differences between the r of initial varable those vares larger ranges or those with smaller ranges yes because principal conference Andis is dealing with variance variance is nothing but the differences between the points it's time to see how distributed the points are um the so like if you look at distribution uh to understand variance distribution and stuff let's say if you take people um data which which covers groups of the people it has better input data that is putting on the table right and um let's say there is some data that we take for example um marriage uh period and what happens now now instead

0:22:0 -  of covering let's assume that people are 0 to 100 just born to 100 years um if you consider marriage as one of the feature like when they got married it would usually be in the range of 20 to 40 kind of stuff with few out layers um so what happens is that feature actually gives you smaller set of data that is 0 to 20 or number of people 40 to 100 will get minimized so the the if if that is the feature that you pick up then what happens is the distribution that it covers um the Fe the number of people it covers in that particular basing on that particular feature is very few so that probability that that being a principal component is is is low um considering it it covers the range um minimal data range so principal component is something we pick up which covers the maximum variance or maximum range um of distribution among the data so that it it brings more table more information to the table kind of stuff so um it it directly says thatal component analysis when I say range distribution and all that and what what we talking about is Y correct how the Y is distributed how is y um uh being covered basing on a feature that's what um we need to look at right okay now standardization most the reason why it is critical to perform standard here is that okay that is quite sensitive regard variation of initial variables so first of all we look at data right data the way we looked at it previously um there are there are two values one is continuous values another one is um categorical values categorical values are where you you say um it could have certain range of values right for example color of a fruit or shape of a fruit and stuff all this they are not values there certain set of values um little high it could be multiple could be could be only it could be binary it could be three class four class but there are categorical values which have certain categories and they get repeated across all the all the rows whereas there could be continuous

0:24:40 -  values where continuous values um the similarity of the feature data is um is continuous that is you you you can't say um call them saying is this belong to certain category or range you can't classify saying these all people are a BC kind of stuff right until unless you put in a Range there and and and then classify uh convert it continuous values are example um um income levels of people right they're continuous VAR is people living in Los Angeles people living in California people living in Mumbai China Germany basing on the basing on the place you live basing on multiple factors you'll have continuous values right somebody's at somebody's close to $10,000 somebody's close to $1,000 somebody's close to 1 million it keeps going so people are all distributed across so this is this is continuous values so when you have continuous values and they have higher what it appears to be is those uh values in computation uh especially because of having bigger numbers and stuff the the uh two ways two things one thing the influence of those numbers on your model prediction will be high because their values are higher kind of stuff compared to the remaining some something like age and income if you look at it age is always between 0 to 100 or 110 kind of stuff whereas income is like if you take it in dollars and stuff it go to thousands um millions or something like that right so you can't they are not uh when you put both of them into an equation W1 X1 plus W tox2 plus b the influence of the output is more because of the um of the salary or income of the person than um the age of the person so to remove such kind of a bias what they do is they do standardization so that the fit in everything to certain range so um we can make a choice on how we want to standardize it with with what kind of a range and stuff but different

0:27:18 -  standardizations give different ranges kind of stuff so basing on the feature we can give it a thought where certain features if they have any kind of an um negative impact we can say minus one to one or we can make it 0 to one that are different ways of doing it um but here let's see um so that is if there is a large difference between the range of initial variables those variables with large ranges will dominate over those with small ranges for example a variable that range between to 100 will dominate over a variable that R from 0 the one which we discuss just now which will lead to bias results of so transforming the data to comparable scales can prevent this problem so what we do we change the scale like 0 to 100 0 to 1 0 to 1 million whatever it is we standard to one single scale so that everything is projected to a single scale okay so then we go on a neutral basis and um we focus not on the number are the value instead we focus on the distribution or how the numbers are varying basing on the um on each for each record and the variance is is the contribution factor to the model's output more than the actual value that is there kind of stuff and it will be relative to feature to feature right like the first guy having 0.2 another guy having 0.5 so the difference in between variation between 0.2 0.5 is more important than that being um as two 100 to 50,000 kind of stuff right so so this can be done by subtracting the mean and dividing by standard deviation of each value of each variable uh so at this point of time once the standardization is done all the will be transformed to the same [Music] scale so let's look at uh this thing right let's spend some time here on understanding the standardization right so value minus so what how do you standardize how do calculate what is the standardized

0:30:5 -  value of a given value of a feature that means there are n features in there are in which there are end records for each value in each record there there will be corresponding values each value each feature if you take it for given feature that value should be standardized right that means we we should bring it to a common range that's our expectation right so how do you do it is the equation of standardization is value minus mean by standard deviation so let's understand more on how uh this will standardize the values maybe we'll look at an example on how um how the standardization will work so value you already know that is whatever is there this value minus mean mean is the average um like if you draw a line um with all the point values on them um what is the average is the value like sum of all numbers by n like n numbers uh sum of N numbers by n so that it gives you an average value and value minus mean is um the difference between the value and the mean the how the value from the mean Point like if you as I said if you think of all those points being on a straight line um that 10 points distributed across mean come somewhere here so we can actually see value minus mean is nothing but the difference in between the the relative position value from the means perspective right so the number could the 10 values mean is not necessarily in the center mean could be somewhere uh in between and there are points laying on either sides and you'll see that um the value is somewhere in between so what we do is uh we take the distance between value and the mean um all the numbers will have certain distance from the mean um they'll have variable distances they yeah it can't be same distance right now then when divided divided by standard deviation so now let's spend some time on what is the what is standard deviation right oh this is anate so last time we looked at it but

0:34:50 -  uh we'll relook at it it's okay we'll ask here for for okay one to n some of the uh data points data set and represents the individual data point  I've been doing with this this is stupid okay doesn't matter let's go with it as long as I'm studying I'm good n is the total number of data points in the data set x i represents each individual data point and X is the mean so we take the differences in between the mean mean and the actual point to see how how much it is spread from the mean and here is a step by step expl on how standard deviation is calculate calculate the mean okay calculate the differences yes Square the differences okay calculate the variance calculate the average of square differences [Music] obtained difference of three this average represents the variance of the DAT sets Okay take the square root finally take the square root of the variance to obtain the standard deviation okay standard deviation is commonly used in including statistics Finance engineering to the varability it provides a valuable insight to the distribution what does the distance from mean [Music] so measure of deviation are variability within the DAT closer to the mean have smaller distances from the mean they are more rep resentative of the typical values in the N points that are further from the distances indicating from the average indentification of outl larger distances from the mean can of indicate the presence of out layers or new observations in the outlay data points that significantly differ from the majority of the data and may have a disproportion impact on statistical analysis these outl can characteristics of underlying variability by examining the distribution of from the mean across small distances from the indicates low

0:38:19 -  variability data set with high variability varability of the data is essential making accurate protections assing risk and drawing stand distance from the mean to scale the the front the mean from each ATA [Music] Point can be SED to have small because let's say value minus mean by standardization um now what happens let's say we take value minus mean by standardization and we calculate mean of it right um value minus mean the n/ by n will not change um standard deviation is a common matter so it now the average of these is always sum of all values minus n into mean so I should put it on a paper that would be much interesting [Music] so this is more for I know more so U sorry you guys can't see what I'm writing but I'll try to speak as much as possible writing so what did he see okay let's start with Basics now mean mean is nothing but sum of the values X1 + X2 + xn by n and uh Varian is um or first of all we'll see distance from from mean that is uh X1 minus let's say m is the mean which is X1 + sum of X1 to n by n is the mean uh then m is the mean then uh X1 minus m is the distance from the mean that is how far is the point from the mean distance from [Music] mean so variance is uh sum of mean of the distances could be positive and um so are negative to avoid the

0:41:2 -  influence of positive negative one thing we can apply mod L other thing is we can apply square square amplifies the the distances between the higher points kind of stuff than the in the closure point so mean of squares of differences from the mean means again sln Sigma I = 1 to n of x i - M whole squ that is variance standard deviation is nothing but square root that means you we okay so in summary what are we doing um it is similar to Absolute uh sum of differences between um I don't know is that value is such saying absolute x x IUS M by n instead of squaring like like like mean absolute error and this is and um you to apply a square root on variance you are you are squaring the you're doing a sum of squares and you're doing a square root that means overall what what you're are trying to refer to is standard deviation is your um average distance uh from the mean to your numbers so that you in turn you're trying to look at from distances perspective overall All Points distance perspective what is the average uh distance that is there so Sigma = to 1 to n x i - m¬≤ by n um gives you not exactly because you're doing a square and sum uh if you do a modulus then and uh it will give you average average distance from mean of all the points um that's what we are calculating as part of um as part of variance and standard standard deviation okay um that's okay but if you add this um so going back to our problem the standardization we are doing x i minus

0:44:9 -  mean by uh standard deviation right so that will become x i - M by square root of Sig = 1 to n x IUS M whole Square just to differentiate I'll make it XJ X by n the square root is applied for the whole number n okay so now if you calculate a mean of these values what will become it is um the Bas is standard deviation is the same uh by n and um it will [Music] become sum of all numbers obviously X1 X+ X sum of all numbers will be there I = to J = to 1 to n XJ minus there will be NM so Min - n into M that's how it will be so mean will become so what will happen now um Sigma of x i- n into M by standard deviation because we doing an average by n right so if you distribute Sigma x i j by n uh I'm I'm just keeping the standard deviation outside and uh minus n by m so so Sigma XJ I by n by n is nothing but this m- n into M by n so n n is cancelled so it will is M minus M that is zero so what are we doing so in respect of Sigma if you calculate the mean of all the differences in between the points and the mean it will become it will become zero so what we are doing is first thing uh we are bringing all the points with mean as zero if that means we are Distributing the points besides zero right um and what we're doing we are dividing

0:49:55 -  the point by standard deviation right um not the point the distance from the mean that we are dividing by standard deviation that means we are again dividing by not exactly but uh roughly we're dividing given distance from mean by average distance of all points given number if it is if it is divided by mean what will happen that means x i by x x a plus X2 plus X3 so on xn by n will become nothing me of how many small value small than [Music] rais than mean in Su doesn't make any any sense the only thing is how big is the number compared to mean is what say that's that's called division okay so mean we bought it to zero dividing by uh standard deviation what it will make to the values it will make the values um I think it'll just scale down the values but it uh doesn't mean um take a sample set of 20 continuous values and calculate mean variance and standard deviation [Music] that for so the differences are varying fromus 24.5 to 15.5 okay so here if you see it isus 24.5 to 15.5 variance is 121.5 standard deviation is 11 this is -19 okay if you look at it the variance is highest smallest

0:57:58 -  point and is the difference from variance is highest for the smaller point and it is lowest for highest point now let's calculate the difference between the original Val and standard deviation standard deviation is also the same way least for smallest point and is highest for biggest Point even though 32 34 36 are closest to 32 34 36 are closest to the mean 36.5 still their values are not um give me I am I don't know why I'm doing it but of above and of [Music] uh so average of um so uh okay I am uh doing okay but anyways these are all standard deviation is all positive values standard deviation can't be negativity is the square root yeah variance is sum of squares by n how can that be negative variance this is not negative but because of doing variance minus original value which is coming as netive variance is also always positive variance is always positive standardization standard deviation is always positive um mean can be negative positive can be anything you can't see absolute differences from mean uh 75925 20.5 so average of differences is 13.75 variance is 75925 standard deviation average is a 20.5 looking at the actual values of uh values are mean is 36.5 variance is 121 11 is that f so the H zero is in between uh 30 and 32 average will become zero that's understood yo something wrong wrong know uh is I

1:65:18 -  think as for my math it should be average of standardized values uh yeah I'm sure average of standardized values is is nothing but um standardized deviation if you remove it one by standardized devation if you remove it XG like okay let's do average of use am I going that wrong no see original value is uh x i so on and what we seeing x i minus M that means sum of it is Sigma x i to nus n into M Sigma of x i by n is equal to M that means Sigma of XI is equal to n into m n into m n into 0 yes e okay so chat GPT is little off on math here it is not calculating it properly but it's okay not it's not okay because now we are now we are more confused on the overall values right like the previous mathod did is it correct or not okay it pushes me towards saying okay we should always ask that it ask it even for small math also we should ask it to validate it whether it is correct or wrong but let's look at it we should have booked up 10 values instead of 20 math will be easier for us also what is the sum of all standardized values from previous culation it will not be zero standardized values it's nothing but this one right this is standardization square root of sorry no x mean by standardization because of round off his Division and stuff he's rounding up to two numbers that's that's where he has [Music] standard deviation standardization values he understood it is nice 36.5 average is correct okay it's also approximation standard deviation I think

1:73:14 -  is an approximation that is the reason it is not going to zero okay fine we understood variance we understood standardization we understood [Music] um standard okay the only thing that we didn't understand is um standardization makes average zero that is clear uh but standardized values distribution range is is relative it is not uh it's not always minus1 to one or something that but they in thetive range okay what is normalization by [Music] to be between zero and one this process back 12 to [Music] subract point that becomes 0 to 40 MH yo come [Music] on [Music] ch [Music] man very bad basic math basic math also is [Music] doing [Music] [Music] we'll look at all this nor that on but we'll go back to the so this is standardization we bringing the mean of the values to zero and um we are scaling the values on either sides by standardization step two coari Matrix computation the a of the step is to understand how the variables of the input data set are varying from the mean with is the number of di XYZ and then XYZ Matrix multiplied same Matrix kind of stuff it's not multiplication though but yeah okay time up up compute the so how is co

1:79:53 -  variant between two features calculated Give an example with the high points Co of X comma Y is u x IUS x y i- y this gu is repeating multiple times oh I'm off to on how much there three operation COI formula sample coari why there is n and n minus one the co of variables X and Y indeed okay [Music] uh a variable X and Y Sigma that's a formula of X variable is the average values of X variables that's values of Y variable V represents the of Y variable and the sum of values both multiplication of distribution of the values from the mean how does it matter [Laughter] okay that's okay that so increases for one to decreases is because of the value okay interesting but uh so you see product of say product of differences right so given feature XI Yi where there are some features M and the means xus M that is difference between the given feature value from its mean and Y value from its mean product of them if it is taken and taking a mean of it me of the product is covariance why are you doing a product of uh x i- m and Y i- N how far is it from its mean X is how far from its mean why is how far from its mean how far is it from product of why why is he doing product of the correlation between the two random variables