# Day 42 Study Time Gen AI

**Day X: AI Agents Overview**  
**Time Interval:** 00:00 - 37:25  
**Summary**  
- **üîç Session Introduction**: The instructor welcomes participants and outlines the focus of the session on AI agents in the context of deep learning and graph-based training.
- **üß† Understanding AI Agents**: The session explains the concept of AI agents designed to replicate human behavior through a thought process akin to human decision-making.
- **üîç Human Task Execution Example**: A detailed example is provided on how humans approach tasks, like analyzing stock market data. It illustrates the steps taken by humans in decision-making and how AI can mimic this process.
- **üõ† Systematic vs. Agent-based Programming**: The instructor differentiates between systematic programming (task-based execution) and agent-based programming (dynamic, responsive actions) highlighting how agents independently assess situations and react accordingly.
- **ü§ñ Incremental Approach of Agents**: Emphasis is placed on how agents learn incrementally, adapting their processes based on previous inputs and outcomes rather than following a pre-programmed directive.
- **üí° The Role of Large Language Models (LLMs)**: The session discusses how LLMs can emulate human reasoning during workflows by making decisions akin to human thought processes.
- **üîå Agents and Tool Usage**: It explains that while tools can perform specific tasks (like searching data), the complex reasoning is handled by agents that use LLMs for analytical tasks.
- **üîß Future Learning Paths**: The instructor discusses upcoming courses and practical applications on agent designs, encouraging continuous learning about agent capabilities and their implementation in varied contexts.

**Takeaways**:  
- Participants learn the fundamentals of AI agents and their potential to replicate human-like functionalities through thoughtful reasoning and process-based task management. Future sessions will explore specific applications and developments further.

# Transcript 


0:2:34 -  hey hi everyone morning so we have um complete that U agent based uh training on on deep learning um short course um which is um um which is pretty interesting um quick brief about U what it does is uh um it's it's it's a graph based so so any problem solution can be implemented as a graph right and then um but the graph is what they are trying to do as an agent is Agent is something which is equalent to human um are equalent to human behavior is what they're replicating using an agent and um and on an agent we um expect uh the agent to um to go through a process which is equalent to what human will do and um so for example given a task what does a human human do for example you you said you asked somebody to um look at online information um about um how the um stock market price um for for certain shares group of shares uh and um out of which you pick up which share is much more suitable for your processing then what would he do is uh first um um for for a given task uh uh humans would think through as if I'm thinking right now like whatever I said I'm thinking saying what should I do to achieve that task right so I should pick up the list of stocks that he has given we need idy what does the each stock mean what is the name of the stock and stuff right so um and once we know the name of the stocks and stuff um so we will we will actually look upon what is the present stock value for each of them and um and similarly if you want to find more details on um how did it is it growing from last 10 days how is the trend of each of it that's something that we'll look up and then we'll compare the um present price of the all this and what is the trend for all the shares and then basing on the comparison we'll rank them and we'll take an action right so this is what humans does so the same thing uh

0:5:13 -  they will act the same process of generating thoughts and then from thoughts what are the actions that we need to do from the actions what is the decision that we make um not just like all the actions being 10 things and 10 task being done and then that's that's um that's a systematic programming R you you have 10 tasks you take task one executed task two executed task three executed but um agents are agents are even human human behavior if you look at it it doesn't work like that they they don't work like that they what they do they execute task one and um um after executing task one whatever is the response that they get they they take the response and they think through again should is is what is the next logical task that needs to be done they don't uh pre-program they will have an assessment saying what tasks needs to be done and stuff but um they don't um one second sorry um but they don't say that we can't do something else different from it basing on what is the task input that happens for example uh when when task for the first stock when we I'm looking at the price um I couldn't get a trend for 10 days um I could get Trend only for 7 days or um I could get some other information which is much more valuable uh but basing on that whatever details I gathered for the first stock when I execute it I'll get similar um value I I'll get similar details for this next stock because that's how I can compare right if I can get ABC details of stock one then for stock two also I should get ABC details so but initial thought was I should get a and b details so like that our approach will be incremental uh on what could be done what cannot be done and stuff and we keep aligning so similarly agents also um agents also this thought process that human has agent is is exactly replicating it and they're able to do it how are they able to do it uh thought reasoning and stuff uh LM can reason given a process it can reason thinking through um different um mindsets that is uh um someone with a mindset of um

0:7:59 -  um analyzing somebody who okay there two parts to it one is tools which do specific tasks like looking up data online searching the data or um um or for example if there is some mathematical calculation to be done and stuff these are all done by tools because that's um tools can help us do such kind of specific actions but um thoughts is something there is no tools which can do thoughts but so but reasoning and um um thinking uh are analyzing and stuff those we will use llm models those are something which human mind can do and that is where whereever human mind when if when a workflow wherever human mind comes in that's where we'll use the models so this is um and the assumption is the models will give a response which is equivalent to um which is equivalent to how the human Minds would think so that's how they they are calling it as agents because uh um agents are and they say agents they're trying to replicate um what humans are doing okay so that being that's with the agents and stuff so we we'll learn more about agents it's a very interesting concept there's there are um let me share the screen and I'll start with entire screen and then we'll go to so there are few more courses on agents so so AI agent in Lang graph is what we have completed uh this is new one with agent design pattern with the autogen so this will help us saying um so whatever I said is one pattern right on how the agents can be done so the agents can be the evaluation is to to go towards multimodel agents kind of stuff or multi- agents kind of stuff so where we we we create a Persona of different agent like one as one as an Creator one as a reviewer one as a um publisher could be different roles like I mean to be give you realistic let's if you want to simulate a simulate a bank we will say this guy is a clerk this guy is a receptionist this guy is an accountant this guy is a manager like that we will create different agents when we'll give them how do we create a Persona is just by simple giving a prompt

0:13:24 -  that's that's how simple it is and um basing on the basing on the um basing on the prompt the the llm model starts behaving like that so which is interesting then uh multi- a system with crew this is very cool so these two are very important should be done um building agent rag with the Llama index that's also important JavaScript drag web apps with L Index this is done this is completed so this one this one both of them are completed um matter of fact this is also completed three are completed so these two needs to be completed in in the next order and then this two so we will periodically come back and keep doing this uh um but at the same time we should also continue with our actual journey of learning embeddings and and stuff right okay so uh coming to embeddings what happened is we have um the list of embeddings that I that we have one second guys Elmo and bird based embeddings are pending so we have a um word to glove uh these we have completed let me see where we [Music] have what CL it be complet within word to there are two forms that is continuous bag of words and uh and Skip gram uh I'm I'm just looking at uh [Music] thank de explore trans one okay e once let's let's Refresh on Refresh on glow yeah just Refresh on glove I hope it is on the screen okay so clove so glob is learning representation foring is performed Onre Global Co statistics from the car person resulting interestingly sub structure similarities training train on the nonzero entries of global word to word cooccurrence matx with OCC with one another in a given Corpus populating this

0:32:6 -  matx entire Corpus to collect this statistics for R can be computationally expensive but it is one time front cost because of the number of one Zer matx the tools in this package [Music] can be ex independently once while quick video on it let's do I think we we go by for for for e e e oh that's but for you what that for for for I okay so glow model is clear but he didn't try program on and deriving it but it's it's it's probability driv is what you saying and uh there is um not just probability there's also a word Vector representation but but cooccurrence is the um currence of huge amount of tax but uh glow is I think computationally very tough because it is doing it on all words versus all words cooccurrence Matrix with given training set so the bigger the trainings are the higher the and the glow will be pre-trained on a corpus data we don't um we don't calculate the embedding for given sentence it's pre-trained with a corpus data and then uh we use it to retrieve the word vectors when we are know yeah that's a good question FL training and oh I can add the words are as as that's how and then is initialized with the fix words in the tring don't m no this is the other way is a this is the question this is from pre Trend glow edings but it was assign to some unique vectors all zeros that's okay okay fast text [Music] dat but

0:46:8 -  [Music] composed a back of but in fast represented as a back of characters F access characters that's difference example character triag and equal to the of for no approach and even want to fast Tex perform much better it can con the H okay that's fast Tex fast Tex is letter by letter or group of letters instead of the whole word being PR that is faster compared to Crossing word by word kind of stuff that's with word to fast text and then glue glue these are the three things that we have covered um to be for for for J for for bag of words let's keep going for oh okay there somebody watching okay why is this truck okay go here what is this okay so that being left there um okay this is okay there's continuous bag of words is where two words are given we predict what is the next word whereas um skip gram is where you give the the word what is the probability uh where we say what is the probability of the context netive sampling glob and Skip gram skip gram is uh n letter it's bound with n letters whereas um um glow is the global context huge set of words SC what is by Google GL is by Stanford the L and most influen models extension of then to out of for this is this is actually an LP not I'm too many man okay let's move on to so fastex glow and um what to we have revised it now so we will cover um we are we are to cover Elmo and B right so for both of them the pre there are prerequisites like let's