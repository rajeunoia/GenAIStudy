# Day 6 , Study Time Gen AI

**Day 6: Study Time Gen AI**   
**Time Interval:** 00:00 - 47:53  
**Summary**  
- **üîç Overview**: The session reflects on the themes from the previous day, particularly addressing a break taken and clarifying concepts of linear regression while progressing to topics on variable optimization and cost functions.  
- **üìà Linear Regression Reflection**: The discourse revisits linear regression, emphasizing the selection of weights (W1, W2) and how to calculate loss and cost functions, considering mean squared error as an essential aspect of model refinement.  
- **üß† Understanding Cost Functions**: Discussion focuses on the necessity of understanding both loss and cost functions in machine learning, explaining how they measure predictive accuracy and guide model adjustments.  
- **üîÑ Cost Calculation Example**: The instructor elaborates on the process of calculating cost using squared differences, illustrating how this affects overall model performance and encourages prudent adjustments based on these calculations.  
- **üéØ Optimizing with Gradient Descent**: Introduction to optimizing the model by taking steps to minimize cost, highlighting gradient descent as a means of achieving optimal values for W1 and W2. The mathematical basics of gradient descent are briefly presented.  
- **üîç Logistic Regression Introduction**: The session transitions to logistic regression, emphasizing its use for binary classification problems, contrasting it with linear regression in terms of output predictions. It presents how outputs are classified based on probabilities derived from regression equations.  
- **üß© Sigmoid Function Discussion**: The significance of the sigmoid function in transforming linear regression outputs into a range suitable for binary classification (0 to 1) is outlined, explaining how it supports our predictions about class memberships.  
- **üîÑ Loss Function in Logistic Regression**: Exploration of the loss function for logistic regression is initiated, focusing on how predicted values are evaluated and optimized based on actual outcomes.  
- **üöÄ Next Steps**: The session concludes with an intention to delve deeper into the details of logistic regression and its implications for machine learning in subsequent discussions.

# Transcript 


0:3:23 -  [Music] hey hi everyone today is day six I am on um there's a problem let's set it up so today is day six and um day five video I said um I so there's a Sunday in between so I took a break on Sunday um which I thought I should not but I took it which is okay um so Saturday was day five but in the video I said day four okay let's get started let's not um so linear regression and we were discussing about um um so linear regression we were discussing about um we discussed on how the equation equation can be formed um how do we um pick up certain random values for W1 W2 and um for those random values let me share my screen as well so that you guys can look at the document that we putting together so for linear equations for certain data set we thought let's pick up something like this we looked at what is the loss and um what is a loss function how do we create it and stuff and then what is a cost function and why do we need a cost function um we have looked at this and then yesterday we were talking about um how to how to choose um W1 W1 and W2 for the equation so without um will calulate cost for all possible W1 [Music] W2 it will be 100 into 100 into 99 combinations um combinations so this is all good um but what happens is 100 into 99 and uh we are still not sure that we uh have optimal W1 and W2

0:6:14 -  because X1 and X2 can different values values besides why can I so range from range between different values and um we can't what is the correct range of W1 W2 so what we look at is we looked at online the three dimensional graph where there is which has plotted W1 W2 and and the corresponding cost function J so the cost function here we'll call it J so that um in of every time saying cost function cost function we'll just refer to J cost function and maybe the L function can be referred to as [Music] L [Music] um so this is loss is also called error error between Y and YP right so yeah this is cost function also mean square m and uh here the the absolute error of this is uh mean absolute error ma okay anyways so now we need to determine W1 W2 so imagine we got all the W1 W2s and then we put them into a graph and um we were actually discussing on how we can pick up a random point and from there how do you navigate down theand random point to um to figure out which is the optimal way like if you okay I think people I should not repeat everything that is there in day five because it will take a lot of time but quickly bringing bringing up on speed so we thought like we should descent down when we um when we do to in order to reach a lowest cost lowest cost is where you have the best possible W1 W2 and um which is that means the error is low lowest cost means error is low error is low means YP and Y are very close YP and Y are very close means the predictions are more accurate that's all right so we are trying we need we trying to in within the graph we are trying to

0:8:48 -  go to the optimal cost kind of stuff so we were actually discussing on how do you how do you W1 W2 can be varied inin the graph it can be varied in in any direction so how do we pick up the first step and also make sure that we traveling towards the minimum right so we are looking at the graph and then for some kind of a shape that is there at that particular Point uh for that particular shape we want to determine what is the um in which direction should I go right so in order to determine the um direction in which go we need to go so we thought we'll pick up the slope at that particular point and slope of a curve at a single point can be calculated using differentiation that's that was our U um discussion yesterday so we were actually uh trying to look at what is what does the differentiation mean and all that stuff so quick look um we we can ask chat GPT quickly on um asking us to explain differentiation I think it's already did we asked it and it already did so if if there is so we talking about cost function are the actual model algorithm equation any of the equations that we look at if if it is um it is more like a specific function U which has certain variables could be f ofx is equal to something or um right like W1 X1 W2 X2 is is is is nothing but when I say function it's more an equation which has X1 X2 as variables where we can put in the values and then see what will be corresponding y right similarly e cost function is nothing but mean square error of Y and YP right so it is a um it it is a function where the Y and YP are put in and then basing on that we determine what is the value of cost function and the cost function um assuming assuming Y is always fixed uh YP will vary and um in YP the varying thing is um Y is nothing but equation W1 X1 + W2

0:11:38 -  x2 in that the variables are W1 W2 so the cost function is primarily influenced by W1 W2 so the cost function J the variables which are varying there are actually W1 W2 agree so that equation J cost functions equation is actually which is here somewhere there is a cost function man it's not here yeah here this one this is mean square error cost function so this equation if you look at it so differentiation of the right side equation will give slope of this particular graph if you draw a graph of this particular equation whatever you get the other day you looked at it that graph in order to slow look at a slope at a certain point of this equation if you differentiate this particular equation you you get the slope of the um equation correct [Music] so differentiation um okay just sorry guys I was I was actually thinking on what is a better way to um explain it but um so differential gives uh slope and then the SL slope um gives us the direction in which the particular line is going for example if you if you take it um to to that particular line at that particular point it would draw a line like this okay so what does this line indicate is and basing on the curve the line will change okay so what happens to this line and to this point it will draw a line okay which actually is parall to the particular Point um to detail on little bit on differentiation right so differentiation is calculated saying in this W1 W2 that is there in this particular equation if I change W1 W2 a little bit how does the function vary like how much does it vary the cost function like W1 W2 if I change it by little bit like plus 0.1 and W2 by plus 0.1 if I vary them how much does a cost function VAR that is actually

0:14:8 -  how small difference that is created in the input how much difference does it create on um the function it is cost is is what is the main thought process of calculating differentiation right so uh um once you get the slope then it says okay that is the amount of difference it would make for a small change in W1 W2 is what we understand in the cost function so that particular differentiation we would apply it saying okay that CHS but this is the direction it is going so the particular differentiation we would use as a metric on the amount of change that we want to give to W1 so that it changes in this direction so W1 if you remember if if in a given graph if you are here in W1 you can actually don't know that the lines are like this right because these are all not calculated you are only here so once you are here you can go this way long or you can go that way you can go from this point that you see here it can go in any direction but in order to go down the to the possible point which goes down we should pick up W1 W2 in that direction so if you see W W2 if you differentiate the W and W2 will be somewhere here and uh we should go to this step to to the step so W1 minus the differentiation is the line that it is drawn like this it gives you the next step on that particular slope correct because the line is drawn like this from the point correct so you you you pick up that point next to pick up that point um from the cost what is the difference on W1 W2 is is the differentiation the differentiation gives us what is the value of what what is the value to which W1 W2 should be changed so that it it actually takes you down the particular slope so what we do to pick up the next value of W1 W2 that is the new W1 W2 will be W1 w11 maybe and w11 and w121 let's say is equal to W1 minus the differentiation that we have that's how um we calculate uh okay this is logistic so here we should here gradient desent differ to reduce

0:16:48 -  W1 init R optimization algorithm so as I said w0 W1 are actually the variables in the mean square error and those actually impact the cost function which is mean square error in of saying cost function is saying mean square error so what he says is um the new value of W1 is is equal to W1 minus uh Alpha of um differential of mean square error this equation by W1 that derivation by W1 is used in order to reduce W1 so that within that slope it we make sure that W1 is going down the particular slope right similarly W2 if it is for as W2 is also that W2 is equal to W2 minus um differential of W2 into mean Square um so when I said differential I said it's a minute step right it could 01 or something like that so in that particular slope Direction what is this Alpha that you see here is we want to go down but at the same time we want to I mean maybe people were actually going down like that to to get to the local optimal value of cost function so that um um we have we have um a good algorithm or model which can predict YP we You can predict YP why so um then what happened still basing on the initial W1 W2 and basing on this steps what happens if if the curve is like this if you are here and in and the optimal path is here uh it would not be a straight line like that but just imagine if you are here at this point and then you you need to travel all this way like this right so how do you come faster to this is the best approach agree so um what s the way you pick up W1 W that's one thing second thing is the steps that you take for example if you take small steps like this it will take lot of time so but if you take bigger steps you can come faster right uh but there is also a possibility what happens this graph it could be like this right you are here and then it could be the local optimal

0:19:25 -  and again it might increase if you if you pick up something much bigger what happens you go this big this big and then you you will you will come here so and that but your local optimal is here so the these the changes that we make to W1 W2 um are are I'm basing on the differential of that particular so that we are going through the slope but at the same time we don't want to take that small steps we want to increase it but how much we increase it is driven by this alha so we multiply the saying instead of moving 0.01 at at one time into five let's say our learning rate is five then move 0.05 and then make the steps so it is a little faster so that's how the W1 W2 is varied uh let us look at um image linear no sent uh image with steps let's say so instead of that complex car what they did is um they were actually explaining it with the they're putting it on a graph like like this right like this is the cost function these are weights like this is J um and this is weights W1 W2 so if you look at it um what happens let's say we picked up W1 W2 here um and and then he picked up this slope and and then we pick up the next step right and then we we keep growing until we reach this right so I think it also explain on what is the impact of picking up a small Alpha to huge bigger Alpha as well yeah so if if you if you go like this what happens if you if your learning rate is too small you're you're doing steps like this so many right it it takes a lot of time but if you give a little bit then what happens from that stage you you pick up steps like this okay but if you look at it what is happening the there is another variant I mean it's it's too early to discuss but what is the ways people are actually thinking okay how do I how do I know what is a big Alpha what is a small

0:21:55 -  Alpha kind of stuff right so um they have come up with one more approach which is interesting because what happens right as you as as I told you guys it's not like one day thing or two days thinking like people like as if I'm thinking right people think through this process and then they see okay how do still it takes more compute those days actually today if somebody would have done they would not have spent so much time on optimizing it because back then when when initial linear regression is invented and stuff the computes were slow the resources that the hardware has were low so we need to be more optimal the the more optimal we can have an algorithm which can efficiently process more data considering all that they were actually continuously optimizing that particular particular thing and and I'm sure all these one each one of thing that we thinking through and stuff is these people would have thought through they would have published it each change that we're talking about right from loss function to cost function cost function to um to to calculating the slope and taking The Descent right descent and how do we take The Descent descent is through differentiation differentiation is nothing but gradient so um so this gradient based descent and after that how do you optimize it instead of taking the baby steps you you can use something called learning rate and and make a difference on how how much how much steps you make so how do you pick up learning rate it should be um what happens if the learning rate is very small lot of the the learning R is very big all that is done and one of the optimization that that they brought in um after series of steps is the the alpha rate instead of being static continuously when you are descending they thought they could make it dynamic where um it would it would we would pick it relatively uh I will not get into the details on how we can relatively pick up an alpha rate the Lear Alpha means learning rate um but it would it would technically decrease so that um the the initial steps um as you see in this image the initial Steps From the starting point will be higher and when you CL come closer to the convergence it

0:24:27 -  it'll get smaller and stuff so that um if you if you go with this path what happens here here here it could come here or it could come here and then it could take a jump here here and it'll it'll it'll cross it'll keep going up and you might not reach the you'll miss the convergence right you see on the screen here sorry so what we need to do um we we we start off with a bigger jump the bigger jump is driven by big Alpha and we keep reducing the alpha as we as we reduce the cost um in a in a way that it would actually take baby steps when it goes to the last piece so that the point it could reach the specific point or else even if it is this size what would happen from here it would go here that means it is it is missing the actual optimal way and even though it it looks like a small thing when you calculate the difference in cost of being here to being here um that would in turn if you boil down that cost variation in turn reduces your algorithm's capability of predicting YP properly so imagine you imagine even a 1% difference means 1% error means you have one million records and um or let's say even simple number 10,000 records and almost you 100 records 1% 100 records where the uh where your values are wrong YP and YP YP are wrong so which is which is impactful thing right so um the optimal is very important that's what I'm trying to say with respect to picking up this learning rate and stuff so so that's the um going back on here he explains on how W1 and W2 can be reduced using this particular differentiation and stuff so now we are clear we pick up random W1 W2 we use this particular equation and pick we calculate cost we we to look at what is the cost there and we pick up the next points so another indicator is how do you know that we reach the bottom right I think it's simple um within this graph

0:26:59 -  so from here also you'll keep going nobody knows that's it's a minimal point because he doesn't know this right this is going upwards going downwards and stuff this is all future only thing that you know is you come to this point then you calculate cost so you know this cost otherwise this all is future predicted so what what happens he did people when the algorithm is run and when you do this gradient Des you do calculate the next step if the next step is increasing that is when you you decide saying okay so the next steps are increasing so this is should be the optimal point to which I should actually go right I mean I even if you look at differentiation of that particular the slope will also be minimal so your your differentiation term that you use W1 minus that will also be minimal so the W1 change will also be very very small and U that gives you indication saying you you reach the optimal Point correct so that's it with the linear regression um we supposed to do it in one day uh and we have dragged it to three days but I believe you you guys understood what's a linear what's a linear regression what is the equation how do you look at what is the objective of creating an equation to identify patterns and stuff and then how do you how do you predict values as YP how do you look at loss how do you make it Absol Ute um eror how do you why do you need to make it mean and then um and then why do you need to go for a cost function which is nothing but a mean square error where you square the we do a mean of squares of the differences between y predicted and Y um which is called as the error and uh basing on cost working backwards the initial equation which has W1 W2 variables basing on the cost how do you pick up what's your next w W2 so and you you go through this loop loop of picking up multiple W1 W2s as you see here until you reach this yellow point which is the optimal W1 W2 with the minimum cost right so that that's that's all this is all linear regression is the complexi grows much higher and higher you have more features you have more W1 W2 W3 W4 and stuff and then calculating doing and stuff will be complicated but as the machine is doing it we don't sit and can do this math on our own the machine is doing it machine is capable

0:29:31 -  of doing doing computation uh in parallel much faster and stuff so it it works in a pretty decent way right so uh there are more Concepts to discuss but I I'll try to cover them in the next thing so let's move to the next uh big thing that is logistic regression so logistic regression um is more a binary classific um problem um when I say binary algorithm um it's not a problem it's an algorithm it's actually a solution so binary classification when I say binary is 01 classification is classifying is to to segregate on what kind it is so binary classification term means you have you pick up two kinds zero and one and then you classify with saying whether it is zero or one that's all that's B classification these terms actually sound complicated but the if you understand the meaning of it is very simple so um few people name um keep do keep meaningful name so that people actually read that name and understand what it what it means so um but what if there are like like whether it is a car or a bus it's an apple or orange those are all binary classifications for you to make two how about you have um multiple choices whether is it um is it a bike or is it a car or it's a bus so this this is multi classification where you can have multiple more than two kind of it can be three four five how much maybe so using logistic regation we can't achieve that multi classification directly but indirectly you can do that means what you do you classify them in parts so you use multiple models one you what you'll do you'll start off with is it bike are not a bike so rest all is one one category and you can classify like that um so you you pick up all bikes and these are all not bikes within not bikes you you again take it input and you create another model and you say is it a car or not a car then you say is it a bus or not a bus like I mean obviously if there are only three anything not a bike not a car will become bus but I'm just saying you you can keep going like that with um multi multiple classifications so previous model what's the difference between linear regression and logistic

0:32:3 -  regression is linear regression is where you actually are um predicting YP and if you look at our document YP is can be anything right it can be um random numbers of different range now he said 5 10 20 50 like we predicted 6 12 248 minus 18 but let's say it's something like um predicted house value for example then predicted house value is could be in thousands right it could be like 300,000 800,000 something like that so the number could go bigger like um if you're predicting something like U um market value of a company a bigger company or something that could vary from few Millions to billions or something like that so the numbers could vary and and and the numbers if they are varying like that we would usually that varing output is called regression okay um but whereas U logistic regression whereas classification is where you know what is the output what are the possible outputs that are there and we try to fit in the output into this particular possible outputs that are there sorry let's say multiclass classification you can call like 0 1 2 3 and and you need to the output should be either zero R1 R2 R3 so let's start off with boundary classification where the output should be zero and one the one which you have here the linear regression it is giving you incremental numbers so they thought but the output that we need is zero and one so um and as I said most of the algorithms underlying Basics is on linear regression so what they did okay let me do linear regression but whatever value comes in linear regression I want it to be um I want it I want that linear regression value to be set into 0 and one means um it would say if if my output of linear regression is five or six then I am zero if it is 24 then it is 1 like that basing on the linear regression value I can classify and say Z and one kind of stuff so basing on the input number that are there I would classify and said some this is zero this is one kind of stuff so to achieve it they they came up with

0:34:50 -  what is an equation which can take this value output that is um the algorithm that we used here this this one YP which gives me a regression value like this and convert it into which is a classification where it tells me whether it is zero or one kind of stuff when I say it it tells zero or one it it is it usually the Valu doesn't say exactly zero exactly one the good part is it would actually give us a value in between 0.05 and usually we would consider anything between 0.05 to Z it would say 0.5 to one and and that we will consider as one kind of stuff so um which actually gives the probability how much probability it has of becoming zero or how much probability it has of becoming one so let's say you get 0.3 that means for probability of 0 if you get 0.3 that means probability of becoming zero is 0.3 and probability of becoming one is 0.7 so the the more it was towards one so we think it would go to one but when we when you give the output the algorithm says actually like it could be one like like let's say apple and orange it could be orange with a probability of 70% so still it says tells you 30% it might not be an orange it might be apple um or something else that could else but imagining there's only two fruits in that particular basket either apple or orange so 70% I'm sure this is orange but 30% it could be apple as well so that's how the logistic ration output output would be um so we're thinking on um how can a value be um even though it has varying values like this how do you how do I put it into an equation and that equation um is um giving from 0 to one I told the same point 10 times I know which is not nice but they came up with something some equation so logistic uh regression formul so they use something called sigmoid function um why did they pick sigmoid function again it's more it's more

0:37:32 -  applying maths to solve the problem so what they want is 0 to one and what they looked at is um which mathematical equation is the one which will take me to zero um to that range of 0 comma 1 and they they have the mathematicians know that it's sigmoid function um okay let's look at the graph of sigmoid function if you look at it quickly you can understand um why they picked up sigmo function in stuff there could be some function so why did they function so a sigmoid function graph looks like this okay so it would it would go very close to zero and Center value will be 0.5 and then it goes to one so this is perfect right like so then what we'll do let's say zero is apple one is orange um then what happens is anything between 0 to 0.5 we can call it Apple 0.5 to 1 it we we can call it orange so that's that's the kind of output that we looking at and sigmoid function the way it is it is perfectly fitting in like that right like it starts off with values less than zero5 and travels all the way between one so it's it's somewhere between zero and one this is this they understood this is the perfect fit for how the sigo function could be and sigo function as we as they know the equation is 1 + 1 by 1 + E power - x that's the thing instead of X what we did so we take points we create linear regression the linear regression output that is there that is YP um instead of calling it YP they are calling it uh Z here if you see um and uh that Z they are putting it here it is minus Z so what is the value of Z is C w0 + W1 x + W2 x + this is nothing but our linear regression equation so linear regression equation is done you calculate Z and then you put it into the sigma function 1 by 1 1 + E power minus Z and that will give us um the probability of for the particular classification correct got it okay let's ask

0:40:10 -  actually why did they pick function logistic regression is motivated by several desirable properties one is output range uh monotonicity is monotonically increasing meaning that as an input Z increases the output propability also increases similarly as Z decreases uh this property ensures that change in the input features correspond to predictable changes in smoothness the sigmoid function is smooth and differentiable everywhere which facilitates optimization sigmoids derivative um is sigmoid function let's say we call it Sigma Sigma is Sigma if it's differentiated he's saying the differentiation is Sigma of 1 minus Sigma I would love to do actually the the math to show you how the differentiation of 1 by 1 + e minus Z will work but um I think this time I'll step back I'll compromise saying I'll I'll put it outside the scope of this discussion but I did I did it when I'm studying it and you guys can also do it so all you need to do understand is difference ation of e x if e x is differentiated what do you do with it kind of stuff so that boils down to 1 by one + E power minus z um how do you differentiate it kind of stuff you should should give it a try so that's how the sigmo function is discussed it is I think in line with what we were discussing so um same logic um as we did in linear regression so so linear regression Y and YP is the output and so what we did we calculated loss in between YP and Y and then we we um calculated cost as well basing on cost we we were changing W1 W2 but whereas logistic regression it is two steps right like you you get the X1 X2 you put it into the regression equation w0 W1 X1 kind of stuff and then you put it again into sigmoid function and then you get the output so your output is not YP now it is your output is actually YP which is put into a my function kind right so um now how do you um Calculate cost because here what happens um in the cost

0:43:6 -  function your cost function is actually um predicted value that is like apple is zero and orange is one and if your predictions are somewhere uh in between 0 to 1 using because it's a sigo function if you do difference of them say 0 - 0 0 um 1 - 1 0 so it's like [Music] um that's how it it will be right like so like let's say if if it's 0.5 then it is- 0 0 - 0.5 if you're trying to predict at 0.3 and uh if it's an orange and you you get 0.3 then 1 minus 0.3 that's you're supposed to be one but your actual value is 0.3 for Apple for orange then the error is 0.7 right 1 minus 0.3 um that's how you should um differentiate so to do this differentiation they came up with a loss function I would love to derive this loss function and it it makes complete sense um but so Sigma is the sum of losses across all the records correct and and we doing a mean so um and if you see we not doing any any Square here um because the values are always in between 0 to one right like anything the law should be within that so um there's there's there's no need for amplifying it as something like that um but um if you if you look at it if you um in in this scenario Yi is the prediction right on what it is kind so why could be in 0 to 0.5 or 0.5 to 1 I'm just thinking when they put in Yi um do they keep um let me see once again this is a good question to ask and and it's nothing wrong actually if you I I I remember looking at it but I just want to reconfirm saying whether the Yi that we see here in the L function is it something like the 0. pi or is it going

0:45:41 -  to be it corresponds to Z is equal to zero um or should it be uh should it be the 0er and one I I remember it should be zero and one that that's the logic but I'll cross check on it maybe I'll give the detail tomorrow um which is okay I can look up now also but instead of interest of time I'm not looking up now um but let's assume it's 0 and one then what happens if it is zero then the loss is very simple zero this becomes whole zero this becomes one then the loss is log of 1 - y okay um if it is one then then this becomes zero and this becomes log of Y correct so yeah so here you can actually see the difference um basing on the output value like whether Y is zero or one we pick up the loss this the loss is subjective okay uh the loss is possibly two two values either it's log y uh or log 1 minus y and when when is it log y when the Y is 1 it is log Y and when Y is zero the loss is log 1 - y okay so um in order to accommodate it they have put together this equation both Y and 1 - y multiped so that you you get the only that specific value other y other log y or log Yus y you get the corresponding value basing on the Y that we trying to predict for zeros this one for all zeros this one for all ones this one and Yi is um Y is definitely not 0 comma 1 um the one which we are discussing previously it's not 0 comma one it uh varies and um um basing on how it varies it is um sorry basing on how it varies it will um it will give specific value out of it the logarithm of that for example log of 0.5 um on either sides because 0.5 is one place where both sides are possible