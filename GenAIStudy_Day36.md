# Day 36 Study Time Gen AI

**Day X: Study Time Gen AI**  
**Time Interval:** 00:00 - 37:25  
**Summary**  
- **üîç Overview**: The session begins with a focus on previous implementations related to **word embeddings** and **stop words** in text processing. The instructor shares their screen to demonstrate the workings of a specific **Word2Vec** implementation.
- **üìä Implementation Recap**: The discussion covers yesterday's work on downloading and using the specific implementations, emphasizing the importance of **stop word handling** in training datasets.
- **üß† Word2Vec Explanation**: The instructor explains the underlying process of **Word2Vec**, especially the **skip-gram model** with **negative sampling**, detailing how the model predicts context words based on given words.
- **üîç Debugging Process**: The tutorial includes step-by-step debugging to understand how inputs and outputs function within the model, allowing for the removal of stop words through specific coding techniques.
- **üìÇ Data Processing**: Emphasis is placed on loading libraries and preparing the data for training, focusing on how to set up **context words** and handle negative samples effectively.
- **üß† Understanding Context**: The instructor illustrates how context words are retrieved and how the model predicts relationships between them, reinforcing the aspect of context in word relationships.
- **üöÄ Next Steps**: The session concludes with a note on preparing data for training with examples and promises to delve deeper into Word2Vec and data optimization strategies in future sessions.

# Transcript 


0:5:11 -  [Music] ice [Music] cream um so let's share my screen [Music] and uh so yesterday work to work uh one of the implementations we have seen we download it oh yeah I forgot the thing man for what else is there stop words training text I think that's all there this [Music] file which folder is this how big is okay let's put it there [Music] okay so let's uh we've got bought this words stop words and training text of this what this guy has created this is uh from rck ma he has a YouTube video he has done on the word to work um embedding so I'm just running his notebook to understand the word work embeding better and this is's trying out um what to work skip gram with um negative sampling which is the optimized version of Word to work um the initial is only skip gram where a word is given we predict the context um but negative sampling is an optimization that people have done on top of it and um so let's see how it how it works but not sure how far the inputs are uh sorry outputs are printed here but we'll try to understand more by debugging it more okay so let's read the stop wordss I loaded these libraries tqm i m plot Li and stuff everything is available now so I bought in [Music] stord let's have debug somewhere so that okay I let's bring in the text this is good let's also look at stop wordss once so these are all picked up as stop wordss so he's um removing stop wordss so if W not in stop wordss then only he's is's counting under St so if you see in this code

0:8:38 -  um oh like Cas sens is it I still [Music] here me is also here print text is like this and if you see print text afterwards this I Me Myself and all that should be gone so text see we will be is gone today learning fundamentals it's only picking up the so removing this top words like about the off and all that it's removing and then it's setting up the remaining text this is the text that is being ConEd for training I am I am has come and what we going to do is um with the window size we'll pick up one word and then predict the remaining words in the context that's that's how we are going to train it okay so this is the text that is being used [Music] and so window size is three number of negative samples is equal to three so when he says window size three I'm assuming when a single word is given so we are trying to predict three words um three words in its context and also we are having negative samples three so there are six words are in the output and we and we predict on um what is the um whether it is in context or not in context that's that's how we we going to trainable data TR overall words [Music] so [Music] to so let's let's add debugs everywhere to see what is going in what is coming out okay print idx comma centercore word and um so if you run this so have text here let's do this so these are the words let's see how they are in which order they're being picked up so he's going with fundamentals first and zero level so see okay if his window size is three that means fundamentals and the context that we need to

0:12:29 -  predict in skip gram is today learning data science these two should be predicted related to fundamental so he is picking up fundamentals and then next word he picks up his data statistics so let's look at um fundamentals data science statistics okay he's skipping the first two and from there he's going the rest of the rest of the words are being picked up I think um let me see means from 2 [Music] 012 colum minus 3 that's the text that he's picking up okay because the first two words and the last two words will be will not have the context to words so he's skipping them makes sense so he's trating through the whole whole words okay so now next iterate over the context words around the center word okay now he picking up context words so if you look at Contex what is context words print uh centercore word comma okay let's go with let's start using F man f is very centore word and um context uncore words so for each uh word he's picking up context words and then for given context words he's iterating through each of the context word and he Z and get words not in the current context as negative samplings that uh so we run this so as as we discussed so if we go to fundamentals today learning data science are the context right for data the context will be learning fundamental science statistics for data learning fundamental s okay so this aligning with whatever we have learned in the embedding right and post that for um instead of doing this let's go with let's define a meod called and one about I

0:17:36 -  that okay so debug is on now dat he's what he's doing centercore word that is fundamentals let's say comma context underscore word that is today one so all the words that are uh there in context he's adding them as one so it will be fundamentals today one fundamentals learning one fundamentals data one fundamental science one and um he's getting negative samples so negative samples if you look at it um w means if if you take these all words 1 2 3 4 five from the text the word which are not there in this five words any word if you pick up pick them up you can consider them as uh this thing in the random choice he's saying number of negative samples is three so we are adding um oh for each word we are adding three samples okay samples data center worlda zero okay so let's uh now then be repeated data is up right [Music] so debug data at the end that's enough it'll be huge now so because he's doing negative samples for uh each of these words 1 2 3 4 1 2 3 4 so these are the words he has used as negative samples for today learning data science they are added so it will become 4 3 is 12 + 4 16 but all of them are being added to data so this is all one set is it okay now data is ready um label data isal to data is equal data okay here ital NP doct 1D do context word comma DF do centercore word [Music] DF centercore word cont text on what is in Words Words is nothing but I think it is

0:23:55 -  intersection between Center words and context words if there is a word which is there in Center word and context word both if the both okay okay okay if the both words are the same then he's dropping them for example if you do I think for these what what would have happened is uh look here these words somewhere [Music] top 10 somewhere alternative is there so if you go here statistics growing FS alternative yeah here alternative these words will not have context word but uh why is alternative dropped do you think alternative is repeated oh like data alternative alternative so DF do centore word is in words so it's it's let's say context underscore word and centore word if the set of words and intersection between these two is taken so these are all the words which are common in between contactor context _ word and centercore word there'll lot of comments why only few came that's okay and um so given so if you de de before I can actually see 983 and 982 only one got eliminated [Music] to or maybe his point is intersection between this word and this word is um for a given row if both are same that's where he's going anybody is here no um these two words are same then he's picking it up but if there are that many that means there should be a word in the list which is alternate to alternate to time time for example if you pick up equal to let's say [Music] okay one second what is that we using that see p dat call Value

0:33:11 -  DF off and okay [Music] fine doesn't make sense but let's use that so let's see if there is time time there's no time time then how is it uh coming inct of of two columns the intersection of is SED [Music] so oh yeah let's let's try this [Music] C all the words will be repeated now sh 33 the this is length of what text 67 only 33 are common in between context words and Center words uh why is that because if Center word is moving from each step plus to four If you eliminate 63 will be there 63 are in between if the window is moving from top to bottom when you go to the next words for example fundamentals will get data data will get fundamentals right all the words will have so that's how it is here also if you see fundamentals has data now next data will have fundamentals so this fundamentals should be there intersection be there let's pick something which is not in words yeah let's pick something which is not for example thank listening talk oh unique words okay okay okay so unique words you should see count of this is just an array right [Music] AR by it's in the first case it's not uh pandas um make words in St words uh set set if you set but that works on using Loops am I SC 34 the last word that those will be there I scord on today's on is different today not there here today because today is in the starting most probably yeah today is the first word and today is not repeated anywhere so