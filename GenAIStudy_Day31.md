# Day 31 Study Time Gen AI

**Day 31: Study Time Gen AI**  
**Time Interval:** 00:00 - 37:25  
**Summary**  

- **üîç Overview**: The session reflects on the journey so far in exploring **AI and machine learning**, focusing particularly on **classification models, regression models**, and **clustering algorithms**. The instructor emphasizes the importance of hands-on programming to solidify understanding.

- **üìà Progress Recap**: The instructor summarizes the topics covered, including generative AI, AI concepts, machine learning, various model types such as **linear regression**, and **logistic regression**, as well as **decision trees** and **random forests**.

- **üõ† Practical Learning Focus**: The emphasis is placed on applying theoretical knowledge practically, highlighting how programming various algorithms with sample data enhances comprehension and application to real-world use cases.

- **üìö Neural Networks Introduction**: The session transitions into a discussion on **neural networks**, including **CNNs (Convolutional Neural Networks)** and **RNNs (Recurrent Neural Networks)**. It covers the importance of image processing and the sequence of inputs used in various applications.

- **üîç Deep Dive into CNNs**: The instructor elaborates on CNNs, explaining how they process images using **filters and kernels** to extract features, detailing the concept of **strides** and **padding**.

- **üß† RNN Functionality**: In discussing RNNs, the focus is on their capability to handle sequential data and their dependence on prior inputs, showcasing their application in **NLP** and other time-series data scenarios.

- **üöÄ Advanced Concepts and Challenges**: The session introduces challenges such as the **vanishing gradient** problem in RNNs and explains how **LSTMs** (Long Short-Term Memory) and **GRUs** (Gated Recurrent Units) address these issues.

- **üîÑ Importance of Context**: Contextual dependencies are highlighted, showing how previous inputs can significantly affect outputs in sequence prediction tasks, such as language translation and sentiment analysis.

- **üìñ Next Steps**: The instructor suggests focusing on practical programming exercises and further studies on **transformers** to deepen the understanding of generative AI concepts, particularly in relation to **NLP** and sequential models. The session wraps up with plans for future explorations and encourages active participation in upcoming discussions.

# Transcript 


0:2:52 -  hey hi everyone um Spotlight portrait mode something I don't know what it is okay let's continue this is day 31 and let's continue our study we have um till now covered um AI um the path toward generative AI we have covered we started with understanding of AI then machine learning and then um we've g into models like um what are classification models regression models linear regression logistic regression you looked at d trees random Forest na base clustering algorithms which are unsupervised um K means clustering and um what else did we look at we need to program all those later on one by one we need to program each of them which I'm been pushing but um yeah the focus should be there because programming them hands on with some sample data will be giving much more um better understanding of how things work and how we can apply them to the specific use cases so thinking like if you take one regression problem we like try it out with multiple approaches that are there like linear regression dish entries na Bas Bas is for classification so uh um neur networks and all that could be we could look at all of them and see how they are working and random for us and um we could also apply different um uh optimizers optimizers respect to varant descent Anda and all that stuff and we can also am I missing any algorithms other than what I have said just now I think we could we covered all of them of them that we have cover there multiple out there but we have covered whatever is relevant to us um now coming to neural de planning um we started with an and artificial neur networks and then we came to CNN we then came to RNN um we looked at um um foration back propagation and then we looked at um how to address the overfitting problem by dropping

0:5:49 -  cells and um dropping cells reducing layers [Music] and uh I don't I'm not able to recollect different techniques into interest over fitting but but primarily the major one is where we actually uh drop certain um cells within the layer so that um uh it doesn't everything and it doesn't get to or fitting stage and um uh coming to CNN I think the focus is on uh um image on how we read the image and the image being big how do we scale it down so it has a frame model where it it keeps moving that particular frame or filter or we call it filter or kernel and that filter kernel will come with certain weights attached to that particular kernel and um we move that filter kernel across the image and that movement steps is called strides so the strides can be defined as one step or two steps kind of stuff um in order to um uh have equal concentration of the edges and the center images and all that stuff we add some padding um to the image so that um it's a dummy padding of zeros around the image so that we cover the edges also in equal um filters or kernels compared to the center ones um basing on different kernels uh after training it will start extracting different features within this particular um um image and um M these kernel outputs are in again converted into a single input um and that input is given to a normal Ann and the Ann tries to predict basing on those inputs it tries to predict what is the output value and the output value can be used for class a purposes for requirement kind of stuff we can revisit CN but this is the outlayer idea of CNN coming to RNN is where the which is what be use for our NLP by the way I got a gift from my daughter

0:8:35 -  she gave me Iron Man so what I'm going to do is like keep this Iron Man every day beside the study time so that it is motivation that's a different story and we'll discuss about it sometime when we really have time okay coming to RNN Rec neural networks I think the challenge with UMX is that it um when it deals with the problem where it needs to consider sequence of inputs where each input is not independent of each other there is an interdependency between each input kind of stuff so um and the output is dependent on the sequence not just the inputs in that particular order like ABC to BCA to cab kind of it has dependency on each of the word each of the letters coming in different sequence kind so kind of Time series data are sentence with list of words a word with list of characters anything like that would be considered as a optimal use case for RNN so RNN uh come with the concept precussion where the uh yeah output of the RNN is given as input to this particular RN itself so the usual pattern is you have W1 x1+ B like WX + b y equal WX + B but what happens is here when we apply recursion the the output of the previous will become the input of the so it will become WX plus u h where H is or you can say u y t minus one like the previous input uh plus b so that's how the equation will be so the previous input will also be considered as part of the so X1 becomes X1 + X1 comma y0 then X2 comma y1 X3 comma like that it will consider the uh um the outputs of the previous value so it's it's not just an output of each input and then combining them together

0:11:12 -  or processing them separately but it is always um um input of output of X1 X1 + X2 X1 plus X2 plus X3 outut X2 X1 plus X2 X3 X4 output so it goes in all in a series but a combination of with with the kind of a context of what is the previous to xn if it if it's xn it consider the context of X1 to xn minus one and then on top of it xn use corresponding output so like if there is a list of words that are there and then these are xn minus one words then xn word is predicted basing on the xn minus one words that is that is there kind of stuff so that's the interesting part of uh RNN so the output as you can see the output is um uh could be to find out the next word um and let's say the word is represented by some some Vector then it's a sequence of letters like n minus one is the input and xn word is the output right word or letter is the output so that is a sequence to Vector output um so there are different combinations that we can see it is sequence to Vector um sequence to sequence that is for example uh translation if you give uh if you have um list of words and um in English and then we want list of the words converted into um French or Italian or whatever then it would take each word and then it would convert the equalent one basing on the context it's not just the word becomes X it's not word by word conversion but based on the context it would do a translation to a sentence meaningful sentence of the specific language so um that is sequence to sequence and then there could also be a chance of sequence to Vector where for example there's a sequence of words like a movie review and then we want to predict saying whether it's um um good review bad review or um you can ask a sentiment analysis saying is it what kind of a sentiment does it um is it like somebody scared or like that kind of so a sequence of words to Vector is also possible so sequence to Vector sequence to sequence and then a vector to sequence is like um more like embedding right like maybe you give a number and basing on the number it actually says

0:13:55 -  what is a word let to that particular number kind of stuff so uh word list of characters or whatever so so it's Vector is what you give us input and that gives you a sequence output so something like that could also be possible um but uh there's a challenge here that is uh uh diminishing gradient or exploding gradient kind of stuff so what happens is um the weights are same the one key thing is X1 weights and the previous output input like y t minus one weights let's say U are same across these recursion even though it goes X1 X2 X3 X4 so what happens is when you multiply when you say u y1 u y t minus 1 YT minus 1 is again um wx0 or xn-1 + um u y t minus 2 correct so then what happens U into U multiplication will happen because you are multiplying again with you so it depends on the U saying if the U is um number like um let a 10 then if there are 100 words then 10^ 100 is the multiple then it becomes too big right so uh uh yeah R let's say if it is u u is 0.1 then 0.1 100 will become very small right so it it it will make it too small that the influence of that particular uh output um our input both of them will be very small so uh both ways uh it's a it's a problem to handle so and what happens is it it it loses the context after means a word that has come 10 steps uh um either will influence the overall output to a major extent or it will diminish to extent that we don't even know that there's a word called um some word X in the particular 10th place is gone because the the weights are it is Multiplied with 0. 0.1 whole power 10 and the impact of it and the output is minimal right so this is the problem with the base RNN so that is the reason rnns base rnns are not used extensively

0:16:27 -  they introduced to solve this problem they introduce lstm and G Gru so long short-term memory lstm and then Gru is cre a current Network um um how does these work is they they will start two parallel paths One path will remember the um One path will carry the chain of output that is coming in from each of them one path will remember the um One path will remember the output separately without being multiplied by the weights so because multiplying by the weights is where it is being impacted right the the diminishing or increasing or whatever so the diminishing or increasing will still be there but it will be there with the the Baseline path and on top of it they're also carrying the actual output of the previous uh separately without multiplying with any weight so it is not influenced by weights that's the logic in elas that's not a it's not a big deal but it's a good Logic on how they have done they they've used a sequence of layers to you know to constru and it also has actually they use sigmoid function to decide on how much they want to remember so it's not like we always remember all the words kind of stuff but um the network on its own takes a call how much it should remember the previous output kind for example sentence that is there the sentence is already completed so um do we need to necessarily remember thing do we need not necessarily remember anything that how much percentage of the previous context we remember is also learned by Network on its own so let's say what happens if I remember 50% what happens if I remember 60% what happens if I remember only 20% so that it will try to adjust as if wej just weights in a uh AI or a machine Learning Network we they just that percentages as well and then that percentages have create some kind of impact and basing on that impact they pick up the percentages at each level and um so that's how this sigmoid that's how this lstm and stuff will work by deciding on what uh amount of influence should it consider for the next level uh so that we are not trying to combine all of the uh inputs on the previous instead it will have some weightage of the previous and then also some weightage of the current data that is there at that particular step they're

0:23:44 -  called like each of the words or whatever input is is is called steps okay till here we are clear only thing we have also looked at an example where it was actually predicting the next number like and how this math Works multiplication works and all that that we have seen Ober T functions how does it work sigmoid how does it work and all we we looked at it and and stuff but what we didn't look at is um are still the open when in a sequence to sequence question that I have raised is sequence to sequence predictions if I have what is my name then the prediction the answer is uh like let's say your name is Raja then how does it know when it it is predicting that the first word should be your because what doesn't mean anything what alone doesn't mean anything what could have a number of possibilities after like what is the meaning what is your name what is the uh place we are trying to go there could be a number of things the answer definitely varies basing on what kind of a question that you have depending on the remaining words there so before looking at the remaining words how does sequent to sequence predict the next next um word or does it go back and correct the words how does it work so sequence to sequence something an open end question so we need to study it and understand it better so that's still kind of open so that is where we are um so we need to go deep into it uh um we need to study transform okay so let's at looked at let's share the screen okay different types of of RR unit LM Miss has something mg you one to one one to many many to one he go go up this know a so for so sequence two sequence model so let it be on more