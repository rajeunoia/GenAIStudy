# Day 15 Study Time Gen AI

**Session Summary on Bias, Variance, and Regularization in Machine Learning**  
**Time Interval:** 00:00 - End  

- **üîç Overview of Bias and Variance**: The session starts by discussing the concepts of **bias** and **variance** in machine learning, defining their roles and importance in model performance, particularly in relation to **underfitting** and **overfitting**.

- **üìä Understanding Overfitting and Underfitting**: The instructor explains how **low bias and low variance** represent the ideal model, while high bias leads to underfitting and high variance leads to overfitting, impacting a model‚Äôs performance on unseen data.

- **üõ† Strategies to Handle Overfitting**: Various methods to avoid overfitting are introduced, including **regularization**, **cross-validation**, **early stopping**, **feature selection**, and model pruning. These strategies aim to improve model generalization on new data.

- **üìà Regularization Techniques**: The session provides a detailed look at **regularization techniques**, focusing on **L1 (Lasso)** and **L2 (Ridge)** regularization methods. The importance of introducing penalty terms in the loss function is discussed to prevent complex models from fitting noise in the training data.

- **üßÆ Loss Function and Generalization**: The relationship between minimizing loss and achieving a well-generalized model is elaborated, explaining that achieving zero loss may lead to overfitting. The session emphasizes the need for a balanced trade-off between complexity and accuracy.

- **üîÑ Iterative Model Optimization**: The process of updating model weights through iterative adjustments based on calculated loss is outlined, introducing the concept of **gradient descent** for optimizing model parameters.

- **üîÑ Variants of Regularization**: The instructor also discusses combining L1 and L2 regularization into **Elastic Net** to leverage the benefits of both techniques while addressing various modeling scenarios.

- **üéØ Conclusion and Next Steps**: The session concludes by summarizing the importance of regularization in machine learning and sets the stage for future discussions on advanced optimization techniques and deeper exploration of model evaluation metrics in the subsequent sessions.

# Transcript 


0:2:36 -  good morning good morning everyone so today um previous day we have actually seen respect to a model what is U bias what is variance and we have also what is underfitting and overfitting both of them both of them are related to the diagram actually showed one low bias and U low variance is actually our Target State and high byy is underfitting and high variance is overfitting so they also discussed on um how we arrive at this conclusions basing on uh um the given data and how do we uh split it into train dat train data and test data um and uh how do we handle um under fitting and and or fitting is is what we started with uh so in continuation to it one thing we um discussed uh with respect to overfitting to or to avoid overfitting is to um to avoid over fitting is to do regularization is one thing we ask also chbt to uh tell us on what are all the possible ways we can avoid over fitting and uh it's at regularization Cross validation early stopping feature selection emble models pruning and documentation I think we have gone through all this but um we we definitely have a need to go through it in detail so let's start off with the regular so that's something that we will cover today and we'll also try to see the main intent is so we said given a data how to find patterns how to build machine learning um so how do you build that equation which actually represents this pattern and this equation is nothing but uh your algorithm or model and um and then how do you optimize that equation in order to fit in this particular pattern fit in closure to the pattern so that the loss is minimized that's what we we've looked at it so now what happened we saying the loss is minimized at the same time we

0:5:10 -  don't want the loss to be zero because if the loss is zero we are going towards or fitting if it is overfitting then the the performance of the model on the unknown data which is our actual Target our Target is not the trading data trading set or test set our Target is to um predict uh y with a given unknown values of X so considering that uh scenario we should not be overfitting to the training data set or else anything that is coming outside the training data set the um the model might not perform well so to consider that scenario we wanted um to have a a model which is actually uh neither overfitting or underfitting um so that it can accommodate the future unknown values and predicted value with minimal loss right so to avoid um overfitting uh what we thought of is um um the overfitting we what we thought of is it has already given us multiple ways um okay sorry I'm I'm I'm coming back to this point I'm repeating what I'm saying but what I'm trying to see the other piece that we are trying to cover today is uh um for a given model if you build a model um then you evaluate it basing on the test data set um and uh so we should also communicate to the people right so you you you put a model in place and then the model should um we should be able to communicate to the people saying this model I have built and um it comes with this level of accuracy somebody using chat GPT and even their evaluation will be specific with specific questions or specific pattern for example they'll say chat GPT on on on coding exercise this is its performance for J gpt3 GT4 has this performance levels on coding or a different scenario of set of questions there and then set of U notot say questions but sorry um different P different um different input criteria will be taken considered for evaluating the particular model um on on its

0:7:41 -  capabilities so basing on that we score it and then and that's how we compare so like if you talk of J World there are a lot of large language model that are being published every day like meta has its own model open model um and then Microsoft has its own model model um Everybody right there a lot of people who are fine tuning it and then everybody comes on the board and says okay my model is good as good as chat gpt3 uh somebody claim saying my model is outperforming gp4 so when this kind of um comparison is happening what they do is they they when they to call that their model is performing better than another model they should have a similar metrics that they need to use a green so these similar metrics are um the way they evaluate a model and all the teams agree saying these are the common metrics using which we evaluate the model and say uh that's the right thing to do but challenge is um if you don't get into a common metric then I'll ask I'll I'll frame my own set of questions like my my own set of 100 coding questions that are there and then ask J GPT or whatever model that I built my own model and then I I make sure that it answers all those questions like as I said if it's overfitting the training data set I have and if my training my test data set is more aligned to training data set then actually I'm little biased with what I'm trying to prove because my my model is already or fitting to this particular training data set and then when I test on this test data set my evaluations will have give me very good results but that doesn't mean the objective of AIML model across the world or anybody who builds it um should be on the prediction capabilities of an unknown unknown data so on this um [Music] so um doing this if the if there is no standardization on how we evaluate the models and how we communicate the metrics uh in a way that people can understand uh is not there then we will not be bble to compare it on equal grounds so people coming back on saying llms um mym performing better than GPT 4 or gp3 or whatever so we should actually

0:10:15 -  ask them um uh okay what's the how much data is your model trained on and um that's which is something we know already second is um you see that um we know another things about model is number of features and number of parameters parameters as in W1 W2 W3 which which gives us weights on each feature and on how to uh correlate them so we should say how much amount of data that it is trained on how many how many parameters are there in your model that actually um helps us to understand how much is is uh being considered parameters at times I need to see I don't know but we will also cross check on how the regular terms also are called parameters or not but usually they are because they also vary with respect to model but they could also be D they may not be correlated to Features uh they may have their own functionalities or they have their own behavior Dynamic Behavior basing on the incoming data or um and other detail right uh with that being said let's ask CH GPT and move forward on how the remaining uh um techniques which uh impact regularization uh which impact overfitting are dealt with and what are the different kinds of techniques so we'll start off with so our Target is we we will not go through this early stopping feature selection emble methods pruning data augumentation model selection are more approaches whereas regularization Coss validation are real Concepts that we use in uh um addressing the model uh to be generalized U um a model being uh a model not being either underfitting or overfitting uh that middle ground is in a way called generalization where uh the the model doesn't rely on the training data or the test data so much it is G iiz to an extent saying um it it can actually U so the model or equation is not really about the data that is there when we when we say generalized it actually is uh able to answer questions or predict numbers or classify things

0:13:49 -  which are even outside the boundary of this but uh is what we were discussing the other day but I'm just repeating on uh that I don't want to go deep into it on generalization but AG is the next big big thing and um in in LinkedIn um anding has given a post uh which is um which is very nice I'll see if I can share it at the end of this video uh on AGI on generalization concept so coming to regularization let's talk of let's get into regularization and see how the regularization can be defined and what is regulation realization and what are differentation is technique used machine learning to prevent or fitting by adding a penality term to the models L function the term discourages over complex models by penalizing L parameter the go ofation is to encourage the model to generalize well to unseen data by uh data by balancing the trade off the balancing trade off between model complexity and fit to the training data there are two main types of regation L1 regularization uh [Applause] do we have regularization it's again going back to this the drop1 L1 L2 elastic both L1 and L2 penalties in the L function it okay that's again combinations of L1 L2 saying but Prim it says um L1 regation and al2 regularization okay so let's look at what is lasso and what is regation um they're very simple as I said what we are driving towards is um loss to be zero uh correct so if loss is zero that means our equation that is there which is predicting the patterns it is exactly predicting the Y values uh as they are given in the training data set correct and that is called overfitting and we want to avoid overfitting so we don't want the so one side we we train that the y y predicted values are as

0:16:28 -  close as possible to why we we try to calculate loss we try to reduce it and stuff other side what what we saying is we don't want it to be equal to why because we don't know what's the training data set and what is the unknown data set that is out there which could be millions or billions of Records considering that we don't want it to be equal okay but our our overall methodology our equations or our loss functions gradient distance everything is designed in a way how do we get to that zero value the L how how do we approach loss to be zero um either I mean technically when you use GL desent and stuff you may or may not have loss as you may not reach loss as zero because the the rcent approach itself take you to local optimals most of the times where you go to the least possible say and then you take a call whether you want to stay here or you want to still pursue go beyond that particular local optimal to reach um Global optimal so but most we don't know whether there is other Global optimal or not so we might say um as long as the accuracy of this particular local optimal is is good enough we might say that this is this is good enough for our model requirements that is been given by our customers or somebody and uh so we Mo move forward with it um um which is this is okay because again as you said we don't we are not trying to reach zero um and zero is definitely a global optimal can all models go take you towards zero may or may not be possible so consider that scenario um that scenar you you usually go for Global optimal loss that that will be the um that will be the um the best possible scenario for the mod um but if your loss could be made Zero by Y is equal y predicted is equal to Y values are there um because your equation of loss function is more on difference between Y and Y predicted correct so uh difference between Y and Y predicted in that L function once if I add

0:19:0 -  more uh value to the loss Beyond y predicted and Y difference what happens the L function will not become zero so um and this additional term that we add is also um dependent on the model not Y and Y predicted right so that gives us a Leverage on um on how the model also influences the law besides the by prediction and Y correct so that is how they have introduced regularization so they not only um make sure that there is certain loss basing on the by prediction and why they also make sure the loss is basing on the rates or parameters that we pick up um for our equation so if you see L1 regulation adds the sum of absolute values of the models coefficients coefficients is nothing but parameters or profs are nothing but the parameters that we take weights the parameters that we take in the equation uh to the loss function it encourages spacity in the model by driving some coefficients to exactly zero effectively performing feature selection the regulation uh term for alization is proportional to the sum of absolute values of the models coefficients so if it's a lasso regularization that is L1 prly called L1 everybody and in the data Science World they understand if you say L1 they understand what what are you talking about declaration uh so here it is nothing but what we do if you have X1 X2 X3 xn we will have W W1 W2 w all W1 so um um and these weights are used to multiply the X1 X2 X3 so even if you have some feature which is derived from this we consider them as another feature for example X1 Square we consider as another feature X1 into X2 into X3 is another feature so all them being included W1 W2 will cover all of them so uh W1 to W1 are the weights that we use for the model and which will actually help us in um create that equation which represents the pattern which is called which we

0:21:31 -  call as a model so these some of the w1s if they are taken um those actually represent uh the overall Models Behavior and those some of them model some of the weights can be used as the regularization uh uh regularization additional process weight that we add want to add to the loss function so we also actually multiply this with the regularization parameter called Lambda there and this Lambda also influences the way the loss functions output is derived and and picking up this Lambda value also makes a difference in how much uh how much what are like you should look at like what percentage of sum of weight should be applied to a model to the loss function so that during training how much it will influence the overall loss like let's say you say Lambda is 100,000 then what happens is the the the weights uh the sum of the weights uh influence the overall loss to a major extent so it will push you from uh optimizing your model more and more right because this is makes this as a big term the Big Value 100,000 into the Lambda sum and basing on that value the some of these values the overall loss becomes more once the loss is more you you you increase your iterations until you reach the optimal value correct so um basing on the Lambda value we can tweak on how much it will influence to W's and yeah it's there it's coming it's there behind but what no no don't disturb me once I'm done we'll talk about it okay thank you so uh that's Lambda and that's sum of w on okay that's lasso regation um I I'll at the end I'll just askp to put it into an equation and then show us so now come to Ridge regularization L2 regularization um here think it's another variant of the regularization that's all the the um so the weights um in lasso are considered as W

0:24:1 -  W1 W2 W3 and we do a sum of it whereas in Ridge regularization we do a sum of squared values of this model so that whenever you do square of something it is more on uh it is to amplify the magnitude right because um if you if you look at Square terms what will happen um as the value grows uh it will increase the value higher side and the value is lower it will decrease the value for example 0.1 um Square will become 0.01 so that is the weight is already minimal the impact of it the the impact or contribution to the loss will become 0.01 uh 0.04 will become 0.16 correct so like that if the the the weights are very low the Lo the impact on loss will be very low whereas the weights are very high then for example if the weights are something like 25 uh the the loss will become 625 whereas the five is 25 so the higher the higher the weights uh so the higher the weights the higher they contribute towards the output value because uh the the w i if it's higher wi into XI is how it should be and wixi will be obviously higher right wi W when it is higher even though respect of X um what is wi wi is the weightage of XI and what it is doing is when wi is higher that means you are saying the XI feature is so important compared to the remaining features that are there in the data set that it influences the output so much so that's reason W should be higher that's what we're trying to say when there are XIs which are influencing your overall output and the remaining features are not being values are not being considered much then in the loss function we we add either wi or W sare um and uh saying the the loss is more so what happens when you when you try to derive the next W1 what we do we do w i um new will be equal to w i old minus you you do do derivative of your loss function right cost function so what happens is you you you you reduce the W

0:26:35 -  basing on your loss or cost function so so that's how you see that the W is reduced basing on your loss in which loss we are adding wi square that means if wi is older value of wi is greater that your loss is higher and you reduce it further so you don't have that kind of a scenario where specific feature is actually influencing your output so much it might be good it might have higher value but at the same time we want to make sure that your model is not going in a Direction saying basing on one feature it is trying to predict everything right because that in the training data set it might have a correlation strong correlation between a given feature and the output but in the real world it may or may not have correlation and if it is really having that level of a correlation people would not um pick up all the features the domain experts will say can I understand that okay this feature actually has stronger correlation and we need not rely on so many features we'll have this feature plus two or three other features and then we'll build a model right so um again that's reason or fitting is where W's that we use in the equation to multiply X W1 X1 W2 X2 W3 X3 so on W and xn W's will be uh reduced in a way that it it doesn't go to an extent where it will overfit the particular equation to the required y that's the that's the circle um so data is given you build an equation to build an equation you you assume certain W1 W3 basing on that equation you calculate loss and cost um you influence the loss and cost basing on y prediction Y and also W's which are unknowns y w is the unknowns you put them there and then you use this particular loss CA functions derivation to reduce to derive your corresponding W new is W new I is equal to w i old minus derivation of this cost function that's what we have seen the previous equation so that in turn reduces W and you you go back to Circle you again build the equation you again do by prediction again do loss cost again you derive so this this iteration will keep going until you reach an optimal State

0:29:29 -  um that's how it U works so that's the reason regularization is nothing but including the weights either wi Su of wi or some of wi squares as an l regation and that would actually influence that would increase the loss function and um increasing the loss function function in turn reduces your W's properly and also increases the number of iterations the loss is higher you increase the iterations that your function goes through and um um we should see how it works so another approach that people have opted for basing on the uh need is um it's more it's more like it's more experimental right that would have used um regation that is L1 somebody would have reached L2 R regularization and it might not have effectively worked on them because of for some reason so they would have tried a combination of L1 plus L2 that is ADD both wi plus W2 square like as if we use WX plus C and then that has given us a straight line and that is not fitting in then we said okay why not WX W 1 x¬≤ + w2x + C so similarly here what they're doing they're doing L1 plus L2 which is nothing but Sigma W2 s s and they'll have Lambda multiplying this overall on as elastic net regularization this is combination of controlled by two hyper parameters so I said we'll multiply one Lambda but it it's not one lamb we used two regation term separately for both of them so it's like Alpha and one ratio I don't know what one ratio is okay anyway let's ask to give us give me equations for L1 L2 and elastic regularizations Alpha is overall regation parameter p is the mixing parameter that determines the ratio between L1 and L2 that means maybe the L1 is working good for few and L2 is working good for good for few so now what they're doing is they're picking up a mixing parameter which actually how much of w should be consider how much of w should be consider like 20% of w or

0:33:3 -  Sigma W and and 80% of W2 squ so is being considered so p is a piing so Alpha and P are picked for elastic nalization um okay I'll show you more add these to how go show all the so as you see to the given law function they are this is l regation where plus Lambda sum of Weights is added and um that means this is the equation right 1X 2 m into m is number of uh data set size of the data set and we sum of the different squares of it's in summary mean square eror mean square error is what we do uh plus Lambda of some of the Ws is what where we calculate the L1 declaration um and coming to L2 it will be um it will be the like same equation but plus Lambda into sum of W2 squ that's how the loss is calculated for al2 regularization whereas elastic net if you look at it it say original loss plus Alpha into this equation that is there that means 1X 2 m sum of plus this equation that is there and oh sorry I I I missed on the derivation so what happens deration what we using in the W new calculation type the weights are being changed in the Grain and stuff so we derive and then to calculate the new weights we we old weights minus the derivative of the cost function is what we have been using right so uh so in derivation if you look at it in the derivation if you when you uh when you do derivation of it basing on wi um this loss function now what happens previous it it is calculating the calculating the derivative of this function production function loss function but now with this addition of this this derivation will also have Lambda into wi and um uh this will influence the overall value and uh and also the Lambda W Square this should [Music] this is square of wi sare deration is 2

0:36:10 -  W that makes sense but whereas for Sigma sum of w it says Lambda into sin W I doubt this to some extent and then um here also says P deration of L1 regular one should be should not be oh because it has a mod value of x what it says is if D of X by X is a mod value it it depends on one and minus one so the values uh would be minus one or one uh cons that fact it says either it could be minus Lambda plus Lambda so that's what trying it's trying to say but it's not trying to say it's it's in the equation it says um s of w Lambda into not W so it says Lambda minus Lambda plus Lambda it depends on the value of w w is negative value it will be minus Lambda it is positive value lamb it's okay that's fine why that is important is especially if you go with the wi negative and positive value few of the parameters influence of the parameter right on the actual value if your W is positive that means there is a positive influence on the overall output whereas W is minus that is there it's a negative impact on the overall value that is coming so we find tune the particular W's according to the values that are there and this this gu is correct when he says he either uses minus Lambda plus Lambda whereas in um um in [Music] in in lasso it's only plus or minus Lambda is the output of the deration that influences the the new W whereas the rization it is two multiplied by um here this one so it is 2 multiplied by Lambda and wi so w term is also part of the uh derivation and it reduces the thing whereas in in elastic net regularization as it has a combination of both of them it will become Alpha