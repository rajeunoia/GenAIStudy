# Day 37 Study Time Gen AI

# Day ?: Study Time Gen AI  
**Time Interval:** 00:00 - 37:25  
**Summary**
- **üîç Overview**: The session continues exploring the topic of **embedding verification**, focusing on the challenges and progress made in understanding this complex area.
- **‚è≥ Slow Process Acknowledgment**: The instructor acknowledges the slow nature of the verification process but emphasizes the importance of grasping the details thoroughly.
- **üìö Previous Concepts**: Previous discussions included removing stop words and utilizing a sliding window technique for selecting context words based on a defined window size.
- **üîÑ Negative Sampling Technique**: The concept of **negative sampling** is introduced, where words not present in the context are also considered, enhancing the training process.
- **üî¢ Embedding Techniques**: The session touches on different embedding techniques (e.g., Word2Vec, ELMo) that help in representing words in a numerical format while also capturing contextual meanings.
- **üß† Feature Extraction**: The instructor discusses how embedding helps in feature extraction similar to processes seen in image analysis, thereby illustrating its relevance in natural language processing.
- **üîÑ Iterative Updates**: It is noted that embeddings change with each iteration of training, raising questions about how models retain and understand word similarities despite these changes.
- **üîç Debugging Process**: There is a focus on debugging embedding processes, analyzing the initial values, and the impact of adjustments on the embedding's quality.
- **üî¨ Understanding Errors**: The session concludes with discussions about calculating similarity scores and the ongoing optimization of the embedding model, setting the stage for future exploration of model efficiency and accuracy improvements.

# Transcript 


0:3:21 -  yeah hi um so today we'll continue continue the verification that we doing with respect to embedding it's taking a long time I understand but um and it is going slow I should actually speed up so I'll try to make it as fast as possible but I really want to understand it in detail before so yesterday we looked at this um code these are the uh text he has taken some text training text and in the training text there he's actually removing certain stop words um to start off text is Tak a Windows S3 that means um you should Che it as a slider of three three World length and it will keep moving down so you keep a slider you pick up the third word move the slider to the third word and pick up the bottom two words so the two words above and below will not will want be picked up if you if your window size is three if your window size is four then three words they starting and three words after them excuse me for having my breakfast in par I really feel hungry for some today for some reason is not good okay so and besides that we also added the negative sampling technique so instead of saying that these are the words that are there in the context we also say these are the words which are not there in the context how connected samples is from the whole text randomly he's picking three samples but making sure those three samples are not part of either the center word or the context words when I say Center word and context word if you look at this flow window size 3 1 2 3 fundamentals is the center word today learning will be context words besides fundamentals data science so these four words will be context words fundamentals will be Center word and then you move to data that will become Center word learning

0:6:26 -  fundamentals will be context words science and statistics will be context world that's how the window keeps moving with respect to Center word and context words now negative words are random words within the sample for example fundamentals today learning data science are context as we have seen and for for each word that we pick up in context there he's picking up three words in negative sampling which are not in context kind of stuff so but when today is being picked up he picked up better artificial expert which is not part of the context and Lear he picked up he picked up really hot really this is not part of the god text we should have avoided the reputation of words here so that it will be much more efficient because predicting same negative word twice doesn't that much what's value so till the negative sampling is how do I see this chat and okay I should get [Music] notification sir what exactly you're learning can you give me a quick summary okay um so what we looking at is embedding till I mean till now what videos we have seen we have we looked at all different steps um as you know NE networks are RNN are Transformers and coders whatever doesn't understand words directly we should EMB them so ideal encoding ideal embedding is simply one hot encoding where you have a list of words and you you keep one at corresponding word kind of stuff right that's how you do encoding but that doesn't really help in the NLP models so they go with the embedding Al I'm trying to understand how the embedding works so one of the there are different embeding techniques like word to fast text um Elmo and stuff so different embedding brings different words embedding not only brings not only helps in representing a word with a number it also actually brings in a context and features like in CNN when there is an image for each

0:9:13 -  part of the image we extract certain features and then understand what it is similarly um for for even words they are extracting features which is very interesting so um how are they extracting features we don't know similar to images right we don't know how they are understanding that a image within a section they understand that it is actually here it is I and stuff right so um the neur network by looking at multiple images they draw patterns and understand things so similarly here they draw patterns and understand word meaning but um majority of the meaning is being given by embeddings so that's the reason embeddings is very important um I'm trying to dig deep into embeding at word work to see how it is actually extracting those features out of the words and representing in a numeric format it's a vector like each word is a set of words set of numbers um the the beautiful part of it we'll see is the words are words get closure how can we say that it's extracting a meaningful context is we know English words and if you look at a group of words they are actually close to each other from the vector perspective that itself says that it is really understanding the word it's not randomly giving you some numbers so how is that happening is my point in exam for somebody who's listening may or may not be necessary for you guys but it's it's actually my personal interest I'm digging deep into it okay so this piece we can we understood the code pie part of it so we just preparing a data in this data actually in turn he has um this also we have understood yesterday where um the context word and Center word if there are any intersection between these two words um with respect to repetion and stuff those words are being removed from the list but in this context there not many are [Music] repeated if you see that around 983 being 984 out of which 983 being are being ConEd just one one row is removed considering it as a duplicate that's all nothing nothing

0:14:22 -  more me some this is this code is helping duplicates as now moving on uh yeah good he he's looking at no no this is my code I was just looking at the unique um unique words in this um output uh so the final data frame that he prepared comes out like this where we have the center word and the context word and the label shows whether it's the right context or not for example fundamentals and learning it says label is one if you look at fundamentals here the learning is here so for fundamentals today learning data science we have one here so yeah I mean let's just quickly look at it insert of um DF of I'm just trying to verify what is I'm cross verifying that the labels are given [Music] properly so for fundamentals I'm I'm verifying on what is all the words that are captured it should be 3 into 4 12 words and uh four should have ones oh okay um there is one more part to the story right so it could um there could be a repetition of fundamentals within the text so there could be more as well posibility so fundamentals had 15 words I don't know man I don't understand everything but uh okay that's fine so there are 15 words he is picking up in which um learning is one data one science one learning data science what happened to today interesting the remote today [Music] m [Music] is it because um today itself is not that um cont what is um what to toally removed it's totally removed that's interesting why is today removed this one be Sig V comm sare sigat function on by one plus

0:22:43 -  exponent ofus s okay this is a complex thing to understand update embedding data frame main embeddings context embeddings running m got differences between main first of all he needs to explain what is main will come to the m again normalized data plot words will come when he calls it will come so now he say eming size is five so say embering size five that means he's saying um he'll use um you'll use five numbers to represent each word that's what he meant um embedding size by it's mostly like um it's mostly like a vector of size five that's what he'll use is so coming to work to back mhm m for for for okay thought this will be fine L after words is five no l words is um [Music] um some 300 or something okay L of the words is some number and um embedding size is five so row underscore Norms we should print all this and see okay so now first let's debug this and B SL square root of um embeding square plus Square some shus same he's doing with containg [Music] also index words so initially by default he's he's giving a it's giving some values so let's just look at the initial [Music] values so this main embeddings initially are like this then draw normalization you're normalizing those values you're dividing them with um this normal [Music] value just it's just like normalization where you're normalizing the values that's all for each five size this is

0:29:2 -  the May numbering this the context this is something interesting to understand right so maning and context eming are being picked up as two different Val right same world [Music] another Vector is context amping this is question to get to to be clarified and post that he just running a loop for 25 times and with this Main aming and containg are being updated okay so we run this um this is another thing every time you run it the embeddings are changing so when you train a [Music] model makes sense because we didn't run the other so we'll go to update eming data frame main embering context embering 0.1 something is passing I don't know let's look at the me updateing dat frame maining context learning rate 0.1 debug equal to false this is good is where is it there is here um here is sending learning rate comma or debug data and science is what he's passing if you don't pass anything it will be false otherwise debug is equal to those two words okay okay so context numbering so this two we he's taken values of both of them in one place and then he's doing a difference of those both values this okay done get similarity scores and errors between main embeding only context embeding do product product of this two Sigma of product this is error okay got it calculate updates updates is differences into errors into learning rate okay that updates for again there so it's a loop in with this he he's looking at the error in between similarity scores and errors

0:37:53 -  scores is by Sig mod so error is sigo values to actual sigo values gives certain um okay let's look at that for [Music] I give space that that the message again oh he's using debuk as a variable here and we are using it as a as a method okay okay now let's see this is the embeddings and um next right song Hey I def find it oh might is on top is it if you go here let's look at the debug first is is I had main embedding context embedding and then do product okay s mod of do product this I think dot product what PN so dot product output so these are these two are the uh let's see what is the difference in between this let's look at it again these are just the part do values do values um only Center word he's taking only context word he's taking this is only centeral words only context words but if you look at U this this is Center word 3 30 4404 [Music] 26983 but here are different and they're being repeated also did we do anything after printing this [Music] I should look at it more into it okay this is dot product output these are scores is scores is nothing but Sigma applied add on them which says which is a one and which is not a [Music] one yeah

0:44:19 -  is that okay [Music] so if I scroll on all the way [Music] down let's use the bar and do it 25 times it will be so the errors got minimized but they're not that low minus 0.3 0.2 0.6 POs to 0.6 are all they high only still they're not that close [Music] errors so these are all the errors they're trying to reduce the overall what is this and what is this I should look at the graph plot graph so now for one each word and corresponding other word he is trying to show us what is the main amings word one and word two okay so main embeddings is the final embeddings of each of them cosine is to look at the correlation between both both of them how much close they are like data and do is 0.5 they far but whereas the remaining are very close that's what it's trying to [Music] say in St of data say look at [Music] fundamentals as we have printed fundamentals I need to go [Music] long send um data and [Music] signs today learning by fundamentals and learning today they have REM that I've seen learning or listening to see so closest to fundamentals is stock become became and um let's say science listening thans long hot good [Music] talk so this is five five dimension Vector we we using PCA we shrinking it down to two two Dimension Vector plot figure size figure size is equal 5x 5 Z One [Music] colum 0 less than minus 25 I don't know why this guy did this this is like studying is coming here