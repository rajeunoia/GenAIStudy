# Day 44 Study Time Gen AI

**Day 5: Study Time Gen AI**  
**Time Interval:** 00:00 - 37:25  
**Summary**  
- **üîç Overview**: The session reviews previously covered embedding techniques, including Word2Vec, GloVe, and Elmo, and transitions into BERT, the last embedding technique discussed.  
- **üìä BERT Introduction**: The speaker explains the significance of BERT as a bidirectional encoder representation from transformers, highlighting its foundational role in understanding language through its transformer architecture.  
- **üß† Understanding Transformers**: A comparative analysis of BERT with LSTM networks illustrates BERT's efficiency in processing data concurrently and capturing better contextual relationships between words.  
- **üõ† Input and Output Mechanisms**: The principles of how BERT processes input through token embeddings, segment embeddings, and position embeddings are detailed, explaining how this leads to producing contextualized word representations.  
- **üéØ Pre-training and Fine-tuning Phases**: The session discusses pre-training with masked language modeling and next sentence prediction; thereafter, it details the fine-tuning process for specific NLP tasks like question answering.  
- **üöÄ Training Efficiency**: The training is efficient, allowing for quick adaptations based on tasks while retaining a foundational understanding of language context.  
- **üìö Practical Examples**: Examples illustrate how inputs are structured for BERT and how outputs are generated, emphasizing the model's versatility across various NLP applications.  
- **üîú Future Directions**: The instructor hints at future sessions focusing on the transformer architecture and planned studies around enhancing embeddings capabilities.

# Transcript 


0:10:2 -  yeah hi everyone so um we have um looked at um Almo um so the last embedding that is uh left is but um we've covered um word to fast text glove um Elmo and now we are going with bir um these are the um most widely used uh embedding techniques um B I think is uh is more an encoder technique so preferably it is good if he uh could look at it after looking at um the um uh Transformer architecture and uh attention mechanism and stuff but uh I'll go through it now quickly and uh post that as well I'll I'll look at uh B on how it works and stuff okay so with that being uh said I should have hooked up a bird beforehand but let me get started with bird here minimize this that before going to bir I just thought of um doing a bir being the last one I thought doing a quick um study on comparison between the uh the embedding text embedding models I I'll read through it form text allows computers to process analyze and interpret human language efficiency in this article dels into the concept Ali models and applications of texing semantic and syntactic ense of words Pres but ping circulation for [Music] okay associations [Music] grossen for I'll do for for okay cool let's go to bir is f [Music] today we're going to talk about Bert so let's jump into it this is the Transformer neural network architecture that was initially created to solve the problem of language translation this was very welled until this point lstm networks had been used to solve this problem but they had

0:12:32 -  a few problems themselves LM networks are slow to train words are passed in sequentially and are generated sequentially it can take a significant number of time steps for the neural net to learn and it's not really the best at capturing the true meaning of words yes even by directional lstms because even here they are technically learning left to right and right to left context separately and then concatenating them so the true context is slightly lost but the Transformer architecture addresses some of these concerns first they are faster as words can be processed simultaneously second the context of words is better learned as they can learn context from both directions simultaneously so for now let's see the transformer in action say we want to train this architecture to convert English to French the Transformer consists of two key components an encoder and a decoder the encoder takes the English words simultaneously and it generates embeddings for every word simultaneously these embeddings are vectors that encapsulate the meaning of the word similar words have closer numbers in their vectors the decoder takes these embeddings from the encoder and the previously generated words of the translated French sentence and then it uses them to generate the next French word and we keep generating the French translation one word at a time until the end of sentence is reached what makes this conceptually so much more appealing than some lstm cell is that we can physically see a separation in tasks the encoder learns what is English what is grammar and more importantly what is context the decoder learns how do English words relate to French words both of these even separately have some underlying understanding of language and it's because of this understanding that we can pick apart this architecture and build systems that understand language we stack the decoders and we get the GPT Transformer architecture conversely if we stack just the encoders we get Bert a bidirectional encoder representation from Transformer which is exactly what it is the OG Transformer has language

0:15:4 -  translation unlock but we can use Berke to learn language translation question answering sentiment analysis text summarization and many more tasks turns out all of these problems require the understanding of language so we can train Bert to understand language and then fine-tune bird depending on the problem we want to solve as such the training of Bert is done in two phases the first phase is pre-training where the model understands what is language and context and the second phase is fine-tuning where the model learns I know language but how do I solve this problem from here we'll go through pre-training and fine-tuning starting at the highest level and then delving further and further into details after every pass so let's go deeper into each phase so pre-training the goal of pre-training is to make Bert learn what is language and what is context Bert learns Language by training on two unsupervised tasks simultaneously they are mass language modeling and next sentence prediction for Mass language modeling Bert takes in a sentence with random words filled with masks the goal is to Output these masks tokens and this is kind of like fill in the blanks it helps Bert understand bidirectional context within a sentence in the case of next sentence prediction Bert takes in two sentences and it determines if the second sentence actually follows the first in kind of what is like a binary classification problem this helps Bert understand context across different sentences themselves and using both of these together Bert gets a good understanding of language great so that's pre-training now the fine tuning phase so we can now further train Bert on very specific NLP tasks for example let's take question answering all we need to do is replace the fully connected output layers of the network with a fresh set of output layers that can basically output the answer to the question we want then we can perform supervised training using a question answering data set it won't take long since it's only the

0:17:36 -  output parameters that are learned from scratch the rest of the model parameters are just slightly fine-tuned and as a result training time is fast and we can do this for any NLP problem that is replace the output layers and then train with a specific data set okay so that's pass one of the explanation on pre training and fine-tuning let's go on to pass two with some more details during Berke pre-training we train on mass language modeling and next sentence prediction in practice both of these problems are trained simultaneously the input is a set of two sentences with some of the words being masked each token is a word and we convert each of these words into embeddings using pre-trained embeddings this provides a good starting point for Bert to work with now on the output side C is the binary output for the next sentence prediction so it would output one if sentence B follows sentence a in context and zero if sentence B doesn't follow sentence a each of the t's here are word vectors that correspond to the outputs for the mass language model problem so the number of word vectors that we input is the same as the number of of word vectors that we output now on the fine-tuning phase though if we wanted to perform question answering we would train the model by modifying the inputs and the output layer we pass in the question followed by a passage containing the answer as inputs and in the output layer zero if sentence B doesn't follow sentence prediction in practice both of these problems are trained simultaneously the input is a set of two sentences with some of the words being masked each token is a word and we convert each of these words into embeddings using pre-trained embeddings this provides a good starting point for Bert to work with now on the output side C is the binary output for the next sentence prediction so it would output one if sentence B follows sentence a in context and zero if sentence B doesn't follow sentence a each of the t's here are word vectors that correspond to the outputs for the mass language model problem so the number of word vectors that we input is the same as the number

0:20:9 -  of word vectors that we output now on the fine-tuning phase though if we wanted to perform question answering we would train the model by modifying the inputs and the output layer we pass in the question followed by a passage containing the answer as inputs and in the output layer we would output the start and the end words that encapsulate the answer assuming that the answer is within the same span of text now that's pass two of the explanation now for pass three where we dive further into detail USAA for homeowners being ready that's why we have safety tips for things like fires this is going to be fun on the input side how are we going to generate these embeddings from the word token inputs well the initial embedding is constructed from three vectors the token embeddings are the pre-trained embeddings the main paper uses word piece embeddings that have a vocabulary of 30,000 tokens the segment embeddings is basically the sentence number that is encoded into a vector and the position embeddings is the position of a word within that sentence that is encoded into a vector adding these three vectors together we get an embedding Vector that we use as input to Bert the segment and position embeddings are required for temporal ordering since all these vectors are fed in simultaneously into Bert and language models need this ordering preserved cool the input is starting to piece together pretty well let's go to the output side now the output is a binary value that the answer is within the same span of text now that's pass two of the explanation now for pass three where we dive further into details this is going to be fun on the input side how are we going to generate these embeddings from the word token inputs well the initial embedding is constructed from three vectors the token embeddings are the pre-trained embeddings the main paper uses word piece embeddings that have a vocabulary of 30,000 tokens the segment embeddings is basically the sentence number that is encoded into a vector and the position embeddings is the position of a word within that sentence that is encoded into a vector adding these three vectors together we get an embedding Vector that

0:22:42 -  we use as input to Bert the segment and position embeddings are required for temporal ordering since all these vectors are fed in simultaneously into Bert and language models need this ordering preserved cool the input is starting to piece together pretty well let's go to the output side now the output is a binary value c and a bunch of word vectors but with training we need to minimize a loss so two key details to note here all of these word vectors have the same size and all of these word vectors are generated simultaneously we need to take each word Vector pass it into a fully connected layered output with the same number of neurons equal to the number of tokens in the vocabulary so that would be an output layer corresponding to 30,000 neurons in this case and we would apply a softmax activation this way we would convert a word Vector to a distribution and the actual label of this distribution would be a one hot encoded Vector for the actual word and so we compare these two distributions and then train the network using the cross entropy loss but note that the output has all the words even though those inputs weren't masked at all the loss though only considers the prediction of the masked words and it ignores all the other words that are output by the network this is done to ensure that more focus is given to predicting these Mass values so that it gets them correct and it increases context awareness so that was the three passes of explaining the pre-training and fine-tuning of ber so let's put this all together we pre-train Bert with mass language modeling and next sentence prediction for every word we get the token embedding from the pre-trained word piece embeddings add the position and segment embeddings to account for the ordering of the inputs these are then passed into Bert which under the hood is a stack of Transformer encoders and it outputs a bunch of word vectors for Mass language modeling and a binary value for an ex sentence prediction the word vectors are then converted into a distribution to train using cross entropy loss once training is complete Bert has some notion of language it's a language

0:29:37 -  model the next step is the fine-tuning phase where we perform a supervised training depending on the task we want to solve and this should happen fast in fact The Bert Squad that is the Stanford question and answer model only takes about 30 minutes to tune from a language model for a 91% performance of course performance depends on how big we want Bert to be now the Bert large model which has 340 million parameters can achieve way higher accuracies than the bird Bas model which only has 110 parameters there's so much more to address about the internals of Bert that I can go on forever but for now I hope this explanation was good to get you an idea what Bert really does under the hood for more details on the Transformer neural network architecture which is the foundations of Bert itself click on this video subscribe and stay safe a lot more content coming your way soon and I'll see you soon byebye okay now let's um look at Bert with an example that is for come good okay I think this explains but properly let's look at it and understand they it with example good um the special so but we'll take two one or two sentences SE token to differentiate them always at the start of the text then is specific to classification TK okay always required only have one sentence about for classification that's how expect to see okay so the Els token and and separator token are mandatory for the inputed so the standard format is CLS to sentence one separator sentence two sentence two is optional that's what it says sentence one is inut tokenization and word word eming next let's take a look at how we convert the words into representations word emings for take this toize it sent is the sentence I want notice how the words Ting is represented Ming has been SP small of words characters this is because bird vocabulary is fixed with the size of 30k tokens words that are ofab represented as sub words and

0:35:7 -  characters this I didn't understand oh this is output toen so the tokenizer tokenizes them and in case if if there is a word that is not part of the tokenizers 30k words it will break it down into smaller pieces is what it is saying is the sentence Bings not part ofab represented as sub words and characters it takes the input sentence and will decide to keep every word as a whole word the sub words with special representation of the first sub [Music] example word characters okay now now um here on we'll use the next two instance of so we have toiz it that is also correct so B is trained on and expect sentence B using ones and zeros let distinguish between the two sentences that is text we must specify which sentence it belongs to so zeros one Mark other WR sentence one segment is one so segment ID is is classification on uh whether it the first sentence or second first sentence is marked with the zero and uh second sentence is um marked with one um in this context as he as he has only one sentence they picked up one sentence with one class token and one separator token they are going to use all ones so token tensor has index tokens and segment tensor has segment IDs token denor is this one index Tok and segment IDs extracting embedding next we need to convert our data to input format and call the bir model we ignore how to create anwers here you can transform Library example we use a separate our mode as opposed to training mode stands [Music]

0:44:33 -  off we have bought the model which is already pre-trained model we have taking it next weate B example text that run the text through bir and collect all the all of the Hidden States produced okay one second one second I missed on the extracting next we need to convert our data to tensors input format for the model and call the B model we are ignoring details of how to create tenses here okay um example will uses a pre-trained model and sets it up in a mode as opposing to training mode which turns of drop out regularization okay tokens different the third item will be the hidden St has four dimensions fallowing the number 13 layers the first El number 13 number of pches one number of to 72 okay for for token okay that 6 1019 right yeah 6 19 between 106 and [Music] 1019 so same bank 94% 69% use for what is the best contextualized embeddings for help in their context is to createing for using bir and experience a different approaches to themed with different approaches to coming these emings and shared [Music] some for okay good so that's it with the bird for today so but is simple but has um two two approaches of um I mean see if you see what to Glow fast text um Elmo but all of these they are actually building models which solve certain problem and while they're solving certain problem they are actually figuring out

0:47:20 -  in the solution of the problem the output is being not exactly mapped as the embeddings but um solve when you are solving the problem the out the byproduct kind of a byproduct is what is your embeddings um like in Elmo the bidirectionality bals are used and the outputs of this balms um H like hi I HJ um and also the with certain weights and also some token together these are kept into an equation and then those that equations output is used as embedding similarly in B what they have done is they trying to solve they have taken two problems um that is uh uh one is mass sentence model I think uh let's look at the names properly MSM Mass sentence and next sentence prediction s p contr F yeah MLM mask language model sorry next sentence prediction NSP is correct MLM is mask language model so M language model and um next sentence production so what they're doing is they're taking a sentence they're taking two sentences matter of fact so and um they are masking certain words that they're removing the certain words within the two sentences and they're trying to predict those two sentences first thing second thing is they're also trying to see uh uh is the first sentence second s next to each other kind of stuff right like an iing sentences that's more understanding of so I think the MLM majorly contributes towards a word prediction and the NSP prediction helps in the context uh prediction at sentence level kind of stuff so the output will be first is a token which will be um uh a binary prediction saying these two sentences are next sentences or not it will be a zero and one and then next will be a list of tokens uh which will also actually uh predict the master words kind of stuff so that's those are the two problems they trying to solve using

0:49:59 -  the um um by by Direction encoder uh Transformers here um only the encoder piece part of the Transformers is being used as um models um what it also is doing is um it is uh tokenization it has a tokenization mechanism of its own which is covering some 30,000 words and uh uh basing on 30,000 word tokens it is is actually initially tokenizing like initially to to give input you should tokenize right so those tokenize tokenization is used and also is passing the segment IDs which shows whether it is a first sentence or a second sentence basing on that um um it is uh like the first sentence is represented by zero second is one if there is only one sentence it's represented by one kind of stuff so um so the segment IDs and the uh to toiz tokens of these words okay in tokenization if there is a there as it is a limited 30k set and if there is a word which is not part of the 30k then and it is broken down into smaller words and those combinations are used so that's one thing so that it is um if there is an unknown word which is not part of 3K that is also addressed so that's one logic and um um and these details of tokenization and the segment IDs they are passed to the model and uh the model predicts um the output [Music] um one thing that I didn't see in this blog that is there is uh uh masking so the expectation is to mask but I think the B model itself takes the tokens and uh and introduce some masking and prediction in in in the process so there are multiple layers that are used you know to uh do the production the the encoders are repeated in in multiple layers uh we'll understand more about the encoders decoders and in the Transformer architecture when we go to the Transformer architecture and stuff so there are there are multiple layers um output is given multiple layers so there are approaches on which layers output should be considered as embeddings right so um there are

0:52:44 -  different approaches taken but um the best of the uh um so best of it is the the last um first second to last hidden layer is giving actually the best output kind of stuff that's what is the end thing so so I mean like each layer will give some output and that output is passed to the next layer right so they said the the finding when they they Tred different combinations of outputs is second to last layer is is the one which is giving the better output um with respect to embedding that is the finding so that's reason I think as a PR best practice uh um I but I'm not sure if this um choice of which layers output should be considered as embedding is um um is is like a hyper parameter where we make a choice of it or not in the bird training we I should look at it but that's one of the things that is there um yeah why he says that he has chosen second to last layer is the the last layer especially is focused on predicting the um the next sentence prediction the class token and the it will also focused on the masked words prediction kind of stuff right so um but our primary target is that that's actually a that's actually a problem statement that we trying to solve for um language understanding and context understanding but the primary focus is to have the embeddings of um of are are the the embedding tokens of representations of each of the words and sentences is what our Target is so that is being predicted word embeddings can be taken from layers but overall sentence embedding how it is taken is something which I didn't understand I'll I'll see if I can understand more but this is how the but embedding is working um it's clear uh to some extent I mean to the depth that is I would not agree it is clear because I think after going to the transform architecture we should come back again here and and understand how things are working in in much more detail but for now it is good so that we can actually say that we understand B