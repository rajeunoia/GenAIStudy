# Day 23 Study Time Gen AI

**Session Summary: Exploring Neural Networks and Optimization Techniques**

**Time Interval:** 00:00 - 42:10

- **üîç Overview**: The session begins with a discussion on neural networks, highlighting their similarities to the human brain. The main focus is on explaining the concept of networks, the corresponding equations, and how these mathematical constructs effectively model complex problems.

- **üìê Neural Network Equations**: The instructor outlines the equations integral to neural networks, including discussions on layers, inputs, and outputs. The concept of hyperparameters and their tuning is also emphasized.

- **üß† Optimizers and Regularization**: Various optimization techniques for neural networks are introduced, along with explanations of regularization methods to prevent overfitting. Hyperparameter tuning approaches for improving model performance are discussed too.

- **üõ† Activation Functions**: The session delves into different activation functions used within neural networks, including the sigmoid and rectified linear unit (ReLU). Each function is explained in detail, highlighting their mathematical formulations and applications in the context of neural networks.

- **üéØ Gradient Descent and Its Variants**: The instructor explains gradient descent, emphasizing its vital role in optimizing neural network parameters. Variants such as Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent, and Adaptive learning rates are discussed, focusing on their benefits and limitations.

- **üöÄ Next Steps**: The session concludes with a brief mention of regularization mechanisms, promising further exploration of these and their importance in neural networks in subsequent lessons. 

Overall, the session provides a comprehensive overview of neural networks, their mathematical foundations, the role of activation functions, and various approaches to optimization.

# Transcript 


0:3:44 -  hey hi as we looked at neural networks and we have compared it with brain and how there is similarity between brain and neural networks but our the main intent was to explain uh how people have come up with a concept called networks and how really it uh bed out pretty well um and we also looked at the equations on how the equations would work uh but today we will go through the equations of neural networks um we'll also try to look at um different kind of optimizers for neural networks we will look at regularization how regularization is done here and U we'll also look at um uh hyper parameters that are there for neural network which hyper parameter tuning how do we do it and stuff so we'll just mostly we'll ask GPD and then we'll do how we want to do it I think that's it we we will decently cover neural networks if we have all these Concepts covered so that should do okay let's get started share my screen let's go Tob start asks sorry [Music] this let's this okay so let's let's look at the overall equations right so let's look at some simple neural network actually yeah so this playground is pretty interesting so usually we can actually see how the overall neural network would work and all that stuff so see we can play around let's say two and [Music] layers output is two then okay we should data classific classification noise bad size ratio of training to test data 50% okay B size 10 we'll look at all those this will be part of but I want to show you how these neural networks will look like so X1 X2 if there are two features if you have taken if you're giving it to two two neurons and um outputs expected are two outputs okay so we can also reduce one

0:7:42 -  uh and then we are classifying only the one that is there let's say let's take this scenario where we have 2 x uh 2 X1 X2 inputs two neurons between and this so let's try to look at this on what happens here right so let's go here and then see so I said each neuron has two parts one is um the linear regression equation Z and then there is an activation equation right so uh linear regression yeah so it's something like this right like okay so we take this [Music] term creation W1 W2 weights B1 X1 X2 features I think I can type it faster than this yeah so this is the equation right so D okay Z of Z so so it will be like this if if you take U activation function Z of Z will be like this so Z is W1 X1 W2 X2 + [Music] b c is equ Al to oops so Z is equal to this equation and um to that equation we apply sorry when I say let me set z z is equal to W X1 plus b and we apply Z of Z that is Max of 0 comma is re function this Z is activation function and activation function can be multiple so what are all the activation [Music] functions used in neural net so sigmoid function 1 by 1 + E power minus Z which is used in logistic regression hyperbolic tangent function is is tan H and um rectified linear [Music] um that is Max of 0 comma z uh leaky Rel is um where it doesn't really become zero

0:10:50 -  but we'll pick up a small Alpha in a way that um um so he's saying Z greater than Z is z okay so now we are talking about negative values that is z less than Z for Z less than 0 instead of making it zero um completely we will actually um multiply it with Alpha where uh Alpha is [Music] um um will help Alpha will help um making it making the value little more than zero so similar to but allows a small positive gradient for negative inputs helps alternative help Elevate the dying re problem where neurons can become inactive yeah so parametric [Music] reu similar to Leaky re but allows the slope of the negative part to be learned during training I don't know what is difference between leaky re and parametric re exponential linear unit but um as a Smo transition for input okay soft Max function So Soft Max function is E power Z of a specific value divided by sum of E power Z that is soft Max okay used in the output layer of multiclass classification problem outputs of probability distribution or k classes where some of the probabilties are equal to one oh is it so okay I didn't know that so if I want soft Max function if I have uh three classes if I do e of Z1 by E of Z1 plus e of Z2 E power Z2 plus E power Z3 then um yeah dividing by some of the particular numbers will actually give you the rough distributed value just thinking 23 4 2 by 9 3x9 and

0:15:22 -  4x9 will actually give you values which are distributed across these three three r is correct yeah so that's right so these are all the activation functions that are brought but I think the list is much more than this let's look at [Music] online St activation function inks see acation function softx sigmoid exponential function being repeated 13 more what is AC thing function that you use to get put off is also known as transfer function okay yes or no it Maps the resulting values in between 0 and 1 orus one to one depending upon the function two typ linear acation function nonlinear activation function okay function as you can see the function is line linear therefore the output of the function most used AC look like something like this easy for the model to generalize ad withy of data and sorry non on the Bas of their Sig ltic 1 by 1 eus Z we use Sig function is because it exists between 0 and 0 to one therefore it is especially used for models where we have to predict the probability as an output probability of the anything exist only between Z and one Sig is Right Choice okay my function can you more generaliz listic activation okay T is also like as logistic but better the range of the tan H function is from minus1 to 1 Sigma 0 to 1 -1 to 1 and it's as shaped okay inputs will be mapped strongly negative and the zero inputs isable theun m is not M main use classification between two classes okay activ in the world right now since it is used in almost all the okay okay but the issue is all the negative values become zero immediately which

0:18:16 -  decreases the ability of model to fit or train from the data properly that means any negative input given to the Rel activation function turns the value into zero immediately in the then affects the resting gra by not mapping the negative values appropriately okay leaky re it is to solve the dying re problem can you see the leak so the leak the leak helps to increase the range of relo function usually the value of a is 0.01 or so when a is not 0 .0 then it is called randomized R okay therefore the range of of leaky is minus infinity to Infinity both leaky and are monotonic in nature also the monic in curve to know in which direction and how much to change or update the curve depending upon the slope that is why we use different differentiation in almost every part of yeah in the backward propagation we do differentiation in order to see how the parameter needs to be adjusted so for that we need functions which are differentiable so that those gradi descent based calculations can be done effectively that's all so it's uh idty f of x is X binary step 0 or one and then um logistic 1 by 1 + e z tan H is 2 by 1 + e - 2 Z - 1 okay I something is wrong is it 2 - 1 E 2x by 1 E power - 2x is also possible divided by is uh tan HX function okay let's let's do it 1 2 - 1 1 1 - E power - 2x by 1 + E power - 2 yes that should do and if you bring it it's e^ 2x - 1 by e^ 2x + 1 that's easy know why to complicate it so much t h of X yeah um AR tan tan inverse of

0:21:40 -  X hey tag tan inverse isn't it C then oh okay let's see um rectified linear unit Ru 0x parametric rectified linear unit alpha x and x leaky re and parametric re is both are same as it okay exponent linear unit Al x - one everybody saying exception um soft plus logarithm of 1 + e^x we can look at the graphs so goian goian is something we have not covered in this goian will work like this okay me Gan will take negative as positive and positive as negative okay I think in this graph he has not shown leaky because if he shows leaky then there will be a line below below the negative axis so yeah it's only Sig Sig T Rel off plus and off plus is is going up remaining are going close to zero um postive reu also will go reu he has shown as if it is going towards zero but actually it is incremental Okay so so that's with the activation functions now um let's take sigmo function if you take sigmoid function as activation function then what happens [Music] um 1 by 1 + E power minus Z sigmoid this one right so if you take this one as activation function now what happen you take w W1 W2 I think I can't do it on this one I should take a and you guys can't see it I'm sorry but let me do it here for understanding purpose so if you have two neurons what happens one is um so activation function is 1 + 1 sigmoid if you take sigmoid as acation function 1 by 1 + E power minus z z is equal to W1 X1 + W2 X2 plus B1 but this we

0:24:15 -  have we have input X inputs two new one layer with two neurons and then output right so uh let's W's will be repeated each the three input will not have parameters will be there for three of these right two neurons here and output layer has one neuron so this will have to differentiate between them let's call it first second third neuron right so you'll make it 1 one W12 and uh b11 okay now what happens this is for first one second neuron below is again the and we call it z11 z11 so 1 + E power minus c um to one of fact it is easy Z1 is one one you not call it okay here it is two okay easy2 and uh Z2 is equal to w 21 X1 + w22 X2 + B21 right and then you have third one is 1+ e minus Z3 third one is nothing but our output l means if you look at this one this one okay um I mean I'm as he said usually we use a soft Max for output um this thing so that it will help in classification of uh the problem but um we can use softmax also not a big deal um but we have any anyways we have only one so we can actually have Sig mod Also let's go with for having uniform it will use so w31 X1 plus w32 X2 Plus b31 okay so these are three notes that we have used now what we are saying now third one is wrong because um this first neurons output we call it as A1 second neurons output let's say a11

0:27:30 -  second neurons output we'll call as a21 then in Z3 it will become w31 A1 Plus w32 a A2 because X1 X2 is inputs to two this two neurons the output of this neuron is A1 output of neuron is A2 A1 A2 will be fed into this neuron so this this neuron Z is calculated as w31 A1 w32 A2 plus b31 the B term for this particular neuron right so that is in calculator as 1 by 1 + E power minus Z minus Z3 right so first we'll put Z3 is equal to w31 what is A1 A1 is nothing but 1 + 1 E power minus Z1 + w32 1 by 1 + E power - Z2 plus b 31 right so w31 by 1 + 1 power e this if I constitute Z here 1 by 1 + W1 X1 + W2 w11 X1 + W12 X2 + b11 w32 into 1 by 1 + E power minus w21 X1 plus w22 X2 + p21 plus b32 b31 so Z3 if you look at it it is actually interal function of X1 X2 itself only thing is we are putting it into multiple so the X1 X2 that is coming here we are putting them into two different sigmoid functions and and that becomes 1 by 1 + E power Z1 1 by 1 + E power minus Z2 these 1 + 1 by 1 + Z1 is is being added here so if you look at it we are doing nothing but uh we are actually arriving at the final function which is a complex um or it's a it's a weighted output of two different

0:30:16 -  models let's imagine the two people who are standing here like Raja and sures so are rames and sures uh so Ramesh this is my model this is correct and um Sur says this is my model and there is a third model uh who who is like the final call whoever gives the final call what he does is he doesn't want to be biased he he doesn't want to take take anyone's Direction he feels that both of them are partially correct so what it does is he takes a weighted average that is okay I'll give W 31 weightage to the first one and W3 to wait is to second one I I'll take both of them so he takes both of them w31 w32 and he will add his own bias b31 a little bit to it so that will become this now if you look at this whole logic what happens when as you go incremental so like let's say you add one more layer like this um and bring one output now what happens A1 A2 is kept both here so it becomes um 1+ oh sorry so it it will become w31 W w31 one doesn't matter okay so W W1 W2 W3 W4 W5 let's things like that so W3 uh Sigma of W3 into a A1 A2 let's say this is not but A1 A2 [Music] um so Sigma W into W3 into A1 output of this and sigma W4 into A4 Plus B term bias let's assume that bias is zero for now but so go with it then um what happens is this will become a is nothing but 1 + E power minus Z1 Z2 here uh here will what will happen it will become 1 + 1 by minus Z3 Z4 but Z3 is equalent

0:33:15 -  to um sorry uh Z3 Z3 is equal to W1 uh sorry W3 a A1 output of this are to give you press w31 A1 w31 A2 and um that's what is 1 + E power minus Z and here it will become w41 A1 w42 A2 now these two if it goes here and then output if you consider A1 A2 A3 A4 this A3 A4 will be inputs to this so it will become W3 1 + um 1 + 1 by uh 1 by 1 + E power minus Z where Z4 say Z1 Z2 Z3 Z4 Z5 then Z Z5 is equal to Z5 is equal to W W1 W2 W W4 W5 W5 of 1 + E power minus C5 right uh no no so it will become this will become A3 A4 A3 is equalent to Z3 right and uh plus W yeah 51 w52 into 1 + E power minus Z 4 so multiples of this two right W weight is weightage sum of two outputs those outputs in turn if you look at it those outputs Z3 and Z4 are actually weighted inputs weed um sum of weighted outputs of these two so it's like a chain we are keeping it like one power e 1 1 by 1 + E power and then again W into 1 by 1 + E power so I'll put it here so it's becoming W into 1 by 1 + E power W into 1 by 1 + E power W and everything is minus minus W into 1 + 1 by E power so as as the number of

0:36:5 -  layers grow uh the equation will keep growing x + B plus okay let's remove the B term for now so so it's something like like this so it will go incremental as W 1 by 1 + E power minus w in this power term itself this is one term 1 by 1 + E power again so what happens is given WX weighted X parameter we are actually applying the sigmoid multiple times with different weights this is W5 W4 W3 W2 so like that it in this term itself you'll have W1 X1 + W um one X2 something that right so you will have uh multi-layers uh that are being applied on a given on a given X1 X2 they are applied with 1 plus sigmoid and the output of them is putting to another sigmoid output of those will be put into another weight sigmoid with weights weighted some of those output is put into other so it's a by basing on number of Loops you will have number of powers to which this will grow so it went to three layers so end of the neural network output if you look at this is your final output so if you have instead of doing doing all this if you have one single equation like this then your neural network is equalent to this equation that's the whole logic so but what we are doing instead of um solving that part particular one complex equation uh we are actually dealing with multiple smaller we breaking down the whole problem of arriving at a complex equation into smaller parts and those smaller parts were being computed separately and those smaller parts are

0:38:37 -  are being given like the this big one play multiple parts we are we are breaking it down to smaller pieces the smallest part is the the last third term right so we are solving that third term so if you look at it this is layer one two nodes this is Layer Two Two nodes and this is layer three one node one neuron two neurons one neuron so first we calculate the first layer these two neurons output and then we calculate this neuron and then we calculate this third neuron so like this we calculate the whole sum but if the if you calculation wise if you look at it this whole equation if you calculate you will get the output so instead of during all these neur neural networks if I have one equation like this but here the assumption is activation function is sigo functioned the activation function Sig function across all neurons then this particular equation one equation will give me instead of running all the neural networks blah blah blah and all that stuff I can remove it and then I can go with this particular equation I can solve this [Music] um but such a complex equation instead of solving it we are breaking it down each piece we are separated it and then bought it as one layer like the top piece first piece layer one second level Layer Two third level layer three so we we broke down the problem so this is layer one layer two layer three and these three layers being cons get the mathematical equation is broken down and then we we using these equations we are actually solving them not only about forward propagation like this where we we apply mathematical equation on each of them separately we are also seeing advantage of how to um so we we start with one W1 W2 W3 W4 W5 right as the parameters those parameters needs to be uh will be randomly picked at one point and then how do we uh how do we increase the parameters or how do we approach to the optimal values of this parameters is we use something optimizers there in the optimizers one we have seen already in is gradient

0:41:13 -  descent where U you pick WW1 and then what you do you you do a gr to you do a gradient descent so till you reach your loal Tima how do you do gradient descent is you you do a w is equal to W or W new is equal to W old minus um Lambda into derivative D of loss function by D of cost function by DW so you you do a derivative of the cost function to because um I'm repeating all those things but nothing wrong you want the cost function to be reduced so you you derivative of cost function will give you direction in which it is going down and you so you take a derivative you multiply it with Lambda which is nothing but your learning right um basing on that which defines the steps in which the W will be gradient design will be done so w minus Lambda into you you use the derivative and you reduce it so W1 W2 W3 W W4 W5 these four weights we can reduce them basing on derivatives and how do you calculate these derivatives um for each of them that is something we'll see as part of um um as part of um back propagation so anyways so to start off you understood that this is nothing but the whole neural is one single equation which is broken down into smaller piece has to be calculated individually so that the computation is easier um and also if you look at it we'll also from compute perspective we'll we'll take the advantage of um running the whole calculation on one single thread we'll split it we can actually go parallel and these two are dependent right as long as X1 X2 is known W one of this and W2 weights of this are are known we can calculate them parall so even if there are 20 layers here like this even if you have multiple layers here then all of them can be calculated in parallel and then output of it once it you get it again these all can be calculated in parallel and then be given as input to this so and see features he can we can add the features here x squ so uh yeah parallel processing is one advantage of breaking down into neural networks and and stuff and the the individual weights uh can be

0:43:52 -  calculated also calculated separately at each layer we'll look at how the weights parameter optimization will be done so let's look uh CH GPT these outputs are not 100% I don't like those outputs to extract them so because this topics are needs to be given in much more detailed way let's look at um online blog on what are different optimization techniques optimization okay optimization optimization import process in the training of go of op man to find the set of parameters that yields the best performance for so whatever gradient design that you do itself is Optimizer the optimization of parameters in order to have uh the least amount of uh cost that is within the cost graph we for given parameters that is there we want to pick up the least cost that is possible so you keep reducing the parameters or you you keep uh not reducing or increasing whatever you keep changing the parameters W1 W2 w345 um until you reach the minimal cost for the overall function because here if you look at it you look at the as a black box so X is input Y is output so you once y output comes and Y original is there you C the cost here the intermediate outs are never compared with why because that's it's not your objective the objective is the output layer so you calculate the cost of the output layer and then you work backwards to find out how you can reduce the uh weights across all the layers in order to uh reduce or change the weights across all the internal neurons so that the Y is closure to the expected y okay so y predicted is closure to actual y so the difference loss in between those values is minimal that's our objective so optimization process minimizing and maximizing mathematical expression this process used to reduce losses and provide the most accurate results possible okay this is optimization so let's look at optimization algorithms in url networks so again there are multiple like activation functions we

0:46:30 -  look at each of them there is also they I think they'll also give explanation on uh which one to be used when kind of stuff and this graph is pretty nice so if you all of these optimizers if they start at a certain point like this then how do they behave is what they are talking about so let's observe one at a time sgb is uh okay there is no color red so this is red color so if you look at it um sgb is actually coming from here to here and it is it is stopping there okay it is not going down whereas momentum uh which is uh green in color I'm not sure it doesn't show here so because the green line is not coming here I'm assuming the green is also being stuck here momentum so but whereas nag adagrad ad Delta and RMS prop all of these if you look at it all these colors If You observe all these four colors are actually coming this way okay 1 2 3 4 four colors are coming this way I'm not sure if the green is coming or not but we'll find out if the green will come or not okay I think green is also coming this way so get it and then we have a concept of floss it because optimization so such as weights and learning rate to reduce the losses optimizers are used to solve optimization Problems by minimizing the function so how do optimize useful hiker trying to get down a mountain with the okay okay he's expanding ra which is okay so it is GD is gradient descent as GD is stochastic uh stochastic gradient descent and MB SGD is mini bad stock gradient descent SD with moment momentum so that is uh the momentum here momentum is SD with momentum and nro accelerator gradient nag this I have not seen adaptive gradient ad grad is this is famous Ada Delta that's also famous ARS is also famous and then Adam is the most famous one and most used one each one of them if you see gradient descent is an optimization that's used for training and machine learning model

0:49:33 -  so this you gradient descent we have already studyed this the one which we have looked at in linear regression the same gradient is being used here um this is oneus th minus Alpha into derivative of cost by th uh so it will come like this this is gr and descent this is all linear regression same how much the output of a function changes if you change the inputs a little bit okay now importance of learning rate um yeah that's okay fine this is learning R smaller learning R smaller steps bigger L bigger steps and bigger so if you have smaller you take more time bigger learning rate you might not reach the local optimal so you should take an ideal value how do you pick up an ideal learning rate that's a different story um we'll get to it later on if possible um Advantage are for GR easy competition easy to May trap at local minimum that's correct so what happens is in here what's happening it will so let's say this is one this is two this is three this is maximum uh maximum Max are the best minimal value but gradient desent at times if it is this is closer or this is closer it might go here basing on the starting point where you where you go so that's that's a disadvantage that to say saying it might not be so now let's go to stockis so we understood disadvantages is May trap at Lo weights also change after calculating the gradients on the whole data so if the data set is too large then this may take years to conver to requires large memory to calculate the grent of the whole data set okay so it takes some time that's the whole summary the compute is high complexity is minimal but the compute is high the time is high the stock estic gradient descent algorithm is an extens of gradient Des that some of the disadvantage of DD gradi descent um lot of memory load and SGD algorithm Dera is computed taking one point at a time so so what they do the input points they don't um go with all the points they'll take one point as

0:52:10 -  performs a parameter update for each training example XI and label Yi so the calculation he he says instead of taking all the X parameters doing the whole math he takes one experiment so he takes One X parameter and Y parameter and then he'll calculate it and then he'll repeat that process so to faster we can take a gradient distance step for each training in example so for each training example he will take a step so it might not be a proper gradient because the cost is calculated for um overall correct but here what you're doing the the gradient isent you're doing it for each example separately so what happens it might not actually result in reducing your cost um because you're not it's looking at the influence of the overall point so what happens you need to this but as you calculate all the points you will end up in gradient decent only but to make your calculation easier you're he's doing it but the problem with this is I think he'll give it in disadvantages um the time required to complete one EPO is large compared to gradient alith because you calate for each example and to complete all the examples it will take time right so the compute requirement could be minimal but the time is more more than GD again s GD is needing more time than GD but the compute math calculation is um easier and the time it takes to converge that's what I was saying right it will take is but still and we don't solve the problem saying still this can get go and get stuck at minimal local op so for that they came up with MB SGD that is mini bats stock istic gradient design so instead of taking one example at a time what he takes does is mini batches nothing but he takes a group of values so he divides the whole uh whole

0:55:13 -  data set into smaller pieces and for each smaller piece for each batch he'll take the cost and then he'll do a derivative blah blah blah and then he'll he'll do it is the batch of training example so for each batch he'll separately calculate everything so disadvantages here is um update of mbgd is much noisy compared to the update of GD algorithm take a longer time to converge than the GD algorithm right may get stuck at local so still the local optimal problem is is being carried across all the gradient it is not being solved anywhere stock istic radiant isent and uh okay this is now new one he says SGD with momentum so a mejor disadvantage of mbgd is the updates of the weights are very noisy s GD with momentum overcomes this by by Den noising the gradients updates of weight are dependent on noisy derivative and if he somehow Denise the derivatives then converging time will decrease the idea is to Deno derivative using exponential waiting a that is to give more weightage to recent updates compared to the previous update okay to accelerate the converion okay this is if you look at it he's adding an additional term to this particular uh this thing so that uh the now the weights are updated as V equal to Theta minus VT so previously it used to be Theta minus Alpha into Alpha of J of theta by Dera of theta but instead we are actually doing weighted average of the previous values and then we doing um we creating a we reducing the noise or something but again let's look at um then has all advantages as in the CD cones faster than GD algorithm we need to compute one more variable for each each update yeah so nesto accelerated gradient I think this is just optimization on top of the SGD with momentum the case of momentum momentum and gradient are computed on previous updated weights it's okay I'll skip that okay let's talk of adaptive gradient descent adagrad algorithms the learning rate

0:58:1 -  remains constant so the key idea of adagrad is to have an Adaptive learning rate for uh yeah so adaptive learning so the problem is if you have a higher learning rate or a lower learning rate what is happening the the learning rate is either giving taking big steps or it say very small steps to reach local optim so adaptive gradient isn't I don't know why I forget this concept every time is something where it actually takes a learning rate relative to the starting of the gradient to going down so initially what will happen the um the learning rate will be higher so the steps will be bigger but as you can see basing on the way the cost is varying the learning rate also varies in in similar fashion so the steps will get reduced so once you start from a higher ground the steps will be bigger and I think he he'll show a graph below and uh it will change um basing on um okay see says GTI what is GTI here change in cost so change in cost is what is GTI so change in cost will be considered as your learning rate will actually get affected by change in uh the way the cost is changing number of iterations becomes very large learning rate decreases to a very small number which leads to a slow convergence so so what happens is from the top it will go fast but as it goes closer to the output it will become slower no need to update the learning rate manually as it changes adaptively with iteration okay at Delta so the problem with the previous algorithm was learning red becomes very small with the large number of iteration which leads to a slow convergence to avoid this Ada Delta algorithm has an idea to take an exponential decay so okay the the way you reduce the learning rate they are actually in of doing basing on uh learning rate by um the difference in cost and stuff they're going for exponential value so that the um learning rate doesn't get reduced uh the the the rate of reduction in

1:61:28 -  learning rate is actually proportional to the square of the change in the cost so that's how they keep mooving the uh learning rate so that it is not uh it doesn't get too low uh the steps doesn't get too small during convergence it will it will become meaningful yeah so Delta GT plus replacing the diagonal matrix GT with the decaying average or past squared gradients e g s of T he he here um t + 1 G GT [Music] is there a fact in fact is identical to the first update Vector of Delta that we derived above must prop as well divides the learning rate by an exponential decaying average of the squared [Music] [Music] radians so the take a certain amount of the previous plus the certain amount of gradi descent of the derivative of the the present one usually set to 0.9 reting SD in terms of the parameter of vector okay coming to RMS prop what does it say again 0.90 .1 everything is same the learning rate by an exponential average of squared gradients Hinton sest y to be 0.9 what is the difference I the difference between have both been developed independent around the same time staming from the need to resolve grads radically diminishing learning rates Adam adaptive momentum [Music] adaptive momentum estimation okay ad and the combination of RMS prop and stock Astic gradient desent with momentum uh okay the Adam that we're looking at it is again not it's not being done at uh um batch tastic is okay I'm lost with stock stic is individual each term

1:64:54 -  right stock estic is this is gradient descent stock atic is each term and [Music] uh mini batch MB SGD is mini batch and SD with momentum MDS [Music] So Adam we should see is it Min bats or is it uh each example Adam comput adaptive learning rates for each parameter in addition to storing and exponential taking a of of the past squar radians under Delta RMS prop Adam also keeps an exponential deing average of poti of past squared gradients a of past gradients so VT is exponential decaying average of past Square gradiance Mt is gradient so like gradient square and gradients both are being included so if you look at it uh the add um Ada grad was doing gradients Ada Delta was doing um Delta was doing gradian square and Adam is doing both so Mt is z and this is z s okay all of them if they're starting by number of iterations if you go the training cost for uh highest is adagrad it's going higher here as the number of is higher RMS prop is second uh ad Delta is third SGD nestro is third fourth and Adam is fifth So Adam is the best among all these the best Al so I don't understand if Adam is already the best why do we need to use uh the remaining okay so these are all the optimization techniques I think this is how they they have evaluated um but let's ask a question that's a good question to ask do we use while

1:69:0 -  Adam is the best Optimizer for neural networks other the optimizers in any other scenario is popular in op for training scenarios where other optim are preferred that's a good question to ask like we can say but as for that graph actually Adam is less compare add them to other optimizers with respect to cost compute and convergence time in table format between Adam and uh GB sgb [Music] mbb Adam Adam Adam Delta there's the comparison Okay add uh columns as cost compute and uh time so let's look at it Adam Delta moderate moderate so this guy is giving everything is moderate okay cost wise is still says lb is low time way is low compute wise everything is high doesn't matter because even though you do do with a bat or individual term your overall compute will be the same only thing is you can do it parallel and then reduce some time and all that stuff but your overall compute will be almost the same right so so if you want to reduce cost then you need to use other optimizers otherwise time wise it is Adam is lowest time wise with respect to all of them compute wise it is high uh if you go with mini batch and stuff they'll they're a little better so it's more compute and cost and uh if time is important go with Adam if you you want to save cost then go with with GR in descent in boosting in boosting in descent man