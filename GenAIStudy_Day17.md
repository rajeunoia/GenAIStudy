# Day 17 Study Time Gen AI

**Session Summary**  
**Time Interval:** 00:00 - 64:00

- **üîç Overview**: The session starts with an apology for yesterday's cancellation and transitions into a review of concepts previously covered, including **bias-variance tradeoff**, **overfitting**, **underfitting**, **regularization**, and **cross-validation** techniques aimed at ensuring model reliability.

- **üìù Model Evaluation Introduction**: The instructor shifts focus to **model evaluation**, discussing methods to validate model accuracy and the significance of selecting appropriate metrics for assessing model performance.

- **üìä Metrics for Evaluation**: Various metrics for regression and classification models are explored; for regression, metrics based on loss and cost are emphasized, while for classification, the instructor highlights metrics like **binary cross-entropy**. The evaluation process includes discussions on accuracy, precision, and recall.

- **‚úîÔ∏è Defining Accuracy**: The instructor provides a straightforward definition of accuracy in the context of predictions, addressing the importance of correctly interpreting the data distribution when measuring accuracy.

- **üè∑Ô∏è Precision and Recall**: The session delves into **precision** (the proportion of true positive predictions among all positive predictions) and **recall** (the proportion of true positive predictions among actual positives), explaining how these metrics relate to the model‚Äôs performance.

- **üìà F1 Score Introduction**: The concepts of precision and recall lead to the introduction of the **F1 score**, which combines both metrics to provide a singular performance measure, particularly useful in imbalanced datasets.

- **üìâ ROC and AUC**: The session concludes with an explanation of the **Receiver Operating Characteristic (ROC) curve** and the **Area Under the Curve (AUC)**, which measure the performance of a classification model across different thresholds. The higher the AUC, the better the model performance.

- **üöÄ Next Steps**: The instructor hints at further exploration of weighted metrics and continues with a discussion on multi-class scenarios and the corresponding confusion matrix in future sessions.

# Transcript 


0:2:42 -  hey hi everyone um so we couldn't do it yesterday we short of time um okay today we what we have seen the other days um um from we we long back we were looking at um um bias variance then to handle um so that which which in turn correlates with the overfitting underfitting um and then we looked at uh regularization um and then we looked at Cross validation and in Cross validation we looked at multiple options of cross validation um possibly that we could make it in a way uh that the cross validation is much more reliable with respect to Output um uh that being said we we we thought we'll look at um um the model evaluation how do we do it like see if you we we got data we put in pattern and then we come up with a model evaluation but um we need to come up with uh um within model evaluation we should come up with how it is accurate right there should be a metric right so I'll just say I'll just have data and then I'll draw a straight line line and then I'll uh um say this is this is the best model and somebody else will come and say that they'll draw a curve and then say this is the best model so um how do you say which is more so one way is to look at U as I said we can look at LW and stuff right loss cost and stuff that's one metrics that we can use to communicate it um but whereas coming to um are there any other metrics uh that can be used um so for regression classification problems regression problems we thought we thought we'll go with the loss uh and cost and and those values can be used as as metrics to show how much uh they are aligning with our expectations second is classification problems we have picked up LW as binary cross entropy the one you remember log y log y plus 1- log 1us we we tried that um okay so that being there we'll also do that uh thing on how do we

0:5:14 -  compare uh on these metrics we we'll do it at end of the day possible if you have time like what is the better one to use like if there are multiple ways of measuring accuracy which one is better to be used with respect to classific model we'll try to see if we can cover regression models evaluation metrics also but if it's not possible then we'll go ahead with classification model then starting off with classification model um if we start uh looking at classification model it's more about um uh you you try to classify uh given data and um the usual EXP expectation or generic or majority usage of classification is to find out um specific thing right for example the objective of finding whether it is um whether particular person has disease doesn't have diseases um objective is to find out if if it depends on the it depends on the use case but most probably the use case is to find out if he has a disease because not having diseas is always uh a generic one so you can again put them classify these two into basing on your objective question on what you are trying to class for example I want to classify if somebody has diabetes or not okay so this question can be answered as s or no correct where yes is he has diabetes no he he doesn't have diabetes then we can label it as yes is a positive answer to our question again it is not about positive somebody having diabetes is not positive at all I agree with the question but all we are trying to solve is a problem statement that is on table and the problem statement is stated like that saying does uh this person has diabetes so um now once you say yes and no you classify your so you have a list of features of the particular person and you put them here right so this is this is let's say this is yes and this is no then you you put them there into yes bucket and no

0:7:50 -  bucket basing on the features your model prediction say this is these are all yes these people have diabetes these are all no these people doesn't have diabetes I mean I've given a simple statement but it could be saying the problem of them getting a type one diabetes there could be no diabetic people and there could be type two diabetic people and then the probability of they entering into type one diabetic diabetes is something could be predicted that's interesting problem stat but at they saying uh for for easy understanding so now one now I have classified them saying these are all diabetes these are all non diabetic so but um that in the training data we already know how many of we training data we already have answers saying how many of them uh we have taken historic data like 10 years back or something like that and how many of them have got type one diabetes or not we know the answers as well because let say we are in supervised learning and we know we are been given with answers as well like the why now model has classified them as yes these people are going to be diabetic and no these people are not not going to be diabetic now and we have answers then the answers now can be can can address each of them like let's say start taking all the people who have been classified as yes these people are diabetic what will happen is the Y will tell me segregate this into two sections one is the model says yes and the prediction also says yes right yes yes the model said yes and the prediction y says no yes no that's those are two categories correct that's how we can when it says yes yes that means it's a models prediction is is is true for the positive scenario correct so that becomes true positive the when it model says yes and the prediction is no then it's um false it's not true right so it's a false positive so that's how you say something as true positive and false positive okay so models output we are actually models

0:10:48 -  output we are actually uh classifying it whether it is true or false that's all very simple okay similar case if you go below also there are people who have been classified as no diabetes that is also important why because somebody has been in class our objective is to find out who all has probability of getting diabetes one is we predict on yes and then we see who are all having the true POs to gives us diabetes but if you look at the no scenario in the no if you look at the no means model says these all people will not have diabetes but why says that is wrong what does it mean some model says these are not diabetic and and the the Y says it is false that means they are diabetic right so our our objective of finding diabetic people is also there also inclined here so we should also focus on this no scenario also so if you go to the no scenario within again no scenario if you take it you can can actually basing on the Y the model says no and you take Y and then y also says no y also says no from data then it is no no good and model says no but y says yes this is no s so this is true negative no no is true negative and this is um false negative right so false positive means model has predicted um false model is bad prediction gave bad prediction what is a bad prediction is positive and similarly false negative is uh let me share the screen I show you the confusion model prediction is no and uh but the actual value is posi that means it's a model has given false negative this this confusion Matrix is why I'm showing you are explaining about confusion matrix it's uh um that's also one of the indication of how the model is performing so that gives you a good classification on how good the classification very simple right so like the true positives and let me pull up the diagram so that you guys can see we'll go to

0:13:23 -  [Music] also yeah this is good actually so if you look at this so as I said the model predicted positive that is diabet is diabet is positive and um the model predicted diabet is negative okay within the positive case if you see the actual is nothing but the Y value right the Y value in the training data says yeah models positive is also positive for me that is true positive and uh the predicted is positive model says positive but the Y value says negative that means it's a wrong prediction right false prediction so it's false positive again uh the model prodction is negative these are all negative cases in which the by says uh negative production of model is correct that means y also says negative so negative negative that is false false will become uh sorry here um first of all here let's go by boxes negative negative that will be true negative your the model prodction is true that it is negative then next scenario is model prodction is negative but the Y value says it is positive so it's it's model prediction is false that it is negative okay that's how you should true that it is positive false that it is positive false that it is negative true that it is negative so I mean that's why you should read it I think people are little lazy to write the whole sentence or something like that are I mean even in what I said uh true that the prediction is positive false with the prediction as positive false with prediction as negative true with prediction that's how you should look at them so the more the the more the data uh within the data set the more the rows that fall under two positive two negatives the model performance is much better and because these two are error scenarios as he says here type one error type two error these two are error scenarios but our objective is here these two are our objective okay the

0:15:57 -  more more you get this correct the more accurate or more capable your model is okay that's how it is now we have jumped the bridge um or no this is good like let's maybe let's we can start defining let's say what is accuracy accuracy is um very simple accuracy is nothing but um what is accuracy is how much um out of so these are all choices right so there are like let's say there are 100 scenarios in 100 scenarios how many are correct that means the the correction is nothing but positive positive negative negative are the correct these two are the correct on so say let's say a b then a plus b is the number of um positive not positive number of current predictions correct so accuracy is nothing but number of correct predictions by total predictions so let's say this is a this is b c d then a + b by a + b plus C plus d so this is accuracy total correct predictions by total number of predictions this is good so that CHS right like how much percentage are we good with respect to prediction why don't we just stick to this and then move forward like model evaluation why don't why don't everybody agree that saying just tell me accuracy and I'm good we'll go forward with it challenge with predicting accuracy is all our this this confusion Matrix that we see on the screen um and the accuracy that I'm calculating everything is depending on data right so basing on the data I'm not saying talking about data's accuracy but I'm I'm talking about the data distribution right for example the training data has 1,000 records in which 900 people are diabetic and 100 people are non- diabetic and our predictions are simply saying everybody is diabetic then what

0:18:30 -  happens that means everybody is is being put in here and uh 900 are true positives and false positive is 100 then what happens it it becomes and this will be 0 0 because there are no predictions in negative 0 0 then what happens 900 to positives sum of 900 + 0 by 100 + 0 that means it's 9 okay 900 by 900 is 9 so it has decent number of uh decent percentage of the people have been predicted as correct sorry what did just do man uh sorry um a plus b that is 9 900 plus 0 900 by 900 + 0 + 100 + 0 a + B+ C plus d so 900 by 1000 is 0.9 that is 0.9 is 90% that means our model is doing 90% accuracy but our model has actually simply has written an equation saying it's like uh Y is equal to um Y is equal to s or right like Y is equal to 1 Let's said diabetic is one and non diabetic is 0 then y equal to 0 that's it our whole after learning so many equations so some simple one liner y equal to0 if I draw it that that gives me 90% accurate prediction so easy right no need of um no need of W's or no need of uh gradient and loss functions L function other can still be calculated but very easy y equal to Z but do you see from pattern perspective is that correct no it is not correct because um we are definitely not identifying anything which is on the true negative side also so we should also find out always our model irrespective of the size of the data it should actually be able to predict both of both the scenarios that is where it is becomes effective and that is where it generalizes to the um to the unknown data that is coming in or else it is like uh you come in diabetic go come in diabetic what happens as per this data that we have n out of 10 people if they're diabetic on on the case where we

0:21:4 -  are taking then we our predictions will also be correct because but the affected is the one non-diabetic will he'll also be classified in diabetic and he'll start using medicines and stuff so that is not a right thing to do correct so it's always not only about predicting one side of it it should should be able to model should always be able to predict both sides of it that's where the model is more generalized and the model is more aligned so the accuracy score that we are calculating is influenced by size and distribution of the data right um [Music] so accuracy can't be taken as a metric more accurate metric correct so then we came up with uh not V then they came up with something called two more terms basing on these values they they came up with two more terms something called precision and called recall okay what does what does they mean is in Precision they look at only the and precision and recall and they said okay let's not um let's not mix these two because these two are two classifications that is being done and uh in actually the way it it it predicts usually in the algorithms and stuff it doesn't say one Z you guys remember right it doesn't say one Z it usually says it's 0.8% sure it is positive diabetic it's 0.2% is diabetic where we say 0.8% is diabetic yes 0.2% is diabetic no correct that's how we read those values but it it it just gives us numbers with in probability scenarios kind of so always in a given prediction uh majority of the cases the possibility of both is always there as uh so we should not given a model we should not actually look at them as one single thing as we are predicting s or no but we should look at the probability of yes and probability of no considering that scenario it is good to calculate what is the accuracy on on diabetic yes and what is the accuracy on diabetic no right so these terms press recall when they

0:24:31 -  bought they they said PR and recall should be not like accuracy PR recall should be calculated for each class separately okay so now what is Precision Precision is um nothing but let me also ask CH gbt to Define um where are we we are somewhere okay give equation PR what happened okay now that's not working so we should go back to [Music] 2al false 2ga so this is accuracy two positive plus 2 negative those two boxes by as a plus b and then by a plus b plus C plus d this is accuracy now Precision again precision as I said precision and recall will be done for both positive and negative so this is precision for POS two classes okay so prec for for postive classes is nothing but if you take these two right this is true positive false positive right so when we say prec recall are calculated for positive and negative values uh separately so we look at within the B we look at um the positive scenarios Prim where and all you have positives 1 2 three because false negative actually is positive right see from this perspective this is positive right so these are all these are the all the three areas where you see positive values whether those could be um right or wrong these are all places on the screen from either on prediction perspective or act perspective you look at it so uh the Precision is precision is more on um says out of the prediction that the model is given how many are correct that's what Precision is trying to see like how precise is the prediction of the model so that is very simple that is out of all the positive values how many are correct very simple so uh uh true positive true positive by true positive

0:27:35 -  plus false positive that means true positive by true positive plus false POS how much percentage of the positive predictions are correct and how much percentage of the negative Productions are correct that means it will become true negative by true negative false negative that is for negative scenarios PR okay clear then let's talk of recall then recall is true positive by true positive plus false negative so recall is nothing but um let's also look at what is the what is the meaning of the term recall but true positive by true positive plus false negative that means Precision goes this way percentage of precision is p uh how much percentage of accurate prediction model has done um recall is how much percentage of expected prediction it has done properly that means the prediction the the objective is is 45 + 15 the 60 value should be expected is expected to be positive from the model as well those are positive is what we know from actual value that are given but the expectation is model should also be able to predict this 60 as positive um so how much percentage of these is correct right so out of the whole 60 45 are able to be predicted as positive that means it is 45 by 60 that is recall Precision is 45 by 53 okay that's precision and recall the number of samples predicted correctly to the belonging to the POS of samples actually belong to the POS recall bring back into one's mind remember I can still vely taken to the hospital recall the student days recall like is there any other meaning recall is like recalling your memory on what has Happ of they daring exploits and the nobility of their call not sure why he said recall

0:30:9 -  but so it's more like we we are kind of uh ordering calling back the prediction in the prediction it said no but you're actually bringing it back saying no no no this is not correct uh this is wrong so that's what it's trying to say but maybe I I definitely feel it's it's it's a the naming could have been much more accurate um uh I would not say accurate is the term much more meaningful so that people can easily I see challenges most of the people who are actually they day they doing modeling somebody who has learned uh the question on table will be what pression what is recall they'll they'll actually think for a think for a moment because um the terms doesn't really give them the actual so for example accuracy is easy like what percentage are your predictions correct very simple accuracy word also means the same thing Precision is some level okay like Precision is how precise is the prediction that's also fine recall is somewhere which is little not uh easy remember or correlate with the what it is but as I said in in the diagram um Precision goes this way and recall goes this way same way Precision for negative values goes this way and recall will go this way okay so it will be it will become total negatives by total negatives plus false negative Si from this perspective it it will become the Precision of the negative values negative scenarios and recall for negative scenar scenarios will be the 32 by 40 total negative by total negative plus false positive okay so I I don't think there is another choice to do but it is you should remember this pression and recall those are the ways um you can actually predict overall accuracy accuracy of model prediction uh for each class accuracy from the actual value perspective the expected value perspective so this is all accuracy precision and recall accuracy is overall Precision is model

0:33:3 -  side of the accuracy and um recall is the expected output side of accuracy okay now pre and recall giving two Matrix and then they people if they start publishing like my for me for my model that I've given my Precision is X recall is y somebody says his PR is y and recall is X now there is a confusion on what should be the um value that should be taken how how should I compare it because it says XY YX is this both same or is this different there will be um discussion as that so we should actually um um we should actually combine these two and then come up with a metric that can be a one to one comparison instead of comparing two and then it it'll become it it not be clear so um they came up with we need to come up with one single metric which has both these terms uh is the expectation so one thing that they picked up is to go with a harmonic mean let's quickly look at what is um harmonic mean M harmonic me is one of the several kinds of average and in particular one of the pyan means I don't know term it is sometimes appropriate for situations when the average rate is desired the harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations as an example but let's question would be why somebody would use harmonic mean right we can use mean we can use harmonic mean why will somebody use harmonic mean instead of a normal mean it is a reciprocal of artha mean reciprocal of Artic mean by terms like artha mean is X1 plus X2 + X3 so on xn by by n what he's saying is we will reciprocal of each terms like 1 by X1 + 1 by X2 + 1X X3 so on 1X xn by 1 byn not only that and reciprocal of the overall thing as well reciprocal of overall thing if you do it's not one by n then n is n Only the mean

0:36:1 -  of reciprocal of the features and or of and uh reciprocal of overall value it's very confusing but you can you can you can see on the screen and we can understand so we can't say it is mean of reciprocals also because it's reciprocal of mean of reciprocals that's it is means 1X X Sigma of 1x X by n is actually mean of reciprocals like X1 is the value then reciprocal of X1 is 1x X1 so the mean of reciprocals is Sigma of 1 byx by n but harmonic mean what he saying is reciprocal of that like n by your you that also and by Sigma 1 byx okay yeah sorry he has given that here so harmonic mean is equal to 1 by arithmetic mean of 1 by X and xn uh that means uh oh he says arithmetic mean is nothing but T of x equal to 1 by harmonic mean of 1 by X2 I doubt this a little bit this equation arithmetically this is X1 + X2 + X1 by n this is 1 by 1 by oh then if you keep this here 1 by n by Sigma 1 by X and some is equal to Sigma 1xx by n is what you'll get oh because he got it get it I get it see uh for him X1 is equal to 1 by X1 so if here if he says 1 by X1 it will become X1 of arithmetic mean that's the reason he was putting it like that makes sense yeah this is reverse s harmonic mean is a scar concave function and dominated by the minimum of its arguments in the sense that for any positive set of arguments Min of X11 so all xn is less than or equal to H of X1 XM as that Al to n into of X1 X the harmonic mean canot be made large

0:39:3 -  by value to bigger ones so the harmonic wind value will always be in between the minimum of the input values and uh and into minimum of the uh given values is what he is saying n into some term is there and um hormon mean is always less than and into the term which is okay uh but our n into the term here is n into 1 by Sigma of 1x x what is saying is n into Sigma of of so n into 1 by Sigma of 1xx is always um less than 1 by should be less than one yeah because yeah it should be because if n into if it is greater than one then harmonic X1 harmonic mean h h is actually will be greater than n but as per this equation what he says is this 1 by one Sigma of 1x X is always less than one so any n if you multiply with less than um less than one it will become less than n value like 0.5 is n by2 right like like that it will become less than n so 1 by Sigma of 1xx should be always be will always be less than one is this point um let's say we have negative points what would happen one by 1X 2 - 1 by 3 let's say there are only two points 2 into 1 by 1X 2 - 1X 3 will be 3 - 2 1 by 6 it's uh six then it will become 2 into 6 12 and it is and it is greater than n oh oh oh oh here that's the reason he says in the sense that for any positive set of arguments here it clearly says negative is coming it this will not work

0:42:18 -  out for positive set of arguments he's saying the harmonic mean will work like this for us the X values if you're taking the Precision and recall they are uh they are always positive what there's no possibility right TP by TP plus FN or TP by TP plus FP any of them will not be negative so they are all positive numbers on so it'll it will always be minimum of TP uh sorry minimum oppression recall and 2 into minum of recall it will become the harmonic Min will be always in between these two by Chang me also concave which is an even stronger property than scar concavity one has to take care of to use positive numbers though since the mean also be concave negative values are used quadratic me Artic mean geometric mean harmonic mean okay qm m [Music] GM H is here okay I think we've already covered a lot of stuff um we will Happ go back on to our discussion okay here so now harmonic mean of PR and recall that means press and recall two times so harmonic mean will become and 2 by 1 by press plus 1 by recall so 2 by 1 by PR plus 1 by recall press plus recall by PR recall so 2 by press plus recall by press and recall this term will go up that will become 2 into pression into recall by press plus recall that is F1 and this will be become minimum of press in between press recall will always be positive values considering that it will become minimum of pre comma recall less than F score less than two so the value will always be less than two oh that's interesting F un score

0:45:3 -  less than two is something I don't know is said here for positive classes but this is actually applicable for negative classes also score will be same because press and recall is calculated for both of them separately then it will be same then he says specificity number of samples predicted correctly to be in negative class out of all samples in data set that actually belong to thetive please of all the samples oh so recall term TN by FP + TN is TN TN plus FP TN this is nothing but recall expected value so actual value so it is recall of negative term is also called as specificity one more term we don't know why he has used recall specificity recall of negative values is specificity my head we can also calculate confusion Matrix for multiple classes um if you have like four like let's say four fruits Apple orange pineapple PE something like that then how do we calculate it becomes is 22 it is 2x2 matrix now it will become 4x4 Matrix that's all because each of the predictions is there is a possibility of being there in the um any of the four classes second is four classes um four classes predictions could be uh false could be true the true scenario is true true scenario is 1 1 2 2 3 3 4 4 right so that is this diagonal here but it can also be one two prediction as one but it is actually two prediction as one actually 3 1 4 so all combinations are possible like each of them can actually be uh the positive scenario will be one for each prediction and negative scarios will be three that is n minus one like like four is n then one is positive and out of n classes and N minus one classes is negative that's

0:47:50 -  how it is so what we do same logic right so 52 so if you say Precision Precision of the prediction is 52 by 52 + uh 12 52 + 3 + 7 + 2 which is nothing but 52 by 64 right right whereas recall is this way 52 by 52 + 2 + 5 + 1 that is 52 by 60 so that is recall and your uh F1 score will will be 2 by 1 by uh 1X 52 by 60 uh plus 1 1 by um 52x 64 hey interestingly we should try to solve this uh keeping the the Precision and recalls equation into F1 score it is TP Square will come because 1 by TP by tp+ FN and TP plus FP these are the equation so it will become a multiplication there so TP into TP is TP squ so 2 TP squ by TP + FN into TP uh + FP like I'm just saying multiplication of these two terms TP plus FP into 1 by 1 by this one + 1 by this one right 2 into TP s by TP + FP by PR recall will be again TP into oh yeah TP into TP plus yeah yeah it's question plus recall so addition of these two terms it will become TP TP is a common term so we get it out then it will TP by 1 by TP + FP + 1 by TP plus FN it will become 2 tp+ FP plus FN by TP plus FP into TP + FN which is also there on the top so it will get cancelled on both terms it will become 2

0:50:54 -  TP s by TP into TP Plus 2 TP plus FN plus FP TP TP TP s by TP so TP canel it's 2 TP by 2tp + fp+ FN what did you derive out of it is nothing oh man you're just doing the math got Okay cool so so even if there multiple class confusion Matrix also can be calculated and we can get the values that's the whole story is there any other concept weighted F1 score interesting weighted PR weighted recall how are the weights being calculated this is precision of the one Precision of recall of one and the f f score of one like that F1 score is calculated for each of them oh yeah here I said I mean like the representation I said FN score equation is same but FN score will be calculated for each of them because obviously if pressure and recall is calculated for each of the class separately then F1 score also should be calculated for each of them separately but but again even though we calculate pre recall F1 score it doesn't actually reflect the overall uh model accuracy right I don't know maybe they can take a mean of the [Music] overall he's taking here mean of macro the measures are up to be okay re score take a sample weighted mean of The Classy scores obtained so the weighted scores obtained are how did he come with 6034 43 54 6034 where did he get 60 34 60 uh yeah 34 oh the sums of it 60 34 waited oh he says weighted not weighted in weights that we have in model he's saying weighted that means he's making it relative to the amount of samples that are that are there so 60 to to to have relative because there could be more class one class 2 class three class 4 as an accuracy so he's trying to

0:53:39 -  say we we'll keep weights for uh uh weighted what he's saying is weighted mean in of doing a plus b plus C A+ plus b c + d by 4 he's doing a weighted me weighted mean of those values that means for 60 samples this is a score precision for 34 samples is the Press 43 samples is the Press weighted okay just a weighted me for all the samples okay this can be used this is this is a good metric on um for using As symetric for comparison overall can we say this is overall F1 s recall we don't know maybe we can on minor specificity this guy has done a lot of job man blog is pretty good oh so he is taking us towards uh um he's taking towards the other Concepts here so I'll produce the zoom let's look at it so waited F1 score so receiver operating characteristics Ro the receiver operating characteristics again some complicated cures man these guys are like they I don't know how they they come up with some terms why don't they just some keep some meaningful so that uh true positive rate that sounds much better than R receiver operating characteristics very odd name with respect to false posture rate oh it's to false positive and different um thres settings Roc CES are usually defined for a binary classification model although that can be extended to a multi class settings which we'll see later the definition of the two Cate ppres exactly sensity or recall so true positive rate is he's saying Pro positive rate is um recall correct because recall is on the actual things it is the true positive rate and

0:57:0 -  um f don't tell me false POS rate is precision false part yeah it's not Precision false POs to rate is uh false positive by false positive plus true negative that's again recall only the recall of negative so DPR is recall of positive values fpr is and uh this is uh the recall of negative values that is but recall of negative values also recall of negative values if you look at it specificity it's true negative by false positive true negative it's not actually false negative by uh this thing right it is has number of negative class samples negative class samples predicted wrongly okay so it is not PR it is not recall it is something different right um one minus BCS is 1 minus recall of negativity or specificity is false positive rate it seems why ma represent okay that's that's the terms he has defined why he has defined of true positive rate false positive rate true positive rate and the other one is false positive rate any sense false positive it is false positive by false positive plus 2 negative why that means he has taken a false positive is here accuracy is what we've calculated but total so accuracy is total right predictions by total number of predictions uh but here he's saying false positive rate will be total Wrong by false positive means wrong total Wrong by total wrong values false positive is is nothing but negative scenario um by the total negative scenarios but it's not the correct negative scenario that is true negative is the correct negative scenario but we are taking false positive okay I don't

1:60:10 -  know why but yeah tpr fpr got it true positive rate is nothing but recall of positive terms FP is 1 minus recall of oh yeah could be to compare both of them you should get it to the same line ideally if it is binary classification and U if it's X it will become the negative side will be 1 - x I think he's using that X and 1 - x kind of stuff true positive rate is One X and then false positive rate will become should be considered as should not be as y it should be 1 minus y so that we can compare both of them that could be scenario so both tpr can be computed from existing thresholds in the context of oh sh where did come from something okay so increase uh given a a is a term to increase Precision what we need to do is pre is where um percentage of uh percentage of predictions positive pred percentage of prediction should be higher so what what happened now I think he's looking at the orange circles all of the orange circles are predicted properly that is 100% prediction right here it is four are positively predicted um out of five 4X 5 is the B here here it is 1 2 3 4 5 6 sixer Precision is out of the all okay then he's refer to the this one um two by 2 + 0 2 by2 100% whereas recall is uh 2 by 4 2x 2 2x 4 correct now it is 4 by 7 approximately he kept it like that but recall is 4x4 100% this is correct so positive class negative class plus this is postive [Music] plus now this this bound threshold can

1:63:19 -  be case b now this St bound threshold can be changed case b where the is 100% case the recall is 100% is 50% corresponding confusion Matrix are shown shown the tpr and fpr values of these three scenarios with the a b c shown as below using this values R curve can be plotted an example of R curve for a binary [Music] EX so true positive rate is nothing but recall of positive compared to 1 minus recall of negative if you look at it um the comparison of those two minimal of two positive rate it is actually the other way if yeah how can it be 0 0 1 1 when the yeah when true positive rate that is recall of [Music] positive is high model is able to predict it doesn't uh recall recall recall uh recall is total positives by total positives plus false negatives if it is 100% all false negatives is zero that means two negatives is 100% that's correct correct if it's Total Recall of positive terms if it is one that means 100% prediction of positive values is done if 100% prediction of positive Val false negatives which are below is become zero if false negatives is zero then true negatives is true negatives is 100% so it's 1 one 0 0 perfect man perfect that's correct R curve is right so now R curve has this for binary classification below

1:67:14 -  okay a learner that makes random prediction is called no skill classifier for a class balanced data set the class wise probability will be 50% practical learner no skill perfect learner so perfect will make recall on postive one and then he would move towards uh recall of all positives say oh so that is perfect L and no skill okay perfect no skill is important because I think this graph is our model and this is no skill when he says no skill um both are getting equal why does he say no skill here is a question no learner that makes random precisions is called the no skill classifier class Balan data set the class wise probabilities will be 50% mhm oh sorry somebody comes in and says okay 50% it is positive 50% negative if you use 50 % 50% as the thing then uh the values that come up is this line the total positive rate and false positive rate is 50% 50% so it's equalent both will be equal so recall of positive terms and one minus recall of negative terms both will be equal as the reason he has a straight line that is no skill 50 plus %. that's the reason he's calling it as no whereas um um the perfect learner is somebody who has always has true positive rate as maximum and false positive rate as zero and the true post being one he actually learns on false postive rate respect of f pulse rate is always on two PSE rate is always one for any true postive rate the false postive rate is zero that means false postive rate is nothing but 1 minus uh recall of negative values and recall of negative

1:70:45 -  values is always zero and pro postive rate is two postive rate is is one and false pa toate okay perfect plan okay this only this Dot and this dot sounds like the ideal values but perfect learner goes towards it at least but why does he go like this is where I'm [Music] confused [Music] the the recall should be highest recall of positive and recall of negative if you say the recall if it is highest that means the percentage of actual predictions is highest the percentage of negative Productions is highest TP TN are high that's actually our higher or expected Behavior right if TP is high total and this is also High then what happens false positive rate will be minimal one minus assuming rate is less than one when DPN is high and fpn is also High when tpn is when fpn is high tpn will be very low not necessarily DP not corrected the probability can B classification right if tpn is high automatically yeah if tpn is [Music] one tpn 1 means recall is one recall if it is DPN is one recall is one correct tpr tpr is one that means recall is one recall is 100% if recall is 100% all uh positive values are predicted properly as positives if all um that means TP is 100 FN is z that doesn't say there are no false positives there could still be FSE positives could be more right yeah this could be false

1:74:19 -  positives and true negatives the two negatives if they are 100% it can still have few false negatives as well it's not one to one nice good good good so these are all possible okay class R is what again receiver operating characteristics I don't know weird names man weird names Roc plot what is Roc plot oh this is Roc plot maybe mhm last will have a plot somewhere in between these two reference lines the more AR now shifted towards the 00 01 0 point just towards the perfect learner curve the batter is its predictive performance across the ultra shs okay so objective is this line that is there it is expected to be going closer to this perfect lar up this is perfect larner and this is uh uh the no skill learner kind of stuff right R examples so appsme data set okay and mainly LPC data set some data set so for multi class data sets red matx into one all mat which then how to do this Pap for example cancel okay I understand Aros curs now okay the pink one is the best closest to one. oh no no no so this is like Roc curve of each of them class Zer class 1 Class 2 class 3 class 4 the class three prodution is pretty good class 4 is okay and uh class one is bad class Z is also okay that's how we should read this graph okay now I'm able to read Roc graphs tools for computing a confusion Matrix okay this is all fine we are not covering um Roc is also done what is au somewhere it should be written Au here that measures the overall

1:79:7 -  performance of a classifier is the area under Roc r a r r just Au Au is area under curve that's why I remember it value as the name suggest it's simply the area measured under the ROC curve under the r curve a higher value of a represents a better classifier the AG of the Practical Lear above is 90% is good score the a of a no skill learner is 50% and that other perfect learner is 100% okay so area under curve is calculated by once we look at the diagram oh area is saying 0.991 and stuff but once let's look at area under curve okay this blog is not normal a it is too good too good examples Pap I should copy oh [Music] clockwise this is too good I should keep it somewhere I should I should remember it matx is there where we have learning too many man I should clean it up yeah okay so this is too good but let's look at a happy a see thank you [Music] R oh so [Music] that means if you draw a box [Music] so course but the area is calculated between this box like this area under Cur the question is uh here this is what I want to understand so the area under curve here is 0.92 that means this whole yellow portion is where he's calculating the area till here so this is 1.1 this 1 by one square piece that the curve is actually calculating between this lines to this thing it say